{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vytvorenie slovníka dvojíc pre účely Named Entity Recognizing\n",
    "#### Creating a dictionary of pairs for the purposes of Named Entity Recognizing: Wiki page - type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Projekt je momentalne rozdeleny do 2 časti.\n",
    "\n",
    "1. časť tvorí stahovanie potrebných súborov(wikipedia dump) na účely spracovania v projekte.\n",
    "2. časť tvorí parsovanie súborov spolu s priradením kategorie jednotlivym clankom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Part : Downloading Wikipedia articles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import re\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stiahnutie dát zo stránky wikipédie. Vyfiltrovanie všetkých súborov, ktoré obsahujú v názve \"pages-articles\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<!DOCTYPE html '"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_url = 'https://dumps.wikimedia.org/enwiki/20201001/'\n",
    "base_html = requests.get(base_url).text\n",
    "base_html[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<li class=\"file\"><a href=\"/enwiki/20201001/enwiki-20201001-pages-articles-multistream.xml.bz2\">enwiki-20201001-pages-articles-multistream.xml.bz2</a> 17.5 GB</li>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup_dump = BeautifulSoup(base_html, 'html.parser')\n",
    "soup_dump.find_all('li', {'class': 'file'}, limit = 10)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('enwiki-20201001-pages-articles-multistream.xml.bz2', ['17.5', 'GB']),\n",
       " ('enwiki-20201001-pages-articles-multistream-index.txt.bz2', ['215.8', 'MB']),\n",
       " ('enwiki-20201001-pages-articles-multistream1.xml-p1p41242.bz2',\n",
       "  ['231.7', 'MB']),\n",
       " ('enwiki-20201001-pages-articles-multistream-index1.txt-p1p41242.bz2',\n",
       "  ['222', 'KB']),\n",
       " ('enwiki-20201001-pages-articles-multistream2.xml-p41243p151573.bz2',\n",
       "  ['313.2', 'MB'])]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files = []\n",
    "for file in soup_dump.find_all('li', {'class': 'file'}):\n",
    "    text = file.text\n",
    "    if 'pages-articles' in text:\n",
    "        files.append((text.split()[0], text.split()[1:]))\n",
    "files[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['enwiki-20201001-pages-articles1.xml-p1p41242.bz2',\n",
       " 'enwiki-20201001-pages-articles2.xml-p41243p151573.bz2',\n",
       " 'enwiki-20201001-pages-articles3.xml-p151574p311329.bz2',\n",
       " 'enwiki-20201001-pages-articles4.xml-p311330p558391.bz2',\n",
       " 'enwiki-20201001-pages-articles5.xml-p558392p958045.bz2']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files_to_download = [file[0] for file in files if re.search('pages-articles\\d{1,2}.xml-p',file[0])]\n",
    "files_to_download[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Použitie knižnice keras na stiahnutie týchto súborov/datasetu. Stiahnú sa len tie súbory, ktoré ešte nie sú stahnuté"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from keras.utils import get_file\n",
    "directory = '/home/xminarikd/.keras/datasets/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading\n",
      "Downloading data from https://dumps.wikimedia.org/enwiki/20201001/enwiki-20201001-pages-articles1.xml-p1p41242.bz2\n",
      "242098176/242093817 [==============================] - 330s 1us/step\n",
      "Downloading\n",
      "Downloading data from https://dumps.wikimedia.org/enwiki/20201001/enwiki-20201001-pages-articles2.xml-p41243p151573.bz2\n",
      "324780032/324777650 [==============================] - 445s 1us/step\n",
      "Downloading\n",
      "Downloading data from https://dumps.wikimedia.org/enwiki/20201001/enwiki-20201001-pages-articles3.xml-p151574p311329.bz2\n",
      "352124928/352119906 [==============================] - 313s 1us/step\n",
      "Downloading\n",
      "Downloading data from https://dumps.wikimedia.org/enwiki/20201001/enwiki-20201001-pages-articles4.xml-p311330p558391.bz2\n",
      "389988352/389987127 [==============================] - 372s 1us/step\n",
      "Downloading\n",
      "Downloading data from https://dumps.wikimedia.org/enwiki/20201001/enwiki-20201001-pages-articles5.xml-p558392p958045.bz2\n",
      "420814848/420806959 [==============================] - 471s 1us/step\n",
      "Downloading\n",
      "Downloading data from https://dumps.wikimedia.org/enwiki/20201001/enwiki-20201001-pages-articles6.xml-p958046p1483661.bz2\n",
      "450748416/450745879 [==============================] - 553s 1us/step\n",
      "Downloading\n",
      "Downloading data from https://dumps.wikimedia.org/enwiki/20201001/enwiki-20201001-pages-articles7.xml-p1483662p2134111.bz2\n",
      "462512128/462504094 [==============================] - 385s 1us/step\n",
      "Downloading\n",
      "Downloading data from https://dumps.wikimedia.org/enwiki/20201001/enwiki-20201001-pages-articles8.xml-p2134112p2936260.bz2\n",
      "471654400/471652241 [==============================] - 344s 1us/step\n",
      "Downloading\n",
      "Downloading data from https://dumps.wikimedia.org/enwiki/20201001/enwiki-20201001-pages-articles9.xml-p2936261p4045402.bz2\n",
      "512147456/512145263 [==============================] - 485s 1us/step\n",
      "Downloading\n",
      "Downloading data from https://dumps.wikimedia.org/enwiki/20201001/enwiki-20201001-pages-articles10.xml-p4045403p5399366.bz2\n",
      "502456320/502449820 [==============================] - 357s 1us/step\n",
      "Downloading\n",
      "Downloading data from https://dumps.wikimedia.org/enwiki/20201001/enwiki-20201001-pages-articles11.xml-p5399367p6899366.bz2\n",
      "486776832/486770345 [==============================] - 393s 1us/step\n",
      "Downloading\n",
      "Downloading data from https://dumps.wikimedia.org/enwiki/20201001/enwiki-20201001-pages-articles11.xml-p6899367p7054859.bz2\n",
      "46866432/46859027 [==============================] - 69s 1us/step\n",
      "Downloading\n",
      "Downloading data from https://dumps.wikimedia.org/enwiki/20201001/enwiki-20201001-pages-articles12.xml-p7054860p8554859.bz2\n",
      "404406272/404401272 [==============================] - 624s 2us/step\n",
      "Downloading\n",
      "Downloading data from https://dumps.wikimedia.org/enwiki/20201001/enwiki-20201001-pages-articles12.xml-p8554860p9172788.bz2\n",
      "164061184/164054426 [==============================] - 131s 1us/step\n",
      "Downloading\n",
      "Downloading data from https://dumps.wikimedia.org/enwiki/20201001/enwiki-20201001-pages-articles13.xml-p9172789p10672788.bz2\n",
      "330563584/330555755 [==============================] - 278s 1us/step\n",
      "Downloading\n",
      "Downloading data from https://dumps.wikimedia.org/enwiki/20201001/enwiki-20201001-pages-articles13.xml-p10672789p11659682.bz2\n",
      "229384192/229381099 [==============================] - 205s 1us/step\n",
      "Downloading\n",
      "Downloading data from https://dumps.wikimedia.org/enwiki/20201001/enwiki-20201001-pages-articles14.xml-p11659683p13159682.bz2\n",
      "393928704/393922075 [==============================] - 348s 1us/step\n",
      "Downloading\n",
      "Downloading data from https://dumps.wikimedia.org/enwiki/20201001/enwiki-20201001-pages-articles14.xml-p13159683p14324602.bz2\n",
      "273661952/273655291 [==============================] - 271s 1us/step\n",
      "Downloading\n",
      "Downloading data from https://dumps.wikimedia.org/enwiki/20201001/enwiki-20201001-pages-articles15.xml-p14324603p15824602.bz2\n",
      "355713024/355706040 [==============================] - 263s 1us/step\n",
      "Downloading\n",
      "Downloading data from https://dumps.wikimedia.org/enwiki/20201001/enwiki-20201001-pages-articles15.xml-p15824603p17324602.bz2\n",
      "307257344/307257124 [==============================] - 317s 1us/step\n",
      "Downloading\n",
      "Downloading data from https://dumps.wikimedia.org/enwiki/20201001/enwiki-20201001-pages-articles15.xml-p17324603p17460152.bz2\n",
      "28254208/28249729 [==============================] - 111s 4us/step\n",
      "Downloading\n",
      "Downloading data from https://dumps.wikimedia.org/enwiki/20201001/enwiki-20201001-pages-articles16.xml-p17460153p18960152.bz2\n",
      "336871424/336865577 [==============================] - 308s 1us/step\n",
      "Downloading\n",
      "Downloading data from https://dumps.wikimedia.org/enwiki/20201001/enwiki-20201001-pages-articles16.xml-p18960153p20460152.bz2\n",
      "314253312/314246115 [==============================] - 205s 1us/step\n",
      "Downloading\n",
      "Downloading data from https://dumps.wikimedia.org/enwiki/20201001/enwiki-20201001-pages-articles16.xml-p20460153p20570392.bz2\n",
      "22953984/22949874 [==============================] - 35s 2us/step\n",
      "Downloading\n",
      "Downloading data from https://dumps.wikimedia.org/enwiki/20201001/enwiki-20201001-pages-articles17.xml-p20570393p22070392.bz2\n",
      "351920128/351918604 [==============================] - 228s 1us/step\n",
      "Downloading\n",
      "Downloading data from https://dumps.wikimedia.org/enwiki/20201001/enwiki-20201001-pages-articles17.xml-p22070393p23570392.bz2\n",
      "362340352/362336803 [==============================] - 221s 1us/step\n",
      "Downloading\n",
      "Downloading data from https://dumps.wikimedia.org/enwiki/20201001/enwiki-20201001-pages-articles17.xml-p23570393p23716197.bz2\n",
      "40402944/40402767 [==============================] - 27s 1us/step\n",
      "Downloading\n",
      "Downloading data from https://dumps.wikimedia.org/enwiki/20201001/enwiki-20201001-pages-articles18.xml-p23716198p25216197.bz2\n",
      "375750656/375742870 [==============================] - 295s 1us/step\n",
      "Downloading\n",
      "Downloading data from https://dumps.wikimedia.org/enwiki/20201001/enwiki-20201001-pages-articles18.xml-p25216198p26716197.bz2\n",
      "347947008/347946542 [==============================] - 251s 1us/step\n",
      "Downloading\n",
      "Downloading data from https://dumps.wikimedia.org/enwiki/20201001/enwiki-20201001-pages-articles18.xml-p26716198p27121850.bz2\n",
      "87384064/87377512 [==============================] - 119s 1us/step\n",
      "Downloading\n",
      "Downloading data from https://dumps.wikimedia.org/enwiki/20201001/enwiki-20201001-pages-articles19.xml-p27121851p28621850.bz2\n",
      "337952768/337946504 [==============================] - 229s 1us/step\n",
      "Downloading\n",
      "Downloading data from https://dumps.wikimedia.org/enwiki/20201001/enwiki-20201001-pages-articles19.xml-p28621851p30121850.bz2\n",
      "297205760/297201089 [==============================] - 213s 1us/step\n",
      "Downloading\n",
      "Downloading data from https://dumps.wikimedia.org/enwiki/20201001/enwiki-20201001-pages-articles19.xml-p30121851p31308442.bz2\n",
      "281026560/281023102 [==============================] - 159s 1us/step\n",
      "Downloading\n",
      "Downloading data from https://dumps.wikimedia.org/enwiki/20201001/enwiki-20201001-pages-articles20.xml-p31308443p32808442.bz2\n",
      "383336448/383334873 [==============================] - 241s 1us/step\n",
      "Downloading\n",
      "Downloading data from https://dumps.wikimedia.org/enwiki/20201001/enwiki-20201001-pages-articles20.xml-p32808443p34308442.bz2\n",
      "349700096/349699080 [==============================] - 394s 1us/step\n",
      "Downloading\n",
      "Downloading data from https://dumps.wikimedia.org/enwiki/20201001/enwiki-20201001-pages-articles20.xml-p34308443p35522432.bz2\n",
      "259284992/259278058 [==============================] - 153s 1us/step\n",
      "Downloading\n",
      "Downloading data from https://dumps.wikimedia.org/enwiki/20201001/enwiki-20201001-pages-articles21.xml-p35522433p37022432.bz2\n",
      "351600640/351597304 [==============================] - 515s 1us/step\n",
      "Downloading\n",
      "Downloading data from https://dumps.wikimedia.org/enwiki/20201001/enwiki-20201001-pages-articles21.xml-p37022433p38522432.bz2\n",
      "340942848/340936052 [==============================] - 502s 1us/step\n",
      "Downloading\n",
      "Downloading data from https://dumps.wikimedia.org/enwiki/20201001/enwiki-20201001-pages-articles21.xml-p38522433p39996245.bz2\n",
      "345997312/345996096 [==============================] - 232s 1us/step\n",
      "Downloading\n",
      "Downloading data from https://dumps.wikimedia.org/enwiki/20201001/enwiki-20201001-pages-articles22.xml-p39996246p41496245.bz2\n",
      "340557824/340555294 [==============================] - 231s 1us/step\n",
      "Downloading\n",
      "Downloading data from https://dumps.wikimedia.org/enwiki/20201001/enwiki-20201001-pages-articles22.xml-p41496246p42996245.bz2\n",
      "351027200/351022715 [==============================] - 304s 1us/step\n",
      "Downloading\n",
      "Downloading data from https://dumps.wikimedia.org/enwiki/20201001/enwiki-20201001-pages-articles22.xml-p42996246p44496245.bz2\n",
      "354238464/354232150 [==============================] - 347s 1us/step\n",
      "Downloading\n",
      "Downloading data from https://dumps.wikimedia.org/enwiki/20201001/enwiki-20201001-pages-articles22.xml-p44496246p44788941.bz2\n",
      "55885824/55884326 [==============================] - 60s 1us/step\n",
      "Downloading\n",
      "Downloading data from https://dumps.wikimedia.org/enwiki/20201001/enwiki-20201001-pages-articles23.xml-p44788942p46288941.bz2\n",
      "228384768/228383839 [==============================] - 217s 1us/step\n",
      "Downloading\n",
      "Downloading data from https://dumps.wikimedia.org/enwiki/20201001/enwiki-20201001-pages-articles23.xml-p46288942p47788941.bz2\n",
      "362102784/362097862 [==============================] - 626s 2us/step\n",
      "Downloading\n",
      "Downloading data from https://dumps.wikimedia.org/enwiki/20201001/enwiki-20201001-pages-articles23.xml-p47788942p49288941.bz2\n",
      "304455680/304454625 [==============================] - 328s 1us/step\n",
      "Downloading\n",
      "Downloading data from https://dumps.wikimedia.org/enwiki/20201001/enwiki-20201001-pages-articles23.xml-p49288942p50564553.bz2\n",
      "235069440/235065016 [==============================] - 175s 1us/step\n",
      "Downloading\n",
      "Downloading data from https://dumps.wikimedia.org/enwiki/20201001/enwiki-20201001-pages-articles24.xml-p50564554p52064553.bz2\n",
      "322166784/322159926 [==============================] - 269s 1us/step\n",
      "Downloading\n",
      "Downloading data from https://dumps.wikimedia.org/enwiki/20201001/enwiki-20201001-pages-articles24.xml-p52064554p53564553.bz2\n",
      "322420736/322418062 [==============================] - 255s 1us/step\n",
      "Downloading\n",
      "Downloading data from https://dumps.wikimedia.org/enwiki/20201001/enwiki-20201001-pages-articles24.xml-p53564554p55064553.bz2\n",
      "306626560/306624664 [==============================] - 192s 1us/step\n",
      "Downloading\n",
      "Downloading data from https://dumps.wikimedia.org/enwiki/20201001/enwiki-20201001-pages-articles24.xml-p55064554p56564553.bz2\n",
      "321986560/321985394 [==============================] - 189s 1us/step\n",
      "Downloading\n",
      "Downloading data from https://dumps.wikimedia.org/enwiki/20201001/enwiki-20201001-pages-articles24.xml-p56564554p57025655.bz2\n",
      "101703680/101699893 [==============================] - 63s 1us/step\n",
      "Downloading\n",
      "Downloading data from https://dumps.wikimedia.org/enwiki/20201001/enwiki-20201001-pages-articles25.xml-p57025656p58525655.bz2\n",
      "335503360/335498693 [==============================] - 193s 1us/step\n",
      "Downloading\n",
      "Downloading data from https://dumps.wikimedia.org/enwiki/20201001/enwiki-20201001-pages-articles25.xml-p58525656p60025655.bz2\n",
      "296157184/296156960 [==============================] - 233s 1us/step\n",
      "Downloading\n",
      "Downloading data from https://dumps.wikimedia.org/enwiki/20201001/enwiki-20201001-pages-articles25.xml-p60025656p61525655.bz2\n",
      "329154560/329146942 [==============================] - 217s 1us/step\n",
      "Downloading\n",
      "Downloading data from https://dumps.wikimedia.org/enwiki/20201001/enwiki-20201001-pages-articles25.xml-p61525656p62585850.bz2\n",
      "231440384/231434326 [==============================] - 145s 1us/step\n",
      "Downloading\n",
      "Downloading data from https://dumps.wikimedia.org/enwiki/20201001/enwiki-20201001-pages-articles26.xml-p62585851p63975909.bz2\n",
      "344973312/344965297 [==============================] - 321s 1us/step\n",
      "Downloading\n",
      "Downloading data from https://dumps.wikimedia.org/enwiki/20201001/enwiki-20201001-pages-articles27.xml-p63975910p65475424.bz2\n",
      "310239232/310232346 [==============================] - 275s 1us/step\n"
     ]
    }
   ],
   "source": [
    "data_paths = []\n",
    "file_info = []\n",
    "\n",
    "for file in files_to_download:\n",
    "    path = directory + file\n",
    "    \n",
    "    if not os.path.exists(directory):\n",
    "        print('neexistuje')\n",
    "    # downaload only when file dont exist\n",
    "    if not os.path.exists(directory + file):\n",
    "        print('Downloading')\n",
    "        data_paths.append(get_file(file, base_url + file))\n",
    "        file_size = os.stat(path).st_size / 1e6\n",
    "        \n",
    "        # Find the number of articles\n",
    "        file_articles = int(file.split('p')[-1].split('.')[-2]) - int(file.split('p')[-2])\n",
    "        file_info.append((file, file_size, file_articles))\n",
    "        \n",
    "    # when file already exist\n",
    "    else:\n",
    "        data_paths.append(path)\n",
    "        file_size = os.stat(path).st_size / 1e6\n",
    "        \n",
    "        file_number = int(file.split('p')[-1].split('.')[-2]) - int(file.split('p')[-2])\n",
    "        file_info.append((file.split('-')[-1], file_size, file_number))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('enwiki-20201001-pages-articles9.xml-p2936261p4045402.bz2',\n",
       "  512.145263,\n",
       "  1109141),\n",
       " ('enwiki-20201001-pages-articles10.xml-p4045403p5399366.bz2',\n",
       "  502.44982,\n",
       "  1353963),\n",
       " ('enwiki-20201001-pages-articles11.xml-p5399367p6899366.bz2',\n",
       "  486.770345,\n",
       "  1499999),\n",
       " ('enwiki-20201001-pages-articles8.xml-p2134112p2936260.bz2',\n",
       "  471.652241,\n",
       "  802148),\n",
       " ('enwiki-20201001-pages-articles7.xml-p1483662p2134111.bz2',\n",
       "  462.504094,\n",
       "  650449)]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(file_info, key = lambda x: x[1], reverse = True)[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Part Parsing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parsovanie prebieha postupne na všetkých súboroch v kompresovanom tvare. Na tento účel je použitý podproces \"bzcat\", ktorý číta a dodáva súbor po jednotlivých riadkoch. Na spracovanie týchto dát je použitý XML SAX parser. Tento parser obsahuje metódu ContentHandler, ktorá zabezpečuje uchovanie riadkov v buffery, pričom sa hľadajú tagy (page, title, text). Po nájdeni ukončovacieho znaku tagu page prebieha spracovanie celého článku.\n",
    "\n",
    "Z článku sú pomocou regulárnych výrazov extrahované informácie:\n",
    "* **infobox**\n",
    "    * atribúty infoboxu\n",
    "    * typ infoboxu\n",
    "* **kategórie čklánku**\n",
    "\n",
    "Následne na základe týchto informácií je určená kategória článku."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import xml.sax\n",
    "import regex\n",
    "import pandas as pd\n",
    "from functools import reduce\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import csv\n",
    "import json\n",
    "import os\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "import nltk\n",
    "import gc\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Momentálne sú priradzované kategórie: Person, Company, Organisation, Place.\n",
    "Priradzovanie prebieha podľa vyššieho poradia na základe parametrov v poradí:\n",
    "* **typ infoboxu** - či sa v zozname danej kategorie nachádza infobox daného článku\n",
    "* **atribúty infoboxu:**\n",
    "    * **person** - birth_date\n",
    "    * **company** - industry, trade_name, products, brands\n",
    "    * **organisation** - zatiaľ žiadne\n",
    "    * **place** - coordinates, locations _|neobsahuje|_ date, founded, founder, founders\n",
    "* **kategorie článku:**\n",
    "    * **organisation** - obsahuje v kategóriach slovo organisaion/s\n",
    "* **text článku** - zatiaľ nepoužité, ale plánované pre prípady, kedy článok neobsahuje infobox a kategórie neposkytnú žiadnu informáciu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get Infobox and Infobox type from article text\n",
    "def ArticleHandler(infobox_types=None):\n",
    "    #source:(stackof) https://regex101.com/r/kT1jF4/1\n",
    "    infobox_regex = '(?=\\{Infobox )(\\{([^{}]|(?1))*\\})'\n",
    "    inf_type_regex = '(?<=Infobox)(.*?)(?=(\\||\\n|<!-|<--))'\n",
    "    #https://regex101.com/r/1vJlms/1\n",
    "    inf_parameters = '(?(?<=\\|)|(?<=\\|\\s))(\\w*)\\s*=\\s*[\\w{\\[]'\n",
    "    #https://regex101.com/r/fl5hAw/1 https://regex101.com/r/Xj0fM3/1\n",
    "    redirect_title = '(?<=\\[\\[)(.*)(?=\\]\\])'\n",
    "    categories = '(?<=\\[\\[Category:)([^\\]]*)(?=\\]\\])'\n",
    "    \n",
    "    Person=['player', 'male', 'actor', 'sportspeopl', 'medalist', 'actress', 'expatri', 'singer', 'musician', 'live', 'writer', 'politician', 'f.c', 'alumni', 'personnel', 'olymp', '20th-centuri', 'faculti', 'coach', 'guitarist']\n",
    "    Company=['brand', 'merger', 'retail', 'exchang', 'stock', 'label', 'nasdaq', 'multin', 'subsidiari', 'acquisit', 'onlin', 'offer', 'held', 'conglomer', 'store', 'bankruptci']\n",
    "    Organisation=['scout', 'think', 'non-profit', 'gang', 'multi-sport', 'event', 'recur', 'religi', 'child-rel', 'non-align', 'non-government', 'critic', 'right', 'chess', 'evangel', 'yakuza', 'advocaci']\n",
    "    Location=['regist', 'unincorpor', 'station', 'popul', 'complet', 'aerodrom', 'villag', 'town', 'landform', 'parish', 'river', 'seaplan', 'open', 'census-design', 'mountain', 'attract', 'neighbourhood', 'suburb', 'rang', 'airport']\n",
    "    \n",
    "    \n",
    "    # infobox_types = getInfoboxTypesList()\n",
    "    \n",
    "    def getCategories(text):\n",
    "        return regex.findall(categories, text)\n",
    "    \n",
    "    \n",
    "    def getArticleAtributes(infobox,text):\n",
    "        i_par = regex.findall(inf_parameters, infobox)\n",
    "        i_type = regex.search(inf_type_regex, infobox)\n",
    "        i_type = i_type.group(0).strip() if i_type is not None else \"none\"\n",
    "        return {'type': i_type.lower(), 'parameters': i_par, 'categories': list(getCategories(text))}\n",
    "    \n",
    "    \n",
    "    def remove_stop_words(data):\n",
    "        stopwords = nltk.corpus.stopwords.words('english')\n",
    "        return [w for w in data if w not in stopwords]\n",
    "\n",
    "\n",
    "    def tokenize(data):\n",
    "        symbols = symbols = \"!\\\"#$%&()*+'-./:;,|<=>?@[\\]^_`{}~\\n\"\n",
    "        tokens = word_tokenize(data)\n",
    "        tokens = [token.lower() for token in tokens if token not in list(symbols)]\n",
    "        return tokens\n",
    "\n",
    "\n",
    "    def stemming(data):\n",
    "        stemmer= PorterStemmer()\n",
    "        return [stemmer.stem(token) for token in data]\n",
    "        \n",
    "    def processCategories(data):\n",
    "        data = tokenize(data)\n",
    "        data = remove_stop_words(data)\n",
    "        data = stemming(data)\n",
    "        return data\n",
    "        \n",
    "    def isRedirect(text):\n",
    "        return regex.search(\"^#redirect\\s*\\[\\[(?i)\", text)\n",
    "        \n",
    "        \n",
    "    def getInfobox(text):\n",
    "        infobox = regex.search(infobox_regex, text)\n",
    "        return infobox.group() if infobox is not None else \"redirect\" if isRedirect(text) is not None else \"no infobox/redirect\"\n",
    "    \n",
    "    def categoryBy_infoboxType(info):\n",
    "        if info['type'] in infobox_types['person']:\n",
    "            return \"Person\"\n",
    "        elif info['type'] in infobox_types['company']:\n",
    "            return 'Company'\n",
    "        elif info['type'] in infobox_types['org']:\n",
    "            return \"Organization\"\n",
    "        elif info['type'] in infobox_types['location']:\n",
    "            return \"Location\"\n",
    "        else:\n",
    "            return None\n",
    "        \n",
    "    def categoryBy_atributes(info):\n",
    "        if 'birth_date' in info['parameters']:\n",
    "            return \"A_Person\"\n",
    "        elif any(i in info['parameters'] for i in ['industry', 'trade_name', 'products', 'brands']):\n",
    "            return 'A_Company'\n",
    "        elif any(i in info['parameters'] for i in ['coordinates', 'locations']) and not(any(i in info['parameters'] for i in ['date', 'founded', 'founder', 'founders'])):\n",
    "            return 'A_Location'\n",
    "        else:\n",
    "            return None\n",
    "        \n",
    "    def categoryBy_categories(info):\n",
    "        stemmed_categories = reduce(lambda x,y: x+y,map(lambda x: processCategories(x), info['categories']),[])\n",
    "        \n",
    "        if any(i in Person for i in stemmed_categories):\n",
    "            return 'C_Person'\n",
    "        elif any(i in Company for i in stemmed_categories):\n",
    "            return 'C_Company'\n",
    "        elif any(i in Organisation for i in stemmed_categories):\n",
    "            return 'C_Organization'\n",
    "        elif any(i in Location for i in stemmed_categories):\n",
    "            return 'C_Location'\n",
    "        \n",
    "        elif list(filter(lambda x: regex.search('\\b(compan(y|ies))\\b(?i)', x), info['categories'])):\n",
    "            return 'C_Company'\n",
    "        elif list(filter(lambda x: regex.search('(organisations*)(?i)', x), info['categories'])):\n",
    "            return 'C_Organization'\n",
    "        else:\n",
    "            return None\n",
    "    \n",
    "    \n",
    "    def first_true(iterable,data=None, default='Other'):\n",
    "        return next((item(data) for item in iterable if item(data) is not None), default)\n",
    "    \n",
    "    \n",
    "    def predictCategory(infobox, info):\n",
    "        if infobox not in ['redirect', 'no infobox/redirect']:\n",
    "            return first_true([categoryBy_infoboxType,categoryBy_atributes,categoryBy_categories], info)\n",
    "            \n",
    "            #tieto clanky maju len kategorie\n",
    "        elif infobox == 'no infobox/redirect':\n",
    "            return first_true([categoryBy_categories], info,\"Other/None\")\n",
    "        else:\n",
    "            return 'redirect::'+info\n",
    "\n",
    "    \n",
    "    def processArticle(title, text):\n",
    "        infobox = getInfobox(text)\n",
    "        \n",
    "        if infobox == \"redirect\":\n",
    "            info = regex.search(redirect_title, text).group(0)\n",
    "        \n",
    "        elif infobox == 'no infobox/redirect':\n",
    "            info = {'categories': list(getCategories(text))}\n",
    "            if info['categories'] == []:\n",
    "                return None\n",
    "        else:\n",
    "            info = getArticleAtributes(infobox, text)\n",
    "\n",
    "        return (title, infobox, info, predictCategory(infobox, info))\n",
    "    return processArticle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ano\n"
     ]
    }
   ],
   "source": [
    "sss = {'categories':[]}\n",
    "if sss['categories'] == []:\n",
    "    print('ano')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#docs: https://docs.python.org/3.8/library/xml.sax.handler.html\n",
    "class ContentHandler(xml.sax.handler.ContentHandler):\n",
    "    def __init__(self):\n",
    "        xml.sax.handler.ContentHandler.__init__(self)\n",
    "        self._buf = None\n",
    "        self._last_tag = None\n",
    "        self._parts = {}\n",
    "        self.output = []\n",
    "        self.article_process = ArticleHandler(infobox_types=getInfoboxTypesList())\n",
    "\n",
    "    def characters(self, content):\n",
    "        if self._last_tag:\n",
    "            self._buf.append(content)\n",
    "\n",
    "    def startElement(self, name, attrs):\n",
    "        if name == 'page':\n",
    "            self._parts = {}\n",
    "        if name in ('title', 'text'):\n",
    "            self._last_tag = name\n",
    "            self._buf = []\n",
    "\n",
    "    def endElement(self, name):\n",
    "        if name == self._last_tag:\n",
    "            self._parts[name] = ''.join(self._buf)\n",
    "        \n",
    "        #whole article\n",
    "        if name == 'page':\n",
    "            data = self.article_process(**self._parts)\n",
    "            if data is not None:\n",
    "                self.output.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def parseWiki(data=None, limit = 200, save = True, test_sample=False):\n",
    "    \n",
    "    if test_sample:\n",
    "        data = os.getcwd().rsplit('/', 1)[0]\n",
    "        data = f'{data}/data/sample_wiki_articles2.xml.bz2'\n",
    "        print(data)\n",
    "    elif data is None:\n",
    "        data = '/home/xminarikd/.keras/datasets/enwiki-20201001-pages-articles9.xml-p2936261p4045402.bz2'\n",
    "    \n",
    "    handler = ContentHandler()\n",
    "\n",
    "    parser = xml.sax.make_parser()\n",
    "    parser.setContentHandler(handler)\n",
    "\n",
    "    for i, line in enumerate(subprocess.Popen(['bzcat'], \n",
    "                             stdin = open(data), \n",
    "                             stdout = subprocess.PIPE).stdout):\n",
    "\n",
    "#         if (i + 1) % 10000 == 0:\n",
    "#             print(f'Spracovanych {i + 1} riadkov.', end = '\\r')\n",
    "#             print('')\n",
    "        try:\n",
    "            parser.feed(line)\n",
    "        except StopIteration:\n",
    "            break\n",
    "        \n",
    "        # get only some results\n",
    "        if len(handler.output) >= limit:\n",
    "            break\n",
    "        \n",
    "    if save:\n",
    "        output_dir = os.getcwd().rsplit('/', 1)[0]\n",
    "        partition_name = data.split('/')[-1].split('-')[-1].split('.')[0]\n",
    "        output_file = f'{output_dir}/output/{partition_name}.tsv'\n",
    "\n",
    "        with open(output_file, 'w+', newline='\\n') as file:\n",
    "            writer = csv.writer(file, delimiter='\\t')\n",
    "            writer.writerow([\"Title\", \"Category\"])\n",
    "            for x in handler.output:\n",
    "                writer.writerow([x[0],x[3] or 'None'])\n",
    "\n",
    "#         with open(output_file, 'w+', newline='\\n') as file:\n",
    "#             for x in handler.output:\n",
    "#                 file.write(json.dumps({\"title\": x[0], \"category\":x[3]}))\n",
    "        \n",
    "        print(f'{output_file} done', end='\\r')\n",
    "        del handler\n",
    "        del parser\n",
    "        gc.collect()\n",
    "        return None\n",
    "    else:\n",
    "        return handler.output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stiahnutie a parsovanie stránky wikipédie, ktorá obsahuje zoznam typov infoboxov. Tento zoznam obsahuje aj členeie týchto typov do rôznych kategórií. Vďaka tomuto je možné jednoducho získať všetky infoboxy, ktoré sú spojené napríklad s osobami."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getInfoboxTypesList():\n",
    "    infobox_list_url = 'https://en.wikipedia.org/wiki/Wikipedia:List_of_infoboxes'\n",
    "    infobox_list_html = requests.get(infobox_list_url).text\n",
    "    soup_dump = BeautifulSoup(infobox_list_html, 'html.parser')\n",
    "    #sib = soup_dump.find_all(\"div\" ,{'id': 'toc'}).next_sibling\n",
    "\n",
    "    template_list = dict();\n",
    "    prev = None\n",
    "    prev_tag = None\n",
    "    prev_parent = None\n",
    "    prev_parent_tag = 2\n",
    "\n",
    "    for i, sibling in enumerate(soup_dump.find(id=\"toc\").next_siblings):\n",
    "\n",
    "        if prev_parent == 'Other':\n",
    "            break\n",
    "\n",
    "        if sibling.name == 'h2':\n",
    "            template_list[sibling.findChild().text] = {}\n",
    "            prev_parent = sibling.findChild().text\n",
    "            prev_tag = 2\n",
    "\n",
    "        if sibling.name == 'h3':\n",
    "            if prev_tag < 3:\n",
    "                template_list[prev_parent][sibling.findChild().text] = list()\n",
    "                prev_tag = 3\n",
    "                prev = sibling.findChild().text\n",
    "\n",
    "            if prev_tag == 3:\n",
    "                template_list[prev_parent][sibling.findChild().text] = list()\n",
    "                prev = sibling.findChild().text\n",
    "\n",
    "        if sibling.name == 'ul':\n",
    "            a = sibling.find_all('a', title=re.compile('^Template:Infobox'))\n",
    "            b = map(lambda x: regex.findall('(?<=Template:Infobox )(.*)(?i)', x.text.lower()), a)\n",
    "            c = reduce(lambda x,y: x+y, b, list())\n",
    "\n",
    "            if prev_tag >=3:\n",
    "                template_list[prev_parent][prev] = [y for x in [template_list[prev_parent][prev], list(c)] for y in x] \n",
    "            else:\n",
    "                template_list[prev_parent] = list(c)\n",
    "\n",
    "    persons = list(reduce(lambda x,y: x+y, template_list[\"Person\"].values()))\n",
    "    locations = list(reduce(lambda x,y: x+y, template_list[\"Place\"].values()))\n",
    "    companies = template_list['Society and social science']['Business and economics']\n",
    "    organizations = template_list['Society and social science']['Organization']\n",
    "    \n",
    "    return {'person': persons, 'location': locations, 'company': companies, 'org': organizations}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spustenie funkcie na spracovanie súborov."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "David Stagg <--> Person\n",
      "Amaranthus mantegazzianus <--> redirect\n",
      "Amaranthus quitensis <--> redirect\n",
      "Maud Queen of Norway <--> redirect\n",
      "Milligram per litre <--> redirect\n",
      "Utica Psychiatric Center <--> Location\n",
      "Olean Wholesale Grocery <--> C_Company\n",
      "Queen Tiye <--> redirect\n",
      "Queen Hatshepsut <--> redirect\n",
      "Clibanarii <--> Other/None\n",
      "Political documentary <--> redirect\n",
      "Final fantasy legends <--> redirect\n",
      "Queen Marie Amelie Therese <--> redirect\n",
      "Political documentaries <--> redirect\n",
      "E-767 <--> redirect\n",
      "Prince Edward-Lennox <--> redirect\n",
      "Arthur Hill (actor) <--> Person\n",
      "Periodic paralysis <--> Other\n",
      "Greenstripe <--> redirect\n",
      "Amaranthus cruentus <--> Other/None\n",
      "Careless weed <--> redirect\n",
      "Zamil idris <--> redirect\n",
      "Khada sag <--> redirect\n",
      "Million instructions per second <--> redirect\n",
      "Ashtadiggajas <--> Other/None\n",
      "John C.Harsanyi <--> redirect\n",
      "Société entomologique de France <--> C_Organization\n",
      "Sangorache <--> redirect\n",
      "Joseph's coat <--> redirect\n",
      "Recipients of the Distinguished Service Award of the Order of the Arrow <--> C_Organization\n",
      "Josephscoat <--> redirect\n",
      "Joseph's-coat <--> redirect\n",
      "URM Stores <--> C_Company\n",
      "General purpose technology <--> Other/None\n",
      "Rough skinned newt <--> redirect\n",
      "Queen Sophie Dorothea of Hanover <--> redirect\n",
      "Cefepime <--> Other/None\n",
      "Larry Zerner <--> Person\n",
      "World of Illushions Starring Mickey Mouse and Donald Duck <--> redirect\n",
      "Category:Aberdeen, Hong Kong <--> C_Location\n",
      "United Retail Merchants <--> redirect\n",
      "Plassenburg <--> Other/None\n",
      "U.R.M. <--> redirect\n",
      "Turriaco <--> Location\n",
      "Piggly Wiggly Alabama Distributing Company <--> redirect\n",
      "Queen Eleanor of Aquitaine <--> redirect\n",
      "Crenshaw Company <--> Other/None\n",
      "Osmia ribifloris <--> Other/None\n",
      "San Pier d'Isonzo <--> Location\n",
      "Early life of Hugo Chávez <--> Person\n",
      "1974–75 NHL season <--> Other\n",
      "Demorrio Williams <--> A_Person\n",
      "Contact explosive <--> Other/None\n",
      "Bartley Secondary School <--> A_Location\n",
      "Tom Ayrton <--> Other\n",
      "Система <--> redirect\n",
      "Bedloe <--> redirect\n",
      "Cape Camarón <--> C_Location\n",
      "Reece Williams <--> Person\n",
      "Amariah Brigham <--> Other/None\n",
      "Microsoft MultiPlan <--> redirect\n",
      "Asadullah Rahman <--> redirect\n",
      "IMac DV <--> redirect\n",
      "Gute Bücher für Alle <--> C_Organization\n",
      "Howard Koh <--> Person\n",
      "Cefotaxime <--> Other/None\n",
      "The Ghost of the Grotto <--> Other\n",
      "Cwm-yr-Eglwys <--> Location\n",
      "Land Beneath the Ground! <--> Other\n",
      "The Immortals (band) <--> Other/None\n",
      "Category:Spanish handball players <--> C_Person\n",
      "Michael Sullivan (rugby league) <--> Person\n",
      "Polyisocyanurate <--> Other/None\n",
      "Blueberry Bee <--> redirect\n",
      "Littlejohn adaptor <--> Other/None\n",
      "Step Up (pricing game) <--> redirect\n",
      "Canon T-80 <--> redirect\n",
      "Grossauheim <--> redirect\n",
      "Yasuo Hamanaka <--> C_Person\n",
      "Rubel Castle <--> Location\n",
      "Columbus before the Council of Salamanca <--> redirect\n",
      "Gag names <--> redirect\n",
      "David Simmons (rugby league) <--> Person\n",
      "Save the Best for Last <--> Other\n",
      "Ken Sagoes <--> Person\n",
      "Buedingen, Hesse <--> redirect\n",
      "Excise Tax <--> redirect\n",
      "5th Marine Regiment <--> Other\n",
      "Eve Johnstone <--> Person\n",
      "Emergencies in India <--> redirect\n",
      "Ray of Creation <--> Other/None\n",
      "Swap Meet (pricing game) <--> redirect\n",
      "State Emergency in India <--> redirect\n",
      "List of presidents of Mississippi State University <--> Other/None\n",
      "National Emergency in India <--> redirect\n",
      "Financial Emergency in India <--> redirect\n",
      "Fish disks <--> redirect\n",
      "Gross-Gerau <--> redirect\n",
      "Gag name <--> Other/None\n",
      "Chaharbagh, Isfahan <--> C_Location\n",
      "Marcomms <--> redirect\n",
      "Montgomery Convention <--> redirect\n",
      "Gross-Umstadt <--> redirect\n",
      "The colophon <--> redirect\n",
      "Purely High School for Boys <--> redirect\n",
      "Danny Nutley <--> Person\n",
      "Amvescap <--> redirect\n",
      "Etxebarri, Anteiglesia de San Esteban - Etxebarri Doneztebeko Elizatea <--> redirect\n",
      "Johnny mosley <--> redirect\n",
      "Chahar bagh <--> redirect\n",
      "Ruesselsheim <--> redirect\n",
      "Pont Alexandre III <--> Location\n",
      "Replacement value <--> Other/None\n",
      "ABAP programming language <--> redirect\n",
      "Galbraith <--> Other/None\n",
      "Royal Bank Building (Toronto) <--> Other/None\n",
      "Norbert Lammert <--> Person\n",
      "MM-UGM <--> redirect\n",
      "Switch? (The Price Is Right) <--> redirect\n",
      "Manipuri (Meiteilon Language) <--> redirect\n",
      "Fountain of life <--> redirect\n",
      "British NVC community SD1 <--> Other/None\n",
      "Meetei <--> redirect\n",
      "MM UGM <--> redirect\n",
      "MMUGM <--> redirect\n",
      "Category:Politics of Cameroon <--> Other/None\n",
      "Piccolo Coro <--> redirect\n",
      "Greatest Native Americans <--> redirect\n",
      "Hapoel Tel Aviv B.C. <--> Other\n",
      "Psychology/rewrite <--> redirect\n",
      "The Wave (Arizona) <--> C_Location\n",
      "Category:Cameroon-related lists <--> Other/None\n",
      "Ault Hucknall <--> Location\n",
      "Frontier Middle School shooting <--> C_Organization\n",
      "Turf house <--> redirect\n",
      "Claris Impact <--> redirect\n",
      "Quannah Parker <--> redirect\n",
      "Switcheroo (pricing game) <--> redirect\n",
      "Moon dog <--> Other/None\n",
      "Rodney Eastman <--> Person\n",
      "Correggio, Emilia-Romagna <--> Location\n",
      "Peters Export and Import <--> redirect\n",
      "Wah Ching <--> Organization\n",
      "Oasis of the Zombies <--> Other\n",
      "Sihl <--> Location\n",
      "Smith & Mighty <--> Person\n",
      "Westinghouse Astronuclear Laboratory <--> Other/None\n",
      "Ovruch <--> Location\n",
      "Brother Alois <--> C_Person\n",
      "John Hannah (U.S. National Security Aide) <--> redirect\n",
      "Category:Spanish handball clubs <--> Other/None\n"
     ]
    }
   ],
   "source": [
    "data = parseWiki(test_sample=False, limit=300, save=False)\n",
    "\n",
    "for i, x in enumerate(data):\n",
    "    if i > 150:\n",
    "        break\n",
    "    if x[1] == 'redirect':\n",
    "        print(x[0], '<-->', x[1])\n",
    "    else:\n",
    "        print(x[0], '<-->', x[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool \n",
    "import tqdm\n",
    "from functools import partial\n",
    "import uuid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_dir = '/home/xminarikd/.keras/datasets/'\n",
    "dataset = [dataset_dir+file for file in os.listdir(dataset_dir)]\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<timed exec>:6: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7944b1414b93499eaecaf6ea09921c68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=58.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/xminarikd/Documents/VINF/output/p15824603p17324602.tsv done/home/xminarikd/Documents/VINF/output/p311330p558391.tsv done/home/xminarikd/Documents/VINF/output/p32808443p34308442.tsv done/home/xminarikd/Documents/VINF/output/p5399367p6899366.tsv done/home/xminarikd/Documents/VINF/output/p52064554p53564553.tsv done/home/xminarikd/Documents/VINF/output/p23716198p25216197.tsv done\n",
      "CPU times: user 264 ms, sys: 150 ms, total: 414 ms\n",
      "Wall time: 16.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "pool = Pool(processes=4)\n",
    "results = []\n",
    "\n",
    "map_parser = partial(parseWiki, limit = 100, save = True)\n",
    "\n",
    "for x in tqdm.tqdm_notebook(pool.imap_unordered(map_parser, dataset), total = len(dataset)):\n",
    "    results.append(x)\n",
    "\n",
    "pool.close()\n",
    "pool.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Elasticsearch([{'host': 'localhost', 'port': 9200}])>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "es = Elasticsearch([{'host': 'localhost', 'port': 9200}])\n",
    "es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readTsv(file):\n",
    "    output = []\n",
    "    with open(file) as f:\n",
    "        for line in csv.DictReader(f, delimiter='\\t'): \n",
    "            output.append(line)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_path = os.getcwd().rsplit('/', 1)[0]\n",
    "data_files = f'{out_path}/output/'\n",
    "data_files = [data_files+file for file in os.listdir(data_files) if file.endswith('.tsv')]\n",
    "len(data_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def toElastic(files, elastic):\n",
    "    for x in files:\n",
    "        data = readTsv(x)\n",
    "        for item in data:\n",
    "            res = elastic.index(index='test', doc_type=\"wikipedia\", id=uuid.uuid4(), body=item)\n",
    "            if res['result'] != 'created':\n",
    "                print('Warning, Error', res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/xminarikd/anaconda3/lib/python3.8/site-packages/elasticsearch/connection/base.py:190: ElasticsearchDeprecationWarning: [types removal] Specifying types in document index requests is deprecated, use the typeless endpoints instead (/{index}/_doc/{id}, /{index}/_doc, or /{index}/_create/{id}).\n",
      "  warnings.warn(message, category=ElasticsearchDeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "toElastic(data_files,es)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sample searching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'_index': 'test', '_type': 'wikipedia', '_id': '7178c4f0-47c1-4cc2-825e-86e47b48880b', '_score': 11.579972, '_source': {'Title': 'Hotline', 'Category': 'Other/None'}}]\n"
     ]
    }
   ],
   "source": [
    "res= es.search(index='test',body={'query':{'match':{'Title':'Hotline'}}})\n",
    "print(res['hits']['hits'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Delete all records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'acknowledged': True}"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "es.indices.delete(index='test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding common categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spracovanych 10000 riadkov.\r"
     ]
    }
   ],
   "source": [
    "from functools import reduce\n",
    "import numpy as np\n",
    "categories = []\n",
    "data_cat = parseWiki(limit=200 ,test_sample=False, save=False)\n",
    "\n",
    "cat_per = list(reduce(lambda x,y: x+y[2]['categories'],filter(lambda x: x[3] == 'Person',data_cat),[]))\n",
    "cat_com = list(reduce(lambda x,y: x+y[2]['categories'],filter(lambda x: x[3] in ['Company', 'W_Company', 'Q_Company'],data_cat),[]))\n",
    "cat_org = list(reduce(lambda x,y: x+y[2]['categories'],filter(lambda x: x[3] in ['Organization', 'W_Organization', 'Q_Organization'],data_cat),[]))\n",
    "cat_loc = list(reduce(lambda x,y: x+y[2]['categories'],filter(lambda x: x[3] in ['Location', 'W_Location'],data_cat),[]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1983',\n",
       " 'birth',\n",
       " 'australian',\n",
       " 'rugbi',\n",
       " 'leagu',\n",
       " 'player',\n",
       " 'rugbi',\n",
       " 'leagu',\n",
       " 'player',\n",
       " 'queensland',\n",
       " 'brisban',\n",
       " 'bronco',\n",
       " 'player',\n",
       " 'canterbury-bankstown',\n",
       " 'bulldog',\n",
       " 'player',\n",
       " 'queensland',\n",
       " 'rugbi',\n",
       " 'leagu',\n",
       " 'state',\n",
       " 'origin',\n",
       " 'player',\n",
       " 'rugbi',\n",
       " 'leagu',\n",
       " 'five-eighth',\n",
       " 'rugbi',\n",
       " 'leagu',\n",
       " 'centr',\n",
       " 'rugbi',\n",
       " 'leagu',\n",
       " 'lock',\n",
       " 'peopl',\n",
       " 'educ',\n",
       " 'padua',\n",
       " 'colleg',\n",
       " 'brisban',\n",
       " 'sportspeopl',\n",
       " 'townsvil',\n",
       " 'rugbi',\n",
       " 'leagu',\n",
       " 'second-row',\n",
       " 'wynnum',\n",
       " 'manli',\n",
       " 'seagul',\n",
       " 'player',\n",
       " 'live',\n",
       " 'peopl']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abc = reduce(lambda x,y: x+y,map(lambda x: preprocess(x),data_cat[0][2]['categories']))\n",
    "abc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "import nltk\n",
    "from collections import Counter\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words(data):\n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "    return [w for w in data if w not in stopwords]\n",
    "\n",
    "\n",
    "def tokenize(data):\n",
    "    symbols = symbols = \"!\\\"#$%&()*+'-./:;,<=>?@[\\]^_`{|}~\\n\"\n",
    "    tokens = word_tokenize(data)\n",
    "    tokens = [token.lower() for token in tokens if token not in list(symbols)]\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def stemming(data):\n",
    "    stemmer= PorterStemmer()\n",
    "    return [stemmer.stem(token) for token in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data):\n",
    "    data = tokenize(data)\n",
    "    data = remove_stop_words(data)\n",
    "    data = stemming(data)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories_processed = []\n",
    "categories_processed.append(reduce(lambda x,y: x+preprocess(y),cat_per,[]))\n",
    "categories_processed.append(reduce(lambda x,y: x+preprocess(y),cat_com,[]))\n",
    "categories_processed.append(reduce(lambda x,y: x+preprocess(y),cat_org,[]))\n",
    "categories_processed.append(reduce(lambda x,y: x+preprocess(y),cat_loc,[]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dfcount(data):\n",
    "    df = {}\n",
    "    for i in range(len(data)):\n",
    "        for token in data[i]:\n",
    "            try:\n",
    "                df[token].add(i)\n",
    "            except:\n",
    "                df[token] = {i}\n",
    "    for i in df:\n",
    "        df[i] = len(df[i])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1983': 3,\n",
       " 'birth': 3,\n",
       " 'australian': 3,\n",
       " 'rugbi': 2,\n",
       " 'leagu': 4,\n",
       " 'player': 2,\n",
       " 'queensland': 4,\n",
       " 'brisban': 2,\n",
       " 'bronco': 1,\n",
       " 'canterbury-bankstown': 1,\n",
       " 'bulldog': 1,\n",
       " 'state': 4,\n",
       " 'origin': 2,\n",
       " 'five-eighth': 1,\n",
       " 'centr': 2,\n",
       " 'lock': 1,\n",
       " 'peopl': 4,\n",
       " 'educ': 4,\n",
       " 'padua': 1,\n",
       " 'colleg': 4,\n",
       " 'sportspeopl': 1,\n",
       " 'townsvil': 1,\n",
       " 'second-row': 1,\n",
       " 'wynnum': 1,\n",
       " 'manli': 1,\n",
       " 'seagul': 1,\n",
       " 'live': 4,\n",
       " '1922': 2,\n",
       " '2006': 3,\n",
       " 'death': 2,\n",
       " 'canadian': 3,\n",
       " 'male': 2,\n",
       " 'film': 3,\n",
       " 'actor': 1,\n",
       " 'stage': 1,\n",
       " 'televis': 4,\n",
       " 'alzheim': 1,\n",
       " \"'s\": 4,\n",
       " 'diseas': 1,\n",
       " 'disease-rel': 1,\n",
       " 'california': 4,\n",
       " 'toni': 2,\n",
       " 'award': 3,\n",
       " 'winner': 3,\n",
       " 'melfort': 1,\n",
       " 'saskatchewan': 2,\n",
       " 'univers': 4,\n",
       " 'british': 3,\n",
       " 'columbia': 3,\n",
       " 'alumni': 2,\n",
       " 'expatri': 1,\n",
       " 'unit': 4,\n",
       " '1963': 3,\n",
       " 'american': 4,\n",
       " 'lawyer': 1,\n",
       " 'lo': 4,\n",
       " 'angel': 4,\n",
       " 'hugo': 1,\n",
       " 'chávez': 1,\n",
       " 'earli': 1,\n",
       " 'individual|chavez': 1,\n",
       " '1980': 4,\n",
       " 'panola': 1,\n",
       " 'counti': 3,\n",
       " 'texa': 4,\n",
       " 'footbal': 3,\n",
       " 'lineback': 1,\n",
       " 'kilgor': 1,\n",
       " 'ranger': 1,\n",
       " 'nebraska': 1,\n",
       " 'cornhusk': 1,\n",
       " 'atlanta': 3,\n",
       " 'falcon': 1,\n",
       " 'kansa': 3,\n",
       " 'citi': 4,\n",
       " 'chief': 2,\n",
       " 'san': 4,\n",
       " 'diego': 2,\n",
       " 'charger': 1,\n",
       " '1985': 3,\n",
       " 'new': 4,\n",
       " 'zealand': 3,\n",
       " 'descent': 3,\n",
       " 'italian': 3,\n",
       " 'cronulla-sutherland': 1,\n",
       " 'shark': 1,\n",
       " 'south': 3,\n",
       " 'wale': 4,\n",
       " 'team': 1,\n",
       " 'refere': 1,\n",
       " 'nation': 3,\n",
       " 'endeavour': 1,\n",
       " 'sport': 4,\n",
       " 'high': 4,\n",
       " 'school': 4,\n",
       " '1952': 3,\n",
       " 'physician': 1,\n",
       " 'massachusett': 4,\n",
       " 'depart': 3,\n",
       " 'health': 4,\n",
       " 'human': 2,\n",
       " 'servic': 4,\n",
       " 'offici': 1,\n",
       " 'harvard': 2,\n",
       " 'public': 3,\n",
       " 'faculti': 2,\n",
       " 'korean': 1,\n",
       " 'politician': 3,\n",
       " 'boston': 3,\n",
       " 'yale': 1,\n",
       " 'medicin': 2,\n",
       " 'doctor': 1,\n",
       " 'obama': 1,\n",
       " 'administr': 2,\n",
       " 'personnel': 2,\n",
       " 'coach': 1,\n",
       " 'northern': 3,\n",
       " 'eagl': 2,\n",
       " 'warrington': 1,\n",
       " 'wolv': 1,\n",
       " 'hooker': 1,\n",
       " 'halfback': 1,\n",
       " 'tare': 1,\n",
       " '1984': 4,\n",
       " 'penrith': 1,\n",
       " 'panther': 2,\n",
       " 'windsor': 1,\n",
       " 'winger': 1,\n",
       " 'fullback': 1,\n",
       " 'anglican': 2,\n",
       " 'sutherland': 2,\n",
       " 'shire': 3,\n",
       " 'sydney': 4,\n",
       " '1964': 3,\n",
       " 'african-american': 3,\n",
       " '1944': 2,\n",
       " 'scottish': 3,\n",
       " 'neuroscientist': 1,\n",
       " 'women': 3,\n",
       " 'fellow': 2,\n",
       " 'royal': 4,\n",
       " 'societi': 4,\n",
       " 'edinburgh': 1,\n",
       " 'psychiatrist': 1,\n",
       " 'schizophrenia': 1,\n",
       " 'research': 2,\n",
       " 'academ': 2,\n",
       " 'glasgow': 3,\n",
       " 'medic': 2,\n",
       " 'surgeon': 1,\n",
       " 'command': 3,\n",
       " 'order': 2,\n",
       " 'empir': 3,\n",
       " '1974': 4,\n",
       " 'crusher': 1,\n",
       " 'castleford': 1,\n",
       " 'tiger': 2,\n",
       " 'rooster': 1,\n",
       " 'redcliff': 1,\n",
       " 'dolphin': 1,\n",
       " 'prop': 1,\n",
       " '1948': 3,\n",
       " '20th-centuri': 3,\n",
       " 'german': 4,\n",
       " '21st-centuri': 2,\n",
       " 'christian': 4,\n",
       " 'democrat': 2,\n",
       " 'union': 3,\n",
       " 'germani': 4,\n",
       " 'roman': 4,\n",
       " 'cathol': 4,\n",
       " 'grand': 3,\n",
       " 'cross': 3,\n",
       " '1st': 1,\n",
       " 'class': 1,\n",
       " 'merit': 1,\n",
       " 'feder': 4,\n",
       " 'republ': 3,\n",
       " 'member': 4,\n",
       " 'bundestag': 1,\n",
       " 'north': 4,\n",
       " 'rhine-westphalia': 2,\n",
       " 'bochum': 1,\n",
       " 'presid': 1,\n",
       " 'oxford': 2,\n",
       " '2013–2017': 1,\n",
       " '2009–2013': 1,\n",
       " '2005–2009': 1,\n",
       " '2002–2005': 1,\n",
       " '1998–2002': 1,\n",
       " '1994–1998': 1,\n",
       " '1990–1994': 1,\n",
       " '1987–1990': 1,\n",
       " '1983–1987': 1,\n",
       " '1980–1983': 1,\n",
       " 'parliamentari': 1,\n",
       " 'secretari': 2,\n",
       " '1967': 1,\n",
       " 'montreal': 2,\n",
       " 'montebello': 1,\n",
       " 'english': 4,\n",
       " 'electron': 2,\n",
       " 'music': 4,\n",
       " 'group': 4,\n",
       " 'bristol': 2,\n",
       " 'cultur': 4,\n",
       " 'trip': 1,\n",
       " 'hop': 2,\n",
       " 'experiment': 1,\n",
       " 'washington': 3,\n",
       " '1848': 1,\n",
       " '1918': 1,\n",
       " '19th-centuri': 2,\n",
       " 'inventor': 2,\n",
       " 'magazin': 1,\n",
       " 'editor': 1,\n",
       " 'brooklyn': 2,\n",
       " 'journalist': 1,\n",
       " 'york': 4,\n",
       " 'engin': 3,\n",
       " 'appli': 1,\n",
       " 'scienc': 4,\n",
       " '1933': 2,\n",
       " 'governor': 3,\n",
       " 'barina': 1,\n",
       " 'venezuelan': 2,\n",
       " 'indigen': 2,\n",
       " 'spanish': 3,\n",
       " 'famili': 3,\n",
       " 'copei': 1,\n",
       " 'schoolteach': 1,\n",
       " 'socialist': 2,\n",
       " 'parti': 2,\n",
       " 'venezuela': 2,\n",
       " 'fifth': 1,\n",
       " 'movement': 4,\n",
       " 'rock': 3,\n",
       " 'establish': 4,\n",
       " '1972': 3,\n",
       " 'disestablish': 3,\n",
       " 'cambridg': 2,\n",
       " 'syd': 1,\n",
       " 'barrett': 1,\n",
       " '1939': 3,\n",
       " 'ohio': 4,\n",
       " 'council': 3,\n",
       " 'mayor': 1,\n",
       " 'toledo': 2,\n",
       " 'denison': 1,\n",
       " 'maume': 1,\n",
       " 'valley': 2,\n",
       " 'countri': 3,\n",
       " 'day': 2,\n",
       " '1855': 2,\n",
       " '1940': 3,\n",
       " 'birmingham': 2,\n",
       " 'idealist': 1,\n",
       " 'philosoph': 1,\n",
       " '1959': 3,\n",
       " '2002': 4,\n",
       " 'argentin': 3,\n",
       " 'comic': 2,\n",
       " 'artist': 3,\n",
       " '1979': 3,\n",
       " 'kollam': 1,\n",
       " 'india': 4,\n",
       " 'one': 1,\n",
       " 'intern': 3,\n",
       " 'cricket': 1,\n",
       " 'test': 1,\n",
       " 'zone': 2,\n",
       " 'kerala': 1,\n",
       " 'malayali': 1,\n",
       " 'challeng': 2,\n",
       " 'bangalor': 2,\n",
       " '1671': 1,\n",
       " '1743': 1,\n",
       " '18th-centuri': 1,\n",
       " 'cheyn': 1,\n",
       " 'family|georg': 1,\n",
       " 'vegetarian': 1,\n",
       " 'activist': 1,\n",
       " '1941': 2,\n",
       " 'loir': 1,\n",
       " 'french': 2,\n",
       " 'franc': 3,\n",
       " 'associ': 4,\n",
       " 'midfield': 1,\n",
       " 'saint-étienn': 1,\n",
       " 'olympiqu': 1,\n",
       " 'lyonnai': 1,\n",
       " 'manag': 3,\n",
       " 'fc': 1,\n",
       " 'girondin': 1,\n",
       " 'de': 4,\n",
       " 'bordeaux': 1,\n",
       " 'montpelli': 1,\n",
       " 'hsc': 1,\n",
       " 'nanci': 1,\n",
       " 'uefa': 2,\n",
       " 'euro': 2,\n",
       " '1996': 4,\n",
       " '1998': 3,\n",
       " 'fifa': 2,\n",
       " 'world': 4,\n",
       " 'cup': 2,\n",
       " 'cup-win': 1,\n",
       " 'inf': 1,\n",
       " 'clairefontain': 1,\n",
       " 'ligu': 1,\n",
       " '1': 2,\n",
       " 'chevali': 2,\n",
       " 'légion': 1,\n",
       " \"d'honneur\": 1,\n",
       " 'ontario': 3,\n",
       " 'altern': 2,\n",
       " 'hamilton': 2,\n",
       " 'toronto': 3,\n",
       " 'paper': 1,\n",
       " 'bag': 1,\n",
       " 'record': 3,\n",
       " 'six': 2,\n",
       " 'shooter': 1,\n",
       " 'juno': 1,\n",
       " 'root': 2,\n",
       " 'tradit': 1,\n",
       " 'album': 2,\n",
       " 'year': 4,\n",
       " '–': 2,\n",
       " '17th-centuri': 1,\n",
       " 'danish': 1,\n",
       " 'painter': 1,\n",
       " '1738': 1,\n",
       " 'court': 2,\n",
       " 'pupil': 1,\n",
       " 'carlo': 1,\n",
       " 'maratta': 1,\n",
       " 'histori': 4,\n",
       " '1904': 3,\n",
       " '1988': 4,\n",
       " 'rockefel': 1,\n",
       " 'taft': 2,\n",
       " 'philanthropist': 1,\n",
       " '1966': 3,\n",
       " 'basebal': 2,\n",
       " 'baltimor': 3,\n",
       " 'oriol': 1,\n",
       " 'florida': 3,\n",
       " 'bowi': 1,\n",
       " 'baysox': 1,\n",
       " 'columbu': 2,\n",
       " 'clipper': 1,\n",
       " 'eri': 1,\n",
       " 'seawolv': 1,\n",
       " 'fort': 3,\n",
       " 'lauderdal': 1,\n",
       " 'yanke': 1,\n",
       " 'frederick': 2,\n",
       " 'key': 2,\n",
       " 'gulf': 2,\n",
       " 'coast': 3,\n",
       " 'dodger': 1,\n",
       " 'major': 2,\n",
       " 'bullpen': 1,\n",
       " 'pitcher': 1,\n",
       " 'minor': 2,\n",
       " 'polk': 2,\n",
       " 'princ': 2,\n",
       " 'william': 1,\n",
       " 'cannon': 1,\n",
       " 'rochest': 1,\n",
       " 'red': 2,\n",
       " 'wing': 1,\n",
       " 'salem': 1,\n",
       " 'lakeland': 1,\n",
       " 'tuskege': 1,\n",
       " 'golden': 2,\n",
       " 'place': 3,\n",
       " 'miss': 4,\n",
       " '1942': 3,\n",
       " 'orlean': 3,\n",
       " 'louisiana': 2,\n",
       " 'senat': 3,\n",
       " 'polit': 4,\n",
       " 'creol': 1,\n",
       " 'st.': 2,\n",
       " 'augustin': 1,\n",
       " 'navi': 2,\n",
       " 'admir': 2,\n",
       " 'naval': 2,\n",
       " 'academi': 4,\n",
       " 'spring': 3,\n",
       " 'hill': 3,\n",
       " 'tennesse': 3,\n",
       " 'militari': 4,\n",
       " 'war': 3,\n",
       " 'spanish–american': 2,\n",
       " 'recipi': 1,\n",
       " 'distinguish': 2,\n",
       " 'medal': 1,\n",
       " '1558': 1,\n",
       " '1605': 1,\n",
       " '16th-centuri': 1,\n",
       " 'earl': 1,\n",
       " 'cumberland|3': 1,\n",
       " 'knight': 3,\n",
       " 'garter': 1,\n",
       " 'lord-lieuten': 1,\n",
       " 'cumberland': 2,\n",
       " 'privat': 4,\n",
       " 'northumberland': 2,\n",
       " 'westmorland': 1,\n",
       " 'nobil': 1,\n",
       " 'clifford': 1,\n",
       " 'sheriff': 2,\n",
       " 'anglo-spanish': 1,\n",
       " '1585–1604': 1,\n",
       " 'baron': 3,\n",
       " '1973': 4,\n",
       " 'metal': 2,\n",
       " 'guitarist': 1,\n",
       " 'limp': 1,\n",
       " 'bizkit': 1,\n",
       " 'maryland': 4,\n",
       " 'snot': 1,\n",
       " 'band': 1,\n",
       " '1877': 2,\n",
       " 'prestonpan': 1,\n",
       " 'forward': 1,\n",
       " 'greenock': 1,\n",
       " 'morton': 1,\n",
       " 'f.c': 2,\n",
       " 'bolton': 1,\n",
       " 'wander': 1,\n",
       " 'tottenham': 1,\n",
       " 'hotspur': 1,\n",
       " 'thame': 2,\n",
       " 'ironwork': 1,\n",
       " 'portsmouth': 1,\n",
       " 'burton': 1,\n",
       " 'motherwel': 1,\n",
       " 'southern': 3,\n",
       " '1957': 3,\n",
       " '2008': 3,\n",
       " 'quilm': 1,\n",
       " 'writer': 1,\n",
       " '1970': 3,\n",
       " 'mar': 2,\n",
       " 'church': 3,\n",
       " 'mission': 2,\n",
       " 'western': 3,\n",
       " 'seminari': 2,\n",
       " 'calvinist': 1,\n",
       " 'reform': 1,\n",
       " 'minist': 3,\n",
       " 'convert': 2,\n",
       " 'calvin': 1,\n",
       " 'catholic': 2,\n",
       " 'fork': 1,\n",
       " 'dakota': 2,\n",
       " 'seattl': 2,\n",
       " 'involv': 2,\n",
       " 'plagiar': 1,\n",
       " 'controversi': 1,\n",
       " 'critic': 2,\n",
       " 'femin': 1,\n",
       " 'arizona': 3,\n",
       " 'seatac': 1,\n",
       " 'highlin': 1,\n",
       " 'swedish': 3,\n",
       " 'defend': 1,\n",
       " 'allsvenskan': 1,\n",
       " 'eredivisi': 1,\n",
       " 'ifk': 1,\n",
       " 'göteborg': 1,\n",
       " 'psv': 1,\n",
       " 'eindhoven': 1,\n",
       " 'landskrona': 1,\n",
       " 'boi': 1,\n",
       " 'norrköp': 1,\n",
       " 'örgryte': 1,\n",
       " 'sweden': 2,\n",
       " '1978': 3,\n",
       " 'soccer': 1,\n",
       " '1968–1984': 1,\n",
       " 'indoor': 2,\n",
       " 'minnesota': 3,\n",
       " 'kick': 1,\n",
       " 'centuri': 4,\n",
       " 'club': 4,\n",
       " 'netherland': 3,\n",
       " 'hallsberg': 1,\n",
       " 'municip': 3,\n",
       " 'alexisonfir': 1,\n",
       " 'punk': 2,\n",
       " 'singer': 1,\n",
       " 'musician': 2,\n",
       " 'date': 1,\n",
       " 'leicest': 2,\n",
       " 'aik': 1,\n",
       " 'fotbol': 1,\n",
       " 'sommar': 1,\n",
       " 'radio': 3,\n",
       " 'program': 2,\n",
       " 'host': 2,\n",
       " '2000': 3,\n",
       " '2004': 3,\n",
       " 'djurgården': 1,\n",
       " 'brommapojkarna': 1,\n",
       " 'categor': 1,\n",
       " 'posit': 1,\n",
       " 'brazilian': 3,\n",
       " 'femal': 1,\n",
       " 'child': 1,\n",
       " 'putney': 1,\n",
       " '1777': 2,\n",
       " '1836': 1,\n",
       " 'leith': 1,\n",
       " 'family|john': 1,\n",
       " 'meath': 1,\n",
       " 'hospit': 2,\n",
       " '1949': 3,\n",
       " 'älmhult': 1,\n",
       " '1990': 3,\n",
       " 'jönköping': 1,\n",
       " 'södra': 1,\n",
       " 'norway': 2,\n",
       " 'vålerenga': 1,\n",
       " 'fotbal': 1,\n",
       " 'lyn': 1,\n",
       " 'al-wasl': 1,\n",
       " '1955': 2,\n",
       " 'weapon': 1,\n",
       " 'design': 2,\n",
       " 'knife': 1,\n",
       " 'maker': 2,\n",
       " 'businesspeopl': 2,\n",
       " 'kyokushin': 1,\n",
       " 'kaikan': 1,\n",
       " 'practition': 2,\n",
       " 'jiu-jitsu': 1,\n",
       " 'wisconsin': 3,\n",
       " 'ndash': 1,\n",
       " 'la': 3,\n",
       " 'karateka': 1,\n",
       " 'iceland': 1,\n",
       " 'footballers|stefan': 1,\n",
       " 'gislason': 1,\n",
       " 'under-21': 1,\n",
       " 'youth': 3,\n",
       " 'arsen': 1,\n",
       " 'knattspyrnufélag': 1,\n",
       " 'reykjavíkur': 1,\n",
       " 'players|stefan': 1,\n",
       " 'strømsgodset': 1,\n",
       " 'toppfotbal': 1,\n",
       " 'grazer': 1,\n",
       " 'ak': 1,\n",
       " 'knattspyrnudeild': 1,\n",
       " 'keflavík': 1,\n",
       " 'brøndbi': 1,\n",
       " 'superliga': 1,\n",
       " 'belgian': 1,\n",
       " 'first': 1,\n",
       " 'divis': 3,\n",
       " 'oud-heverle': 1,\n",
       " 'leuven': 1,\n",
       " 'breiðablik': 1,\n",
       " 'ubk': 1,\n",
       " 'eliteserien': 1,\n",
       " 'norwegian': 1,\n",
       " 'austrian': 1,\n",
       " 'bundesliga': 1,\n",
       " 'england': 4,\n",
       " 'england|stefan': 1,\n",
       " 'norway|stefan': 1,\n",
       " 'austria': 3,\n",
       " 'austria|stefan': 1,\n",
       " 'denmark': 2,\n",
       " 'denmark|stefan': 1,\n",
       " 'belgium': 1,\n",
       " 'belgium|stefan': 1,\n",
       " 'úrvalsdeild': 1,\n",
       " 'karla': 1,\n",
       " '1.': 1,\n",
       " 'deild': 1,\n",
       " 'lommel': 1,\n",
       " 's.k': 1,\n",
       " '1935': 3,\n",
       " 'profession': 4,\n",
       " 'wrestl': 2,\n",
       " 'execut': 2,\n",
       " 'wwe': 1,\n",
       " '1971': 3,\n",
       " 'rosenborg': 1,\n",
       " 'bk': 1,\n",
       " 'borussia': 1,\n",
       " 'dortmund': 1,\n",
       " 'fk': 1,\n",
       " 'bodø/glimt': 1,\n",
       " '1981': 3,\n",
       " 'del': 1,\n",
       " 'plata': 2,\n",
       " 'basingstok': 1,\n",
       " 'town': 3,\n",
       " 'wycomb': 1,\n",
       " 'peterborough': 1,\n",
       " 'lincoln': 1,\n",
       " 'crawley': 1,\n",
       " 'naturalis': 1,\n",
       " 'citizen': 1,\n",
       " 'itali': 2,\n",
       " 'whitehawk': 1,\n",
       " 'eastbourn': 1,\n",
       " 'borough': 3,\n",
       " '1889': 3,\n",
       " 'indiana': 3,\n",
       " 'detroit': 1,\n",
       " 'indianapoli': 1,\n",
       " 'duluth': 1,\n",
       " 'white': 2,\n",
       " 'sox': 2,\n",
       " 'paul': 2,\n",
       " 'saint': 2,\n",
       " 'aa': 1,\n",
       " 'winona': 1,\n",
       " 'pirat': 2,\n",
       " 'taller': 1,\n",
       " 'remedio': 1,\n",
       " 'escalada': 1,\n",
       " 'deportivo': 1,\n",
       " 'español': 1,\n",
       " 'race': 2,\n",
       " 'avellaneda': 1,\n",
       " 'u.s.': 3,\n",
       " 'cremones': 1,\n",
       " 'spain': 4,\n",
       " 'switzerland': 4,\n",
       " 'ac': 1,\n",
       " 'bellinzona': 1,\n",
       " 'locarno': 1,\n",
       " '1953': 3,\n",
       " 'indian': 3,\n",
       " 'pharmaceut': 1,\n",
       " 'industri': 3,\n",
       " 'biotechnologist': 1,\n",
       " 'padma': 1,\n",
       " 'bhushan': 1,\n",
       " 'shri': 1,\n",
       " 'trade': 3,\n",
       " 'businesswomen': 1,\n",
       " 'karnataka': 2,\n",
       " 'australia': 4,\n",
       " 'biologist': 1,\n",
       " 'scientist': 1,\n",
       " 'give': 1,\n",
       " 'pledger': 1,\n",
       " 'billionair': 1,\n",
       " 'nikkei': 1,\n",
       " 'asia': 4,\n",
       " 'prize': 2,\n",
       " 'mount': 4,\n",
       " 'carmel': 1,\n",
       " '1945': 2,\n",
       " '1997': 4,\n",
       " 'model': 2,\n",
       " 'ojibw': 1,\n",
       " 'emphysema': 1,\n",
       " 'uc': 1,\n",
       " 'berkeley': 3,\n",
       " 'natur': 3,\n",
       " 'resourc': 3,\n",
       " 'john': 2,\n",
       " 'f.': 2,\n",
       " 'kennedi': 2,\n",
       " 'govern': 3,\n",
       " 'busi': 3,\n",
       " 'comput': 3,\n",
       " 'system': 2,\n",
       " 'sun': 2,\n",
       " 'microsystem': 1,\n",
       " 'kleiner': 1,\n",
       " 'perkin': 1,\n",
       " 'caufield': 1,\n",
       " 'byer': 1,\n",
       " '1872': 2,\n",
       " '1937': 2,\n",
       " 'socialit': 1,\n",
       " 'mccormick': 1,\n",
       " 'japanes': 2,\n",
       " 'voic': 1,\n",
       " 'actress': 1,\n",
       " 'pop': 2,\n",
       " 'avex': 1,\n",
       " 'kure': 1,\n",
       " 'hiroshima': 1,\n",
       " 'prefectur': 2,\n",
       " '1782': 1,\n",
       " '1846': 1,\n",
       " 'leonardtown': 1,\n",
       " 'georgetown': 1,\n",
       " 'mari': 1,\n",
       " 'jesuit': 2,\n",
       " 'bishop': 1,\n",
       " 'archdioces': 2,\n",
       " 'holi': 2,\n",
       " 'burial': 1,\n",
       " 'pastor': 1,\n",
       " 'triniti': 3,\n",
       " 'd.c': 3,\n",
       " 'folk': 1,\n",
       " 'essenti': 2,\n",
       " '1994': 4,\n",
       " 'west': 3,\n",
       " 'emirati': 1,\n",
       " '1968': 2,\n",
       " 'mumbai': 2,\n",
       " 'cheshir': 2,\n",
       " 'arab': 1,\n",
       " 'emir': 2,\n",
       " 'alappuzha': 1,\n",
       " 'district': 3,\n",
       " 'kniksen': 1,\n",
       " '1969': 2,\n",
       " 'kongsving': 1,\n",
       " 'il': 1,\n",
       " 'odd': 1,\n",
       " 'moss': 1,\n",
       " 'goalkeep': 1,\n",
       " 'archbishop': 1,\n",
       " 'armagh': 2,\n",
       " 'particip': 2,\n",
       " 'second': 2,\n",
       " 'vatican': 1,\n",
       " 'irish': 2,\n",
       " 'cardin': 1,\n",
       " '1913': 3,\n",
       " '1977': 2,\n",
       " 'ireland': 4,\n",
       " 'religi': 3,\n",
       " 'leader': 1,\n",
       " 'belfast': 1,\n",
       " 'brother': 2,\n",
       " 'grammar': 2,\n",
       " 'creat': 1,\n",
       " 'pope': 2,\n",
       " 'vi': 1,\n",
       " 'post-reform': 1,\n",
       " 'ebolowa': 1,\n",
       " 'cameroonian': 1,\n",
       " 'cameroon': 1,\n",
       " 'under-20': 1,\n",
       " 'african': 2,\n",
       " 'olymp': 3,\n",
       " 'gold': 1,\n",
       " 'medalist': 1,\n",
       " 'summer': 3,\n",
       " 'liga': 1,\n",
       " 'isra': 2,\n",
       " 'premier': 1,\n",
       " 'segunda': 1,\n",
       " 'división': 1,\n",
       " 'b': 2,\n",
       " 'coventri': 2,\n",
       " 'sheffield': 2,\n",
       " 'nant': 1,\n",
       " 'maccabi': 1,\n",
       " 'petah': 1,\n",
       " 'tikva': 1,\n",
       " 'ashdod': 1,\n",
       " 'cd': 1,\n",
       " 'numancia': 1,\n",
       " 'puertollano': 1,\n",
       " 'barcelona': 1,\n",
       " 'dubai': 1,\n",
       " 'csc': 1,\n",
       " 'al': 1,\n",
       " 'hilal': 1,\n",
       " 'sfc': 1,\n",
       " 'wrexham': 1,\n",
       " 'a.f.c': 1,\n",
       " 'alvi': 1,\n",
       " 'saudi': 1,\n",
       " 'arabia': 2,\n",
       " 'israel': 2,\n",
       " 'malaysia': 3,\n",
       " 'kingdom': 4,\n",
       " 'uae': 1,\n",
       " 'pro': 1,\n",
       " '1946': 4,\n",
       " '2009': 2,\n",
       " 'puppet': 1,\n",
       " 'kidney': 1,\n",
       " 'cancer': 2,\n",
       " 'cañada': 1,\n",
       " 'flintridg': 1,\n",
       " 'chester': 1,\n",
       " 'pennsylvania': 3,\n",
       " '1907': 2,\n",
       " '2007': 3,\n",
       " 'law': 4,\n",
       " 'jewish': 2,\n",
       " 'songwrit': 1,\n",
       " 'jew': 3,\n",
       " 'tisch': 1,\n",
       " 'art': 4,\n",
       " 'newark': 1,\n",
       " 'jersey': 3,\n",
       " 'bronx': 2,\n",
       " 'uncertain': 1,\n",
       " 'dj': 1,\n",
       " 'tranc': 1,\n",
       " 'sint-niklaa': 1,\n",
       " 'danc': 2,\n",
       " '1993': 2,\n",
       " 'hip': 2,\n",
       " 'dub': 3,\n",
       " 'nois': 2,\n",
       " 'alabama': 3,\n",
       " 'assassin': 2,\n",
       " 'board': 2,\n",
       " '1894': 3,\n",
       " '1954': 3,\n",
       " 'tallapoosa': 1,\n",
       " 'georgia': 3,\n",
       " 'murder': 2,\n",
       " 'firearm': 2,\n",
       " 'croix': 1,\n",
       " 'guerr': 1,\n",
       " '1914–1918': 1,\n",
       " 'jacksonvil': 2,\n",
       " 'anti-corrupt': 1,\n",
       " '1899': 3,\n",
       " '2005': 3,\n",
       " 'centenarian': 1,\n",
       " 'ii': 3,\n",
       " 'sailor': 1,\n",
       " 'offic': 3,\n",
       " '2010': 3,\n",
       " 'crimin': 1,\n",
       " 'drug': 1,\n",
       " 'traffick': 1,\n",
       " 'victim': 1,\n",
       " 'organis': 3,\n",
       " 'crime': 3,\n",
       " 'figur': 1,\n",
       " 'convict': 3,\n",
       " 'die': 1,\n",
       " 'prison': 2,\n",
       " 'custodi': 1,\n",
       " 'sentenc': 1,\n",
       " 'multipl': 1,\n",
       " 'life': 3,\n",
       " 'melbourn': 3,\n",
       " 'beat': 1,\n",
       " 'gangland': 1,\n",
       " 'kill': 2,\n",
       " 'victoria': 3,\n",
       " 'imprison': 1,\n",
       " 'detent': 1,\n",
       " 'dominican': 1,\n",
       " 'wrestler': 1,\n",
       " 'social': 2,\n",
       " 'institut': 4,\n",
       " 'bloc': 1,\n",
       " 'nwa': 1,\n",
       " 'heavyweight': 1,\n",
       " 'champion': 1,\n",
       " 'el': 1,\n",
       " 'seibo': 1,\n",
       " 'provinc': 2,\n",
       " 'canarian': 1,\n",
       " 'lebanes': 1,\n",
       " 'russian': 2,\n",
       " 'serbian': 1,\n",
       " 'talk': 2,\n",
       " 'show': 1,\n",
       " 'stomach': 1,\n",
       " '1925': 3,\n",
       " 'oklahoma': 2,\n",
       " 'canton': 2,\n",
       " 'terrier': 1,\n",
       " 'caribbean': 1,\n",
       " 'seri': 1,\n",
       " 'chicago': 4,\n",
       " 'colorect': 1,\n",
       " 'leon': 2,\n",
       " 'caraca': 2,\n",
       " 'louisvil': 2,\n",
       " 'colonel': 1,\n",
       " 'pitch': 1,\n",
       " 'milwauke': 1,\n",
       " 'brewer': 1,\n",
       " 'scout': 4,\n",
       " 'philadelphia': 3,\n",
       " 'philli': 1,\n",
       " 'loui': 3,\n",
       " 'brown': 1,\n",
       " 'scranton': 1,\n",
       " 'rainier': 1,\n",
       " 'cleveland': 2,\n",
       " 'blue': 2,\n",
       " 'jay': 1,\n",
       " 'tulsa': 1,\n",
       " 'oiler': 1,\n",
       " 'basketbal': 3,\n",
       " '1925–1955': 1,\n",
       " 'carbondal': 1,\n",
       " 'ace': 1,\n",
       " 'miner': 2,\n",
       " '1975': 3,\n",
       " 'turkey': 2,\n",
       " 'men': 2,\n",
       " 'hawk': 1,\n",
       " 'bull': 2,\n",
       " 'galatasaray': 1,\n",
       " 'warrior': 1,\n",
       " 'houston': 1,\n",
       " 'rocket': 1,\n",
       " 'draft': 1,\n",
       " 'pick': 1,\n",
       " 'idaho': 2,\n",
       " 'stamped': 1,\n",
       " 'cba': 1,\n",
       " 'isidor': 1,\n",
       " 'newman': 1,\n",
       " 'lsu': 1,\n",
       " 'mcdonald': 1,\n",
       " 'all-american': 1,\n",
       " 'hornet': 2,\n",
       " 'parad': 1,\n",
       " 'boy': 2,\n",
       " 'phoenix': 2,\n",
       " 'point': 2,\n",
       " 'guard': 3,\n",
       " 'superson': 1,\n",
       " 'sioux': 1,\n",
       " 'fall': 2,\n",
       " 'skyforc': 1,\n",
       " 'utah': 3,\n",
       " 'jazz': 1,\n",
       " 'hous': 4,\n",
       " 'repres': 2,\n",
       " 'republican': 1,\n",
       " 'ibm': 1,\n",
       " 'employe': 1,\n",
       " 'omaha': 1,\n",
       " 'princeton': 1,\n",
       " 'stanford': 1,\n",
       " 'graduat': 1,\n",
       " 'america': 4,\n",
       " '1926': 1,\n",
       " 'michigan': 3,\n",
       " 'buffalo': 1,\n",
       " 'bison': 1,\n",
       " 'cub': 1,\n",
       " 'dallas-fort': 1,\n",
       " 'worth': 3,\n",
       " 'memphi': 1,\n",
       " 'chickasaw': 1,\n",
       " 'brave': 1,\n",
       " 'giant': 1,\n",
       " 'nl': 1,\n",
       " 'oneonta': 1,\n",
       " 'pittsburgh': 3,\n",
       " 'roanok': 1,\n",
       " 'wellsvil': 1,\n",
       " 'nitro': 1,\n",
       " 'wichita': 1,\n",
       " '1928': 3,\n",
       " '2016': 3,\n",
       " 'all-star': 1,\n",
       " 'paso': 1,\n",
       " 'texan': 1,\n",
       " 'athlet': 3,\n",
       " 'shortstop': 1,\n",
       " 'jose': 2,\n",
       " 'francisco': 4,\n",
       " 'tamalpai': 1,\n",
       " 'ludvika': 1,\n",
       " 'singer-songwrit': 1,\n",
       " 'minden': 1,\n",
       " 'bossier': 1,\n",
       " 'grammi': 1,\n",
       " 'ole': 1,\n",
       " 'opri': 1,\n",
       " 'corpor': 3,\n",
       " 'rca': 1,\n",
       " 'victor': 2,\n",
       " 'epic': 1,\n",
       " 'intracrani': 1,\n",
       " 'aneurysm': 1,\n",
       " 'mexican': 2,\n",
       " 'oregon': 2,\n",
       " 'duck': 1,\n",
       " 'track': 2,\n",
       " 'field': 2,\n",
       " 'runner': 1,\n",
       " 'hammond': 1,\n",
       " 'tight': 1,\n",
       " ...}"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DF = dfcount(categories_processed)\n",
    "DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8641"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(DF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_idf(data, doc_freq):\n",
    "    tfidf = {}\n",
    "    for i in range(len(data)):\n",
    "        counter = Counter(data[i])\n",
    "        count_w = len(data[i])\n",
    "        for token in np.unique(data[i]):\n",
    "            tf = counter[token]/count_w\n",
    "            df = doc_freq[token]\n",
    "            idf = np.log((len(data)+1)/(df+1))\n",
    "            tfidf[i, token] = tf*idf\n",
    "    return tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = tf_idf(categories_processed, DF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_person = {term:x for (doc, term), x in tmp.items() if doc == 0}\n",
    "c_person = sorted(c_person, key=c_person.__getitem__,reverse=True)\n",
    "\n",
    "c_company = {term:x for (doc, term), x in tmp.items() if doc == 1}\n",
    "c_company = sorted(c_company, key=c_company.__getitem__,reverse=True)\n",
    "\n",
    "c_org = {term:x for (doc, term), x in tmp.items() if doc == 2}\n",
    "c_org = sorted(c_org, key=c_org.__getitem__,reverse=True)\n",
    "\n",
    "c_location = {term:x for (doc, term), x in tmp.items() if doc == 3}\n",
    "c_location = sorted(c_location, key=c_location.__getitem__, reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['player', 'expatri', 'death', 'sportspeopl', 'male', 'actor', 'birth', 'alumni', 'writer', 'cricket', 'footbal', 'actress', 'singer', 'fc', 'f.c', '21st-centuri', 'coach', 'medalist', 'soccer', 'hockey']\n",
      "['exchang', 'manufactur', 'brand', 'defunct', 'acquisit', 'merger', 'label', 'stock', 'chain', 'retail', 'softwar', 'publish', 'pipelin', 'cloth', 'vehicl', 'fast-food', 'motor', 'disestablish', 'record', 'oil']\n",
      "['organis', 'chariti', 'non-profit', 'parti', 'manama', 'tobago', 'ambul', 'gang', 'learn', 'youth', 'são', '501', 'country|bel', 'event', 'slough', 'treati', 'guid', 'nation', 'advocaci', 'claro']\n",
      "['pyrénées-atlantiqu', 'regist', 'station', 'airport', 'complet', 'aerodrom', 'commun', 'need', 'popul', 'counti', 'river', 'railway', 'unincorpor', 'villag', 'venu', 'build', 'french', 'place', 'translat', 'leicestershir']\n"
     ]
    }
   ],
   "source": [
    "print(c_person[:20])\n",
    "print(c_company[:20])\n",
    "print(c_org[:20])\n",
    "print(c_location[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "import nltk\n",
    "from collections import Counter\n",
    "import threading\n",
    "\n",
    "def getSignificanteCategories(limit=2000, write=True):\n",
    "    categories = []\n",
    "\n",
    "    data_cat = parseWiki(limit=limit ,test_sample=False, save=False)\n",
    "    categories.append(reduce(lambda x,y: x+preprocess(y),reduce(lambda x,y: x+y[2]['categories'],filter(lambda x: x[3] == 'Person',data_cat),[]),[]))\n",
    "    categories.append(reduce(lambda x,y: x+preprocess(y),reduce(lambda x,y: x+y[2]['categories'],filter(lambda x: x[3] in ['Company', 'A_Company'],data_cat),[]),[]))\n",
    "    categories.append(reduce(lambda x,y: x+preprocess(y),reduce(lambda x,y: x+y[2]['categories'],filter(lambda x: x[3] in ['Organization'],data_cat),[]),[]))\n",
    "    categories.append(reduce(lambda x,y: x+preprocess(y),reduce(lambda x,y: x+y[2]['categories'],filter(lambda x: x[3] in ['Location', 'A_Location'],data_cat),[]),[]))\n",
    "    \n",
    "    del data_cat\n",
    "    \n",
    "    DF = dfcount(categories)\n",
    "    tfidf = tf_idf(categories, DF)\n",
    "    \n",
    "    c_person = None\n",
    "    c_company = None\n",
    "    c_org = None\n",
    "    c_location = None\n",
    "    \n",
    "    def task1():\n",
    "        c_person = {term:x for (doc, term), x in tfidf.items() if doc == 0}\n",
    "        c_person = sorted(c_person, key=c_person.__getitem__,reverse=True)\n",
    "        print('Person: ', c_person[:20])\n",
    "\n",
    "    def task2():\n",
    "        c_company = {term:x for (doc, term), x in tfidf.items() if doc == 1}\n",
    "        c_company = sorted(c_company, key=c_company.__getitem__,reverse=True)\n",
    "        print('Company: ', c_company[:20])\n",
    "\n",
    "    def task3():\n",
    "        c_org = {term:x for (doc, term), x in tfidf.items() if doc == 2}\n",
    "        c_org = sorted(c_org, key=c_org.__getitem__,reverse=True)\n",
    "        print('Organisation: ', c_org[:20])\n",
    "\n",
    "    def task4():\n",
    "        c_location = {term:x for (doc, term), x in tfidf.items() if doc == 3}\n",
    "        c_location = sorted(c_location, key=c_location.__getitem__, reverse=True)\n",
    "        print('Location: ', c_location[:20])\n",
    "    \n",
    "    t1 = threading.Thread(target=task1, name='t1') \n",
    "    t2 = threading.Thread(target=task2, name='t2') \n",
    "    t3 = threading.Thread(target=task3, name='t3') \n",
    "    t4 = threading.Thread(target=task4, name='t4')\n",
    "    \n",
    "    t1.start()\n",
    "    t2.start()\n",
    "    t3.start()\n",
    "    t4.start()\n",
    "    \n",
    "    \n",
    "    t1.join()\n",
    "    t2.join()\n",
    "    t3.join()\n",
    "    t4.join()\n",
    "    \n",
    "    if write:\n",
    "        print('Person: ', c_person[:20])\n",
    "        print('')\n",
    "        print('Company: ', c_company[:20])\n",
    "        print('')\n",
    "        print('Organisation: ', c_org[:20])\n",
    "        print('')\n",
    "        print('Location: ', c_location[:20])\n",
    "    \n",
    "    return {'person': c_person, 'company': c_company, 'org': c_org, 'location': c_location}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Person:  ['player', 'birth', 'male', 'death', 'expatri', 'peopl', 'alumni', 'live', 'sportspeopl', 'actor', 'writer', 'descent', 'footbal', '21st-centuri', 'cricket', '20th-centuri', 'singer', 'politician', 'musician', 'actress']\n",
      "Company:  ['exchang', 'brand', 'acquisit', 'merger', 'defunct', 'manufactur', 'softwar', 'label', 'cloth', 'vehicl', 'retail', 'disestablish', 'restaur', 'video', 'fast-food', 'nasdaq', 'onlin', 'publish', 'chain', 'stock']\n",
      "Organisation: Location:  ['station', 'build', 'pyrénées-atlantiqu', 'regist', 'popul', 'complet', 'place', 'airport', 'venu', 'town', 'school', 'aerodrom', 'need', 'counti', 'railway', 'villag', 'unincorpor', 'great', 'mountain', 'open']\n",
      " ['sahara', 'scout', 'youth', '501', 'gang', 'non-profit', 'polisario', 'chariti', 'c', 'learn', 'polit', 'societi', 'advocaci', 'ambul', 'anti-christian', 'anti-vaccin', 'child-rel', 'kazakhstan', 'multi-sport', 'non-government']\n"
     ]
    }
   ],
   "source": [
    "res = getSignificanteCategories(limit=30000, write=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-74-de3302d29ff4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Person: '\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'person'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Company: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'company'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Organisation: '\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'org'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "print('Person: ',res['person'][:100])\n",
    "print('')\n",
    "print('Company: ', res['company'][:100])\n",
    "print('')\n",
    "print('Organisation: ',res['org'][:100])\n",
    "print('')\n",
    "print('Location: ', res['location'][:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "300 000 articles cca 45 minutes need refactoring\n",
    "\n",
    "Person=['player', 'male', 'actor', 'sportspeopl', 'medalist', 'actress', 'expatri', 'singer', 'musician', 'live', 'writer', 'politician', 'f.c', 'alumni', 'personnel', 'olymp', '20th-centuri', 'faculti', 'coach', 'guitarist']\n",
    "\n",
    "Company=['brand', 'merger', 'retail', 'exchang', 'stock', 'label', 'nasdaq', 'multin', 'subsidiari', 'acquisit', 'onlin', 'offer', 'held', 'conglomer', 'drink', 'vehicl', 'softwar', 'equip', 'store', 'bankruptci']\n",
    "\n",
    "Organisation=['scout', 'think', 'non-profit', 'girl', 'gang', 'multi-sport', 'event', 'recur', 'religi', 'tank', 'child-rel', 'non-align', 'non-government', 'critic', 'right', 'chess', 'evangel', 'movement|', 'yakuza', 'advocaci']\n",
    "\n",
    "Location=['regist', 'unincorpor', 'station', 'popul', 'complet', 'aerodrom', 'villag', 'town', 'landform', 'parish', 'river', 'seaplan', 'open', 'census-design', 'mountain', 'attract', 'neighbourhood', 'suburb', 'rang', 'airport']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "300 000 first 100\n",
    "Person:  ['player', 'male', 'actor', 'sportspeopl', 'medalist', 'actress', 'expatri', 'singer', 'musician', 'live', 'writer', 'politician', 'f.c', 'alumni', 'personnel', 'olymp', '20th-centuri', 'faculti', 'coach', 'guitarist', 'basebal', 'novelist', 'emigr', 'descent', 'cup', '21st-centuri', 'mp', 'painter', 'femal', 'journalist', 'poet', 'compos', 'draft', 'pick', 'repres', '19th-centuri', 'summer', 'champion', 'screenwrit', 'lawyer', 'director', 'swimmer', 'soccer', 'forward', 'skater', 'burial', 'midfield', 'field', 'ice', 'gold', 'non-fict', 'basketbal', 'winter', 'recipi', 'comedian', 'fifa', 'filipino', 'businesspeopl', 'defend', 'senat', 'silver', 'major', 'songwrit', 'scientist', 'minist', 'medal', 'fc', 'medallist', 'staff', 'singer-songwrit', 'voic', 'scholar', 'fellow', 'boxer', 'wrestler', 'historian', 'pan', 'drummer', 'universiad', 'rock', 'figur', 'bundesliga', 'cemeteri', 'rugbi', 'bronz', 'pianist', 'dramatist', 'merit', 'playwright', 'cyclist', 'stage', 'inducte', 'mayor', 'under-21', 'activist', 'xi', 'republican', 'first', 'governor', 'presid']\n",
    "\n",
    "Company:  ['brand', 'merger', 'retail', 'exchang', 'stock', 'label', 'nasdaq', 'multin', 'subsidiari', 'acquisit', 'onlin', 'offer', 'held', 'conglomer', 'drink', 'vehicl', 'softwar', 'equip', 'store', 'bankruptci', 'file', 'cloth', 'non-renew', 'chapter', 'shoe', 'supermarket', 'initi', 'formerli', 'properti', 'publish', 'portfolio', 'chain', 'supplier', 'chocol', 'luxuri', 'tokyo', 'equiti', 'phone', 'applianc', 'part', 'ga', 'motor', 'truck', 'bakeri', 'group|', 'midwestern', 'toy', 'housebuild', 'web', 'hold', 'fashion', 'headquart', 'studio', 'breweri', '11', 'government-own', 'snack', 'spin-off', 'energi', 'fast-food', 'oil', 'pharmaceut', 'amplifi', 'eyewear', 'nationalis', 'encyclopedia', '2010', 'resourc', 'discontinu', 'euronext', 'outsourc', 'r.a', 're-establish', 'guitar', 'colorado', '2017', 'magazin', 'mobil', 'firearm', 'googl', 'warrant', '2008', 'indiana', 'pipelin', 'provid', 'chaebol', 'condiment', 'dairi', 'discount', 'index', 'mortgag', 'poultri', 'coffe', 'cosmet', 'distribut', 'fuel', '2020', 'consult', 'rock', 'station']\n",
    "\n",
    "Organisation:  ['scout', 'think', 'non-profit', 'girl', 'gang', 'multi-sport', 'event', 'recur', 'religi', 'tank', 'child-rel', 'non-align', 'non-government', 'critic', 'right', 'chess', 'evangel', 'movement|', 'yakuza', 'advocaci', 'patronag', 'usa', 'games|', 'sahara', 'accreditor', 'america|', 'associations|', 'association|', 'hispanic-american', 'ioc-recognis', 'lobbi', 'metalwork', 'polisario', 'supraorgan', '501', 'bolivia', 'femin', 'intergovernment', 'secret', 'traffick', 'learn', 'asian', 'publish', 'ambul', 'anti-abort', 'anti-vaccin', 'consortia', 'feminist', 'parachurch', 'shelter', 'veteran', 'diego', 'adi', 'advaita', 'anti-vivisect', 'awards|', 'caloust', 'churches|thailand', 'education|', 'federation|', 'foundation|', 'genet', 'gmb', 'gulbenkian', 'irredent', 'metric', 'pageants|california', 'philanthrop', 'positiv', 'puri', 'shankara', 'shankaracharya', 'states–european', 'sub-confeder', 'taxat', 'treati', 'trust|', 'vedanta', 'vexillolog', 'center', 'confeder', 'local', 'nebraska', 'olymp', 'anglican', 'denomin', 'labor', 'missionari', 'scientolog', 'welfar', '1778', 'activist', 'anti-christian', 'biblic', 'carpent', 'certif', 'combat', 'emerg', 'homeless', 'israeli–palestinian']\n",
    "\n",
    "Location:  ['regist', 'unincorpor', 'station', 'popul', 'complet', 'aerodrom', 'villag', 'town', 'landform', 'parish', 'river', 'seaplan', 'open', 'census-design', 'mountain', 'attract', 'neighbourhood', 'suburb', 'rang', 'airport', 'certifi', 'secondari', 'district|', 'site', 'skyscrap', 'pyrénées-atlantiqu', 'basketbal', 'stadium', 'demolish', 'need', 'vaud', 'coast', 'tributari', 'arena', 'neighborhood', 'dam', 'tunnel', 'saskatchewan', 'monument', 'serv', 'multi-purpos', 'mall', 'lighthous', 'pradesh', 'locat', 'volcano', 'norfolk', 'coastal', 'mojav', 'territori', 'canton', 'township', 'subprefectur', 'desert', 'volleybal', 'derbyshir', 'grassland', 'hill', 'censu', 'castl', 'casino', 'landmark', 'governor', 'voivodeship', 'glacier', 'line', 'valley', 'residenti', 'subway', 'nova', 'colorado', 'close', 'scotia', 'princ', 'reservoir', 'grade', 'offic', 'properti', 'abellio', 'scotrail', 'local', 'indoor', 'lrt', 'uninhabit', 'metropolitan', 'oklahoma', 'suffolk', 'wikipedia', 'montana', 'translat', 'cumbria', 'indiana', 'dioces', 'sculptur', 'divis', 'punggol', 'navarr', 'instal', 'reserv', 'verd']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test and data searching area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "import seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>David Stagg</td>\n",
       "      <td>{Infobox rugby league biography\\n|name        ...</td>\n",
       "      <td>{'type': 'rugby league biography', 'parameters...</td>\n",
       "      <td>Person</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Amaranthus mantegazzianus</td>\n",
       "      <td>redirect</td>\n",
       "      <td>Amaranthus caudatus</td>\n",
       "      <td>redirect::Amaranthus caudatus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Amaranthus quitensis</td>\n",
       "      <td>redirect</td>\n",
       "      <td>Amaranthus hybridus</td>\n",
       "      <td>redirect::Amaranthus hybridus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Maud Queen of Norway</td>\n",
       "      <td>redirect</td>\n",
       "      <td>Maud of Wales</td>\n",
       "      <td>redirect::Maud of Wales</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Milligram per litre</td>\n",
       "      <td>redirect</td>\n",
       "      <td>Gram per litre</td>\n",
       "      <td>redirect::Gram per litre</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Utica Psychiatric Center</td>\n",
       "      <td>{Infobox NRHP | name =Utica State Hospital, Ma...</td>\n",
       "      <td>{'type': 'nrhp', 'parameters': ['name', 'nrhp_...</td>\n",
       "      <td>Location</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Olean Wholesale Grocery</td>\n",
       "      <td>no infobox/redirect</td>\n",
       "      <td>{'categories': ['Companies based in Cattaraugu...</td>\n",
       "      <td>C_Company</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Queen Tiye</td>\n",
       "      <td>redirect</td>\n",
       "      <td>Tiye</td>\n",
       "      <td>redirect::Tiye</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Queen Hatshepsut</td>\n",
       "      <td>redirect</td>\n",
       "      <td>Hatshepsut</td>\n",
       "      <td>redirect::Hatshepsut</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Clibanarii</td>\n",
       "      <td>no infobox/redirect</td>\n",
       "      <td>{'categories': ['Cavalry', 'Asian armour', 'Ty...</td>\n",
       "      <td>Other/None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           0  \\\n",
       "0                David Stagg   \n",
       "1  Amaranthus mantegazzianus   \n",
       "2       Amaranthus quitensis   \n",
       "3       Maud Queen of Norway   \n",
       "4        Milligram per litre   \n",
       "5   Utica Psychiatric Center   \n",
       "6    Olean Wholesale Grocery   \n",
       "7                 Queen Tiye   \n",
       "8           Queen Hatshepsut   \n",
       "9                 Clibanarii   \n",
       "\n",
       "                                                   1  \\\n",
       "0  {Infobox rugby league biography\\n|name        ...   \n",
       "1                                           redirect   \n",
       "2                                           redirect   \n",
       "3                                           redirect   \n",
       "4                                           redirect   \n",
       "5  {Infobox NRHP | name =Utica State Hospital, Ma...   \n",
       "6                                no infobox/redirect   \n",
       "7                                           redirect   \n",
       "8                                           redirect   \n",
       "9                                no infobox/redirect   \n",
       "\n",
       "                                                   2  \\\n",
       "0  {'type': 'rugby league biography', 'parameters...   \n",
       "1                                Amaranthus caudatus   \n",
       "2                                Amaranthus hybridus   \n",
       "3                                      Maud of Wales   \n",
       "4                                     Gram per litre   \n",
       "5  {'type': 'nrhp', 'parameters': ['name', 'nrhp_...   \n",
       "6  {'categories': ['Companies based in Cattaraugu...   \n",
       "7                                               Tiye   \n",
       "8                                         Hatshepsut   \n",
       "9  {'categories': ['Cavalry', 'Asian armour', 'Ty...   \n",
       "\n",
       "                               3  \n",
       "0                         Person  \n",
       "1  redirect::Amaranthus caudatus  \n",
       "2  redirect::Amaranthus hybridus  \n",
       "3        redirect::Maud of Wales  \n",
       "4       redirect::Gram per litre  \n",
       "5                       Location  \n",
       "6                      C_Company  \n",
       "7                 redirect::Tiye  \n",
       "8           redirect::Hatshepsut  \n",
       "9                     Other/None  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(data)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Other/None        79378\n",
       "Other             21720\n",
       "Person            20884\n",
       "C_Person          12142\n",
       "Location          11074\n",
       "C_Location         6246\n",
       "A_Location         4270\n",
       "C_Organization     3251\n",
       "C_Company          2491\n",
       "Company            1830\n",
       "A_Person           1591\n",
       "Organization        681\n",
       "A_Company             9\n",
       "Name: 3, dtype: int64"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[3].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "redirect\n"
     ]
    }
   ],
   "source": [
    "data_path = '/home/xminarikd/.keras/datasets/enwiki-20201001-pages-articles9.xml-p2936261p4045402.bz2'\n",
    "sample_data_path = '/home/xminarikd/Documents/VINF/data/sample_wiki_articles2.xml.bz2'\n",
    "# Object for handling xml\n",
    "handler = ContentHandler()\n",
    "\n",
    "# Parsing object\n",
    "parser = xml.sax.make_parser()\n",
    "parser.setContentHandler(handler)\n",
    "\n",
    "for i, line in enumerate(subprocess.Popen(['bzcat'], \n",
    "                         stdin = open(data_path), \n",
    "                         stdout = subprocess.PIPE).stdout):\n",
    "    parser.feed(line)\n",
    "    \n",
    "    if len(handler.output) > 20000:\n",
    "        break\n",
    "\n",
    "print(handler.output[2][1])\n",
    "#print(regex.search(exp_inf_type, infobox).group().strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.DataFrame(data)\n",
    "rr = df2.loc[df2[3] == 'Other/None']\n",
    "rr = rr.loc[rr[2] == {'categories':[]}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Speciesbox\n",
    "Citation\n",
    "Image\n",
    "div\n",
    "Licensing\n",
    "summary\n",
    "May refers to\n",
    "Use dmy dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [0, 1, 2, 3, 4]\n",
       "Index: []"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Shipping companies of the United States', 'Companies based in Virginia']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp5 = ['History of Atlanta',\n",
    "  'North Carolina in the American Civil War',\n",
    "  'Shipping companies of the United States',\n",
    "  'Companies based in Virginia']\n",
    "list(filter(lambda x: regex.search('(compan[y|ies])(?i)', x), temp5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ano\n"
     ]
    }
   ],
   "source": [
    "temp2 = ['ano','nie jasd sad', 'asdasdasd asd']\n",
    "temp3 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Organisations based in Manama']"
      ]
     },
     "execution_count": 322,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(filter(lambda x: regex.search('(organisations*|associations*)(?i)', x),temp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'p2936261p4045402'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tt = '/home/xminarikd/.keras/datasets/enwiki-20201001-pages-articles9.xml-p2936261p4045402.bz2'\n",
    "tt.split('/')[-1].split('-')[-1].split('.')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/xminarikd/Documents/VINF/data/sample_wiki_articles2.xml.bz2'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "dirname = os.getcwd().rsplit('/', 1)[0]\n",
    "dirname = f'{dirname}/data/sample_wiki_articles2.xml.bz2'\n",
    "dirname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/xminarikd/.keras/datasets/enwiki-20201001-pages-articles9.xml-p2936261p4045402.bz2'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tt = '/home/xminarikd/.keras/datasets/enwiki-20201001-pages-articles9.xml-p2936261p4045402.bz2'\n",
    "tt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
