{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vytvorenie slovníka dvojíc pre účely Named Entity Recognizing\n",
    "#### Creating a dictionary of pairs for the purposes of Named Entity Recognizing: Wiki page - type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Projekt je momentalne rozdeleny do 2 časti.\n",
    "\n",
    "1. časť tvorí stahovanie potrebných súborov(wikipedia dump) na účely spracovania v projekte.\n",
    "2. časť tvorí parsovanie súborov spolu s priradením kategorie jednotlivym clankom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Part : Downloading Wikipedia articles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import re\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stiahnutie dát zo stránky wikipédie. Vyfiltrovanie všetkých súborov, ktoré obsahujú v názve \"pages-articles\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<!DOCTYPE html '"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_url = 'https://dumps.wikimedia.org/enwiki/20201001/'\n",
    "base_html = requests.get(base_url).text\n",
    "base_html[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<li class=\"file\"><a href=\"/enwiki/20201001/enwiki-20201001-pages-articles-multistream.xml.bz2\">enwiki-20201001-pages-articles-multistream.xml.bz2</a> 17.5 GB</li>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup_dump = BeautifulSoup(base_html, 'html.parser')\n",
    "soup_dump.find_all('li', {'class': 'file'}, limit = 10)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('enwiki-20201001-pages-articles-multistream.xml.bz2', ['17.5', 'GB']),\n",
       " ('enwiki-20201001-pages-articles-multistream-index.txt.bz2', ['215.8', 'MB']),\n",
       " ('enwiki-20201001-pages-articles-multistream1.xml-p1p41242.bz2',\n",
       "  ['231.7', 'MB']),\n",
       " ('enwiki-20201001-pages-articles-multistream-index1.txt-p1p41242.bz2',\n",
       "  ['222', 'KB']),\n",
       " ('enwiki-20201001-pages-articles-multistream2.xml-p41243p151573.bz2',\n",
       "  ['313.2', 'MB'])]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files = []\n",
    "for file in soup_dump.find_all('li', {'class': 'file'}):\n",
    "    text = file.text\n",
    "    if 'pages-articles' in text:\n",
    "        files.append((text.split()[0], text.split()[1:]))\n",
    "files[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['enwiki-20201001-pages-articles1.xml-p1p41242.bz2',\n",
       " 'enwiki-20201001-pages-articles2.xml-p41243p151573.bz2',\n",
       " 'enwiki-20201001-pages-articles3.xml-p151574p311329.bz2',\n",
       " 'enwiki-20201001-pages-articles4.xml-p311330p558391.bz2',\n",
       " 'enwiki-20201001-pages-articles5.xml-p558392p958045.bz2']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files_to_download = [file[0] for file in files if re.search('pages-articles\\d{1,2}.xml-p',file[0])]\n",
    "files_to_download[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Použitie knižnice keras na stiahnutie týchto súborov/datasetu. Stiahnú sa len tie súbory, ktoré ešte nie sú stahnuté"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from keras.utils import get_file\n",
    "directory = '/home/xminarikd/.keras/datasets/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading\n",
      "Downloading data from https://dumps.wikimedia.org/enwiki/20201001/enwiki-20201001-pages-articles1.xml-p1p41242.bz2\n",
      "242098176/242093817 [==============================] - 330s 1us/step\n",
      "Downloading\n",
      "Downloading data from https://dumps.wikimedia.org/enwiki/20201001/enwiki-20201001-pages-articles2.xml-p41243p151573.bz2\n",
      "324780032/324777650 [==============================] - 445s 1us/step\n",
      "Downloading\n",
      "Downloading data from https://dumps.wikimedia.org/enwiki/20201001/enwiki-20201001-pages-articles3.xml-p151574p311329.bz2\n",
      "352124928/352119906 [==============================] - 313s 1us/step\n",
      "Downloading\n",
      "Downloading data from https://dumps.wikimedia.org/enwiki/20201001/enwiki-20201001-pages-articles4.xml-p311330p558391.bz2\n",
      "389988352/389987127 [==============================] - 372s 1us/step\n",
      "Downloading\n",
      "Downloading data from https://dumps.wikimedia.org/enwiki/20201001/enwiki-20201001-pages-articles5.xml-p558392p958045.bz2\n",
      "420814848/420806959 [==============================] - 471s 1us/step\n",
      "Downloading\n",
      "Downloading data from https://dumps.wikimedia.org/enwiki/20201001/enwiki-20201001-pages-articles6.xml-p958046p1483661.bz2\n",
      "450748416/450745879 [==============================] - 553s 1us/step\n",
      "Downloading\n",
      "Downloading data from https://dumps.wikimedia.org/enwiki/20201001/enwiki-20201001-pages-articles7.xml-p1483662p2134111.bz2\n",
      "462512128/462504094 [==============================] - 385s 1us/step\n",
      "Downloading\n",
      "Downloading data from https://dumps.wikimedia.org/enwiki/20201001/enwiki-20201001-pages-articles8.xml-p2134112p2936260.bz2\n",
      "471654400/471652241 [==============================] - 344s 1us/step\n",
      "Downloading\n",
      "Downloading data from https://dumps.wikimedia.org/enwiki/20201001/enwiki-20201001-pages-articles9.xml-p2936261p4045402.bz2\n",
      "512147456/512145263 [==============================] - 485s 1us/step\n",
      "Downloading\n",
      "Downloading data from https://dumps.wikimedia.org/enwiki/20201001/enwiki-20201001-pages-articles10.xml-p4045403p5399366.bz2\n",
      "502456320/502449820 [==============================] - 357s 1us/step\n",
      "Downloading\n",
      "Downloading data from https://dumps.wikimedia.org/enwiki/20201001/enwiki-20201001-pages-articles11.xml-p5399367p6899366.bz2\n",
      "486776832/486770345 [==============================] - 393s 1us/step\n",
      "Downloading\n",
      "Downloading data from https://dumps.wikimedia.org/enwiki/20201001/enwiki-20201001-pages-articles11.xml-p6899367p7054859.bz2\n",
      "46866432/46859027 [==============================] - 69s 1us/step\n",
      "Downloading\n",
      "Downloading data from https://dumps.wikimedia.org/enwiki/20201001/enwiki-20201001-pages-articles12.xml-p7054860p8554859.bz2\n",
      "404406272/404401272 [==============================] - 624s 2us/step\n",
      "Downloading\n",
      "Downloading data from https://dumps.wikimedia.org/enwiki/20201001/enwiki-20201001-pages-articles12.xml-p8554860p9172788.bz2\n",
      "164061184/164054426 [==============================] - 131s 1us/step\n",
      "Downloading\n",
      "Downloading data from https://dumps.wikimedia.org/enwiki/20201001/enwiki-20201001-pages-articles13.xml-p9172789p10672788.bz2\n",
      "330563584/330555755 [==============================] - 278s 1us/step\n",
      "Downloading\n",
      "Downloading data from https://dumps.wikimedia.org/enwiki/20201001/enwiki-20201001-pages-articles13.xml-p10672789p11659682.bz2\n",
      "229384192/229381099 [==============================] - 205s 1us/step\n",
      "Downloading\n",
      "Downloading data from https://dumps.wikimedia.org/enwiki/20201001/enwiki-20201001-pages-articles14.xml-p11659683p13159682.bz2\n",
      "393928704/393922075 [==============================] - 348s 1us/step\n",
      "Downloading\n",
      "Downloading data from https://dumps.wikimedia.org/enwiki/20201001/enwiki-20201001-pages-articles14.xml-p13159683p14324602.bz2\n",
      "273661952/273655291 [==============================] - 271s 1us/step\n",
      "Downloading\n",
      "Downloading data from https://dumps.wikimedia.org/enwiki/20201001/enwiki-20201001-pages-articles15.xml-p14324603p15824602.bz2\n",
      "355713024/355706040 [==============================] - 263s 1us/step\n",
      "Downloading\n",
      "Downloading data from https://dumps.wikimedia.org/enwiki/20201001/enwiki-20201001-pages-articles15.xml-p15824603p17324602.bz2\n",
      "307257344/307257124 [==============================] - 317s 1us/step\n",
      "Downloading\n",
      "Downloading data from https://dumps.wikimedia.org/enwiki/20201001/enwiki-20201001-pages-articles15.xml-p17324603p17460152.bz2\n",
      "28254208/28249729 [==============================] - 111s 4us/step\n",
      "Downloading\n",
      "Downloading data from https://dumps.wikimedia.org/enwiki/20201001/enwiki-20201001-pages-articles16.xml-p17460153p18960152.bz2\n",
      "336871424/336865577 [==============================] - 308s 1us/step\n",
      "Downloading\n",
      "Downloading data from https://dumps.wikimedia.org/enwiki/20201001/enwiki-20201001-pages-articles16.xml-p18960153p20460152.bz2\n",
      "314253312/314246115 [==============================] - 205s 1us/step\n",
      "Downloading\n",
      "Downloading data from https://dumps.wikimedia.org/enwiki/20201001/enwiki-20201001-pages-articles16.xml-p20460153p20570392.bz2\n",
      "22953984/22949874 [==============================] - 35s 2us/step\n",
      "Downloading\n",
      "Downloading data from https://dumps.wikimedia.org/enwiki/20201001/enwiki-20201001-pages-articles17.xml-p20570393p22070392.bz2\n",
      "351920128/351918604 [==============================] - 228s 1us/step\n",
      "Downloading\n",
      "Downloading data from https://dumps.wikimedia.org/enwiki/20201001/enwiki-20201001-pages-articles17.xml-p22070393p23570392.bz2\n",
      "362340352/362336803 [==============================] - 221s 1us/step\n",
      "Downloading\n",
      "Downloading data from https://dumps.wikimedia.org/enwiki/20201001/enwiki-20201001-pages-articles17.xml-p23570393p23716197.bz2\n",
      "40402944/40402767 [==============================] - 27s 1us/step\n",
      "Downloading\n",
      "Downloading data from https://dumps.wikimedia.org/enwiki/20201001/enwiki-20201001-pages-articles18.xml-p23716198p25216197.bz2\n",
      "375750656/375742870 [==============================] - 295s 1us/step\n",
      "Downloading\n",
      "Downloading data from https://dumps.wikimedia.org/enwiki/20201001/enwiki-20201001-pages-articles18.xml-p25216198p26716197.bz2\n",
      "347947008/347946542 [==============================] - 251s 1us/step\n",
      "Downloading\n",
      "Downloading data from https://dumps.wikimedia.org/enwiki/20201001/enwiki-20201001-pages-articles18.xml-p26716198p27121850.bz2\n",
      "87384064/87377512 [==============================] - 119s 1us/step\n",
      "Downloading\n",
      "Downloading data from https://dumps.wikimedia.org/enwiki/20201001/enwiki-20201001-pages-articles19.xml-p27121851p28621850.bz2\n",
      "337952768/337946504 [==============================] - 229s 1us/step\n",
      "Downloading\n",
      "Downloading data from https://dumps.wikimedia.org/enwiki/20201001/enwiki-20201001-pages-articles19.xml-p28621851p30121850.bz2\n",
      "297205760/297201089 [==============================] - 213s 1us/step\n",
      "Downloading\n",
      "Downloading data from https://dumps.wikimedia.org/enwiki/20201001/enwiki-20201001-pages-articles19.xml-p30121851p31308442.bz2\n",
      "281026560/281023102 [==============================] - 159s 1us/step\n",
      "Downloading\n",
      "Downloading data from https://dumps.wikimedia.org/enwiki/20201001/enwiki-20201001-pages-articles20.xml-p31308443p32808442.bz2\n",
      "383336448/383334873 [==============================] - 241s 1us/step\n",
      "Downloading\n",
      "Downloading data from https://dumps.wikimedia.org/enwiki/20201001/enwiki-20201001-pages-articles20.xml-p32808443p34308442.bz2\n",
      "349700096/349699080 [==============================] - 394s 1us/step\n",
      "Downloading\n",
      "Downloading data from https://dumps.wikimedia.org/enwiki/20201001/enwiki-20201001-pages-articles20.xml-p34308443p35522432.bz2\n",
      "259284992/259278058 [==============================] - 153s 1us/step\n",
      "Downloading\n",
      "Downloading data from https://dumps.wikimedia.org/enwiki/20201001/enwiki-20201001-pages-articles21.xml-p35522433p37022432.bz2\n",
      "351600640/351597304 [==============================] - 515s 1us/step\n",
      "Downloading\n",
      "Downloading data from https://dumps.wikimedia.org/enwiki/20201001/enwiki-20201001-pages-articles21.xml-p37022433p38522432.bz2\n",
      "340942848/340936052 [==============================] - 502s 1us/step\n",
      "Downloading\n",
      "Downloading data from https://dumps.wikimedia.org/enwiki/20201001/enwiki-20201001-pages-articles21.xml-p38522433p39996245.bz2\n",
      "345997312/345996096 [==============================] - 232s 1us/step\n",
      "Downloading\n",
      "Downloading data from https://dumps.wikimedia.org/enwiki/20201001/enwiki-20201001-pages-articles22.xml-p39996246p41496245.bz2\n",
      "340557824/340555294 [==============================] - 231s 1us/step\n",
      "Downloading\n",
      "Downloading data from https://dumps.wikimedia.org/enwiki/20201001/enwiki-20201001-pages-articles22.xml-p41496246p42996245.bz2\n",
      "351027200/351022715 [==============================] - 304s 1us/step\n",
      "Downloading\n",
      "Downloading data from https://dumps.wikimedia.org/enwiki/20201001/enwiki-20201001-pages-articles22.xml-p42996246p44496245.bz2\n",
      "354238464/354232150 [==============================] - 347s 1us/step\n",
      "Downloading\n",
      "Downloading data from https://dumps.wikimedia.org/enwiki/20201001/enwiki-20201001-pages-articles22.xml-p44496246p44788941.bz2\n",
      "55885824/55884326 [==============================] - 60s 1us/step\n",
      "Downloading\n",
      "Downloading data from https://dumps.wikimedia.org/enwiki/20201001/enwiki-20201001-pages-articles23.xml-p44788942p46288941.bz2\n",
      "228384768/228383839 [==============================] - 217s 1us/step\n",
      "Downloading\n",
      "Downloading data from https://dumps.wikimedia.org/enwiki/20201001/enwiki-20201001-pages-articles23.xml-p46288942p47788941.bz2\n",
      "362102784/362097862 [==============================] - 626s 2us/step\n",
      "Downloading\n",
      "Downloading data from https://dumps.wikimedia.org/enwiki/20201001/enwiki-20201001-pages-articles23.xml-p47788942p49288941.bz2\n",
      "304455680/304454625 [==============================] - 328s 1us/step\n",
      "Downloading\n",
      "Downloading data from https://dumps.wikimedia.org/enwiki/20201001/enwiki-20201001-pages-articles23.xml-p49288942p50564553.bz2\n",
      "235069440/235065016 [==============================] - 175s 1us/step\n",
      "Downloading\n",
      "Downloading data from https://dumps.wikimedia.org/enwiki/20201001/enwiki-20201001-pages-articles24.xml-p50564554p52064553.bz2\n",
      "322166784/322159926 [==============================] - 269s 1us/step\n",
      "Downloading\n",
      "Downloading data from https://dumps.wikimedia.org/enwiki/20201001/enwiki-20201001-pages-articles24.xml-p52064554p53564553.bz2\n",
      "322420736/322418062 [==============================] - 255s 1us/step\n",
      "Downloading\n",
      "Downloading data from https://dumps.wikimedia.org/enwiki/20201001/enwiki-20201001-pages-articles24.xml-p53564554p55064553.bz2\n",
      "306626560/306624664 [==============================] - 192s 1us/step\n",
      "Downloading\n",
      "Downloading data from https://dumps.wikimedia.org/enwiki/20201001/enwiki-20201001-pages-articles24.xml-p55064554p56564553.bz2\n",
      "321986560/321985394 [==============================] - 189s 1us/step\n",
      "Downloading\n",
      "Downloading data from https://dumps.wikimedia.org/enwiki/20201001/enwiki-20201001-pages-articles24.xml-p56564554p57025655.bz2\n",
      "101703680/101699893 [==============================] - 63s 1us/step\n",
      "Downloading\n",
      "Downloading data from https://dumps.wikimedia.org/enwiki/20201001/enwiki-20201001-pages-articles25.xml-p57025656p58525655.bz2\n",
      "335503360/335498693 [==============================] - 193s 1us/step\n",
      "Downloading\n",
      "Downloading data from https://dumps.wikimedia.org/enwiki/20201001/enwiki-20201001-pages-articles25.xml-p58525656p60025655.bz2\n",
      "296157184/296156960 [==============================] - 233s 1us/step\n",
      "Downloading\n",
      "Downloading data from https://dumps.wikimedia.org/enwiki/20201001/enwiki-20201001-pages-articles25.xml-p60025656p61525655.bz2\n",
      "329154560/329146942 [==============================] - 217s 1us/step\n",
      "Downloading\n",
      "Downloading data from https://dumps.wikimedia.org/enwiki/20201001/enwiki-20201001-pages-articles25.xml-p61525656p62585850.bz2\n",
      "231440384/231434326 [==============================] - 145s 1us/step\n",
      "Downloading\n",
      "Downloading data from https://dumps.wikimedia.org/enwiki/20201001/enwiki-20201001-pages-articles26.xml-p62585851p63975909.bz2\n",
      "344973312/344965297 [==============================] - 321s 1us/step\n",
      "Downloading\n",
      "Downloading data from https://dumps.wikimedia.org/enwiki/20201001/enwiki-20201001-pages-articles27.xml-p63975910p65475424.bz2\n",
      "310239232/310232346 [==============================] - 275s 1us/step\n"
     ]
    }
   ],
   "source": [
    "data_paths = []\n",
    "file_info = []\n",
    "\n",
    "for file in files_to_download:\n",
    "    path = directory + file\n",
    "    \n",
    "    if not os.path.exists(directory):\n",
    "        print('neexistuje')\n",
    "    # downaload only when file dont exist\n",
    "    if not os.path.exists(directory + file):\n",
    "        print('Downloading')\n",
    "        data_paths.append(get_file(file, base_url + file))\n",
    "        file_size = os.stat(path).st_size / 1e6\n",
    "        \n",
    "        # Find the number of articles\n",
    "        file_articles = int(file.split('p')[-1].split('.')[-2]) - int(file.split('p')[-2])\n",
    "        file_info.append((file, file_size, file_articles))\n",
    "        \n",
    "    # when file already exist\n",
    "    else:\n",
    "        data_paths.append(path)\n",
    "        file_size = os.stat(path).st_size / 1e6\n",
    "        \n",
    "        file_number = int(file.split('p')[-1].split('.')[-2]) - int(file.split('p')[-2])\n",
    "        file_info.append((file.split('-')[-1], file_size, file_number))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Part Parsing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parsovanie prebieha postupne na všetkých súboroch v kompresovanom tvare. Na tento účel je použitý podproces \"bzcat\", ktorý číta a dodáva súbor po jednotlivých riadkoch. Na spracovanie týchto dát je použitý XML SAX parser. Tento parser obsahuje metódu ContentHandler, ktorá zabezpečuje uchovanie riadkov v buffery, pričom sa hľadajú tagy (page, title, text). Po nájdeni ukončovacieho znaku tagu page prebieha spracovanie celého článku.\n",
    "\n",
    "Z článku sú pomocou regulárnych výrazov extrahované informácie:\n",
    "* **infobox**\n",
    "    * atribúty infoboxu\n",
    "    * typ infoboxu\n",
    "* **kategórie čklánku**\n",
    "\n",
    "Následne na základe týchto informácií je určená kategória článku."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import xml.sax\n",
    "import regex\n",
    "import pandas as pd\n",
    "from functools import reduce\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import csv\n",
    "import json\n",
    "import os\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "import nltk\n",
    "import gc\n",
    "from nltk.util import ngrams\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact\n",
    "from itertools import chain, islice \n",
    "#nltk.download('punkt')\n",
    "#nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Momentálne sú priradzované kategórie: Person, Company, Organisation, Place.\n",
    "Priradzovanie prebieha podľa vyššieho poradia na základe parametrov v poradí:\n",
    "* **typ infoboxu** - či sa v zozname danej kategorie nachádza infobox daného článku\n",
    "* **atribúty infoboxu:**\n",
    "    * **person** - birth_date\n",
    "    * **company** - industry, trade_name, products, brands\n",
    "    * **organisation** - zatiaľ žiadne\n",
    "    * **place** - coordinates, locations _|neobsahuje|_ date, founded, founder, founders\n",
    "* **kategorie článku:**\n",
    "    * **organisation** - obsahuje v kategóriach slovo organisaion/s\n",
    "* **text článku** - zatiaľ nepoužité, ale plánované pre prípady, kedy článok neobsahuje infobox a kategórie neposkytnú žiadnu informáciu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get Infobox and Infobox type from article text\n",
    "def ArticleHandler(infobox_types=None, evaluation=None):\n",
    "    #source:(stackof) https://regex101.com/r/kT1jF4/1\n",
    "    infobox_regex = '(?=\\{Infobox )(\\{([^{}]|(?1))*\\})'\n",
    "    inf_type_regex = '(?<=Infobox)(.*?)(?=(\\||\\n|<!-|<--))'\n",
    "    #https://regex101.com/r/1vJlms/1\n",
    "    inf_parameters = '(?(?<=\\|)|(?<=\\|\\s))(\\w*)\\s*=\\s*[\\w{\\[]'\n",
    "    #https://regex101.com/r/fl5hAw/1 https://regex101.com/r/Xj0fM3/1\n",
    "    redirect_title = '(?<=\\[\\[)(.*)(?=\\]\\])'\n",
    "    categories = '(?<=\\[\\[Category:)([^\\]]*)(?=\\]\\])'\n",
    "    testing = evaluation\n",
    "    \n",
    "    Person=['player', 'male', 'actor', 'sportspeopl', 'medalist', 'actress', 'expatri', 'singer', 'musician', 'live', 'writer', 'politician', 'f.c', 'alumni', 'personnel', 'olymp', '20th-centuri', 'faculti', 'coach', 'guitarist']\n",
    "    Company=['brand', 'merger', 'retail', 'exchang', 'stock', 'label', 'nasdaq', 'multin', 'subsidiari', 'acquisit', 'onlin', 'offer', 'held', 'conglomer', 'store', 'bankruptci']\n",
    "    Organisation=['scout', 'think', 'non-profit', 'gang', 'event', 'recur', 'religi', 'child-rel', 'non-align', 'non-government', 'critic', 'evangel', 'yakuza', 'advocaci']\n",
    "    Location=['regist', 'unincorpor', 'station', 'popul', 'complet', 'aerodrom', 'villag', 'town', 'landform', 'parish', 'river', 'seaplan', 'open', 'census-design', 'mountain', 'attract', 'neighbourhood', 'suburb', 'rang', 'airport']\n",
    "    \n",
    "    PersonBi=['living peopl', '20th century', 'f c', 'c play', 'century american', '21st century', 'american male', 'association football', 'league play', 'expatriate footballers', 'expatriate sportspeople', 'cup play', 'international footbal', 'rugby league', 'university alumni', 'fc play', 'musical groups', 'ice hockey', 'world cup', 'american people', 'fifa world', 'male actors', 'football league', 'male actor', 'expatriate footbal', 'military personnel', 'people educated', 'hockey players', 'male writ', 'records artist', 'draft pick', 'century indian', 'football manag', 'male television', 'film actor', 'uk mps', 'male film', 'soccer play', 'television actor', 'united f', 'year birth', 'living people', 'winter olymp', 'birth missing', 'football midfield', 'missing living', 'major league', 'school alumni', '19th century', 'new zealand']\n",
    "    CompanyBi=['companies established', 'companies based', 'companies united', 'services companies', 'financial services', 'mergers acquisit', 'american companies', 'stock exchanges', 'video game', 'game companies', 'manufacturing companies', 'chains united', 'companies listed', 'exchanges africa', 'restaurants established', 'stock exchang', 'companies disestablished', 'mass media', 'media companies', 'restaurant chains', 'retail companies', 'companies filed', 'defunct companies', 'filed chapter', 'internet properties', 'properties established', 'retailers united', '11 bankruptcy', 'chapter 11', 'companies canada', 'companies formerly', 'established 1960', 'established 1995', 'fast food', 'formerly listed', 'listed new', 'manufacturers united', 'york stock', 'established 1989', 'establishments california', 'based austin', 'british companies', 'clothing companies', 'companies england', 'companies isle', 'development compani', 'established 1950', 'established 1974', 'established 2003', 'food chains']\n",
    "    OrganisationBi=['based united', 'non profit', 'learned societies', 'profit organizations', 'organizations established', 'organisations based', 'organizations based', '3 organ', '501 c', 'associations based', 'c 3', 'charities based', 'consultative status', 'established 1946', 'professional associations', 'psychology organizations', 'relief organ', 'societies canada', 'status united', 'english football', 'establishments united', 'youth organizations', 'united nations', '1845 establishments', '1859 establishments', '1864 establishments', '1907 establishments', '1908 establishments', '1959 establishments', '1982 establishments', '1996 establishments', '19th centuri', 'academy financial', 'advocacy organ', 'aid organ', 'air ambulance', 'ambulance servic', 'ambulance services', 'american council', 'american organized', 'ancient near', 'awards h', 'banks texa', 'bar associ', 'based geneva', 'based hong', 'based montr', 'based surrey', 'based switzerland', 'based tyne']\n",
    "    LocationBi=['pyrénées atlantiqu', 'communes pyrénées', 'articles needing', 'atlantiques communes', 'communes articles', 'french wikipedia', 'needing translation', 'pyrénées atlantiques', 'translation french', 'lower navarr', 'populated places', 'register historic', 'national register', 'historic places', 'towns luxembourg', 'unincorporated communities', 'civil parishes', 'buildings structures', 'neighborhoods pittsburgh', 'sports venues', 'western australia', 'parishes leicestershir', 'villages leicestershir', 'cities towns', 'protected areas', 'borough charnwood', 'places established', 'suburbs perth', 'west virginia', 'tourist attractions', 'buildings completed', 'city rockingham', 'suburbs city', 'shopping malls', 'new jersey', 'county west', 'historic house', 'house museums', 'venues completed', 'county california', 'county virginia', 'alzette canton', 'communes esch', 'county massachusett', 'esch sur', 'former communes', 'mountains hills', 'rhode island', 'road bridges', 'sur alzette']\n",
    "    \n",
    "    \n",
    "    # infobox_types = getInfoboxTypesList()\n",
    "    \n",
    "    def filterArticles(title):\n",
    "        if regex.search(\"^Category:|^Template:|^File:\", title):\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    def getCategories(text):\n",
    "        return regex.findall(categories, text)\n",
    "    \n",
    "    \n",
    "    def getArticleAtributes(infobox,text):\n",
    "        i_par = regex.findall(inf_parameters, infobox)\n",
    "        i_type = regex.search(inf_type_regex, infobox)\n",
    "        i_type = i_type.group(0).strip() if i_type is not None else \"none\"\n",
    "        return {'type': i_type.lower(), 'parameters': i_par, 'categories': list(getCategories(text))}\n",
    "    \n",
    "    \n",
    "    def remove_stop_words(data):\n",
    "        stopwords = nltk.corpus.stopwords.words('english')\n",
    "        return [w for w in data if w not in stopwords]\n",
    "\n",
    "\n",
    "    def tokenize(data):\n",
    "        symbols = symbols = \"!\\\"#$%&()*+'-./:;,|<=>?@[\\]^_`{}~\\n\"\n",
    "        tokens = word_tokenize(data)\n",
    "        tokens = [token.lower() for token in tokens if token not in list(symbols)]\n",
    "        return tokens\n",
    "\n",
    "\n",
    "    def stemming(data):\n",
    "        stemmer= PorterStemmer()\n",
    "        return [stemmer.stem(token) for token in data]\n",
    "        \n",
    "    def processCategories(data):\n",
    "        data = tokenize(data)\n",
    "        data = remove_stop_words(data)\n",
    "        data = stemming(data)\n",
    "        return data\n",
    "    \n",
    "    \n",
    "    def get_bigrams(text):\n",
    "        bigrams = []\n",
    "        for sen in text:\n",
    "            token = nltk.word_tokenize(sen)\n",
    "            bigrams.append(list(map(lambda x: ' '.join(x),list(ngrams(token,2)))))\n",
    "        return bigrams\n",
    "\n",
    "\n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "    def process_whole_sentence(text):\n",
    "        sen = ' '.join(w for w in text.split() if w not in stopwords)\n",
    "        sen = re.sub(r'\\W', ' ', str(sen))\n",
    "        sen = re.sub(r'\\s+', ' ', sen, flags=re.I)\n",
    "        sen = sen.lower()\n",
    "        return sen\n",
    "        \n",
    "        \n",
    "    def isRedirect(text):\n",
    "        return regex.search(\"^#redirect\\s*\\[\\[(?i)\", text)\n",
    "        \n",
    "        \n",
    "    def getInfobox(text):\n",
    "        infobox = regex.search(infobox_regex, text)\n",
    "        return infobox.group() if infobox is not None else \"redirect\" if isRedirect(text) is not None else \"no infobox/redirect\"\n",
    "    \n",
    "    \n",
    "    def categoryBy_infoboxType(info):\n",
    "        if info['type'] in infobox_types['person']:\n",
    "            return 'Person'\n",
    "        elif info['type'] in infobox_types['company']:\n",
    "            return 'Company'\n",
    "        elif info['type'] in infobox_types['org']:\n",
    "            return 'Organization'\n",
    "        elif info['type'] in infobox_types['location']:\n",
    "            return 'Location'\n",
    "        else:\n",
    "            return None\n",
    "  \n",
    "\n",
    "    def anotherCategoryBy_infoboxType(info):\n",
    "        if info['type'] in infobox_types['other']:\n",
    "            return 'Another'\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "        \n",
    "    def categoryBy_atributes(info):\n",
    "        if 'birth_date' in info['parameters']:\n",
    "            return \"A_Person\"\n",
    "        elif any(i in info['parameters'] for i in ['industry', 'trade_name', 'products', 'brands']):\n",
    "            return 'A_Company'\n",
    "        elif any(i in info['parameters'] for i in ['coordinates', 'locations']) and not(any(i in info['parameters'] for i in ['date', 'founded', 'founder', 'founders'])):\n",
    "            return 'A_Location'\n",
    "        else:\n",
    "            return None\n",
    "        \n",
    "        \n",
    "    def categoryBy_categories(info):\n",
    "        stemmed_categories = reduce(lambda x,y: x+y,map(lambda x: processCategories(x), info['categories']),[])\n",
    "        bigramCategories = sum(get_bigrams(stemming(map(lambda x: process_whole_sentence(x), info['categories']))),[])\n",
    "        \n",
    "#         if any(i in PersonBi for i in bigramCategories) or any(i in Person for i in stemmed_categories):\n",
    "#             return 'C_Person'\n",
    "#         elif any(i in CompanyBi for i in bigramCategories) or any(i in Company for i in stemmed_categories):\n",
    "#             return 'C_Company'\n",
    "#         elif any(i in OrganisationBi for i in bigramCategories) or any(i in Organisation for i in stemmed_categories):\n",
    "#             return 'C_Organization'\n",
    "#         elif any(i in LocationBi for i in bigramCategories) or any(i in Location for i in stemmed_categories):\n",
    "#             return 'C_Location'\n",
    "        \n",
    "#         if any(i in Person for i in stemmed_categories):\n",
    "#             return 'C_Person'\n",
    "#         elif any(i in Company for i in stemmed_categories):\n",
    "#             return 'C_Company'\n",
    "#         elif any(i in Organisation for i in stemmed_categories):\n",
    "#             return 'C_Organization'\n",
    "#         elif any(i in Location for i in stemmed_categories):\n",
    "#             return 'C_Location'\n",
    "\n",
    "        if any(i in PersonBi for i in bigramCategories):\n",
    "            return 'C_Person'\n",
    "        elif any(i in CompanyBi for i in bigramCategories):\n",
    "            return 'C_Company'\n",
    "        elif any(i in OrganisationBi for i in bigramCategories):\n",
    "            return 'C_Organization'\n",
    "        elif any(i in LocationBi for i in bigramCategories):\n",
    "            return 'C_Location'\n",
    "        \n",
    "        elif list(filter(lambda x: regex.search('^\\d*\\sbirths*(?i)', x), info['categories'])):\n",
    "            return 'C_Person'\n",
    "        elif list(filter(lambda x: regex.search('\\b(compan(y|ies))\\b(?i)', x), info['categories'])):\n",
    "            return 'C_Company'\n",
    "        elif list(filter(lambda x: regex.search('(organisations*)(?i)', x), info['categories'])):\n",
    "            return 'C_Organization'\n",
    "        else:\n",
    "            return None\n",
    "    \n",
    "    \n",
    "    def first_true(iterable,data=None, default='Other'):\n",
    "        return next((item(data) for item in iterable if item(data) is not None), default)\n",
    "    \n",
    "    \n",
    "    def predictCategory(infobox, info):\n",
    "        if infobox not in ['redirect', 'no infobox/redirect']:\n",
    "            if not(testing):\n",
    "                return first_true([categoryBy_infoboxType,categoryBy_atributes, anotherCategoryBy_infoboxType, categoryBy_categories], info)\n",
    "            else:\n",
    "                if categoryBy_infoboxType(info) is not None or anotherCategoryBy_infoboxType(info) is not None: \n",
    "                    return first_true([categoryBy_atributes,categoryBy_categories], info)\n",
    "                else:\n",
    "                    return None\n",
    "            #tieto clanky maju len kategorie\n",
    "        elif infobox == 'no infobox/redirect':\n",
    "            return first_true([categoryBy_categories], info,\"Other/None\")\n",
    "        else:\n",
    "            return 'redirect::'+info\n",
    "\n",
    "    \n",
    "    def processArticle(title, text):\n",
    "        infobox = getInfobox(text)\n",
    "        \n",
    "        if filterArticles(title):\n",
    "            return None\n",
    "        \n",
    "        if infobox == \"redirect\":\n",
    "            info = regex.search(redirect_title, text)\n",
    "            if info is None:\n",
    "                return None\n",
    "            info = info.group(0)\n",
    "        \n",
    "        elif infobox == 'no infobox/redirect':\n",
    "            info = {'categories': list(getCategories(text))}\n",
    "            if info['categories'] == []:\n",
    "                return None\n",
    "        else:\n",
    "            info = getArticleAtributes(infobox, text)\n",
    "\n",
    "        return (title, infobox, info, predictCategory(infobox, info))\n",
    "    return processArticle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#docs: https://docs.python.org/3.8/library/xml.sax.handler.html\n",
    "class ContentHandler(xml.sax.handler.ContentHandler):\n",
    "    def __init__(self, testing=None):\n",
    "        xml.sax.handler.ContentHandler.__init__(self)\n",
    "        self._buf = None\n",
    "        self._last_tag = None\n",
    "        self._parts = {}\n",
    "        self.output = []\n",
    "        self.evaluation = testing\n",
    "        self.article_process = ArticleHandler(infobox_types=getInfoboxTypesList(),evaluation=self.evaluation)\n",
    "\n",
    "    def characters(self, content):\n",
    "        if self._last_tag:\n",
    "            self._buf.append(content)\n",
    "\n",
    "    def startElement(self, name, attrs):\n",
    "        if name == 'page':\n",
    "            self._parts = {}\n",
    "        if name in ('title', 'text'):\n",
    "            self._last_tag = name\n",
    "            self._buf = []\n",
    "\n",
    "    def endElement(self, name):\n",
    "        if name == self._last_tag:\n",
    "            self._parts[name] = ''.join(self._buf)\n",
    "        \n",
    "        #whole article\n",
    "        if name == 'page':\n",
    "            data = self.article_process(**self._parts)\n",
    "            if data is not None:\n",
    "                self.output.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def parseWiki(data=None, limit = 200, save = True, test_sample=False, evaluation=False):\n",
    "    \n",
    "    if test_sample:\n",
    "        data = os.getcwd().rsplit('/', 1)[0]\n",
    "        data = f'{data}/data/sample_wiki_articles2.xml.bz2'\n",
    "        print(data)\n",
    "    elif data is None:\n",
    "        data = '/home/xminarikd/.keras/datasets/enwiki-20201001-pages-articles9.xml-p2936261p4045402.bz2'\n",
    "    \n",
    "    handler = ContentHandler(testing=evaluation)\n",
    "\n",
    "    parser = xml.sax.make_parser()\n",
    "    parser.setContentHandler(handler)\n",
    "\n",
    "    for i, line in enumerate(subprocess.Popen(['bzcat'], \n",
    "                             stdin = open(data), \n",
    "                             stdout = subprocess.PIPE).stdout):\n",
    "\n",
    "#         if (i + 1) % 10000 == 0:\n",
    "#             print(f'Spracovanych {i + 1} riadkov.', end = '\\r')\n",
    "#             print('')\n",
    "        try:\n",
    "            parser.feed(line)\n",
    "        except StopIteration:\n",
    "            break\n",
    "        \n",
    "        # get only some results\n",
    "        if limit and len(handler.output) >= limit:\n",
    "            break\n",
    "        \n",
    "    if save:\n",
    "        output_dir = os.getcwd().rsplit('/', 1)[0]\n",
    "        partition_name = data.split('/')[-1].split('-')[-1].split('.')[0]\n",
    "        if not(evaluation):\n",
    "            output_file = f'{output_dir}/output/{partition_name}.tsv'\n",
    "        else:\n",
    "            output_file = f'{output_dir}/output/eval/{partition_name}.tsv'\n",
    "\n",
    "        \n",
    "        f1 = open(output_file, 'w+', newline='\\n')\n",
    "        f2 = open(f'{output_file}-redirects', 'w+', newline='\\n')\n",
    "        \n",
    "        writer1 = csv.writer(f1, delimiter='\\t')\n",
    "        writer2 = csv.writer(f2, delimiter='\\t')\n",
    "        writer1.writerow([\"Title\",\"Category\"])\n",
    "        writer2.writerow([\"Title\",\"Source\"])\n",
    "        \n",
    "        for x in handler.output:\n",
    "            if x[1] == 'redirect':\n",
    "                writer2.writerow([x[0],x[2] or 'None'])\n",
    "            else:\n",
    "                writer1.writerow([x[0],x[3] or 'None'])\n",
    "        \n",
    "        print(f'{output_file} done', end='\\r')\n",
    "        del handler\n",
    "        del parser\n",
    "        gc.collect()\n",
    "        return None\n",
    "    else:\n",
    "        return handler.output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stiahnutie a parsovanie stránky wikipédie, ktorá obsahuje zoznam typov infoboxov. Tento zoznam obsahuje aj členeie týchto typov do rôznych kategórií. Vďaka tomuto je možné jednoducho získať všetky infoboxy, ktoré sú spojené napríklad s osobami."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getInfoboxTypesList():\n",
    "    infobox_list_url = 'https://en.wikipedia.org/wiki/Wikipedia:List_of_infoboxes'\n",
    "    infobox_list_html = requests.get(infobox_list_url).text\n",
    "    soup_dump = BeautifulSoup(infobox_list_html, 'html.parser')\n",
    "    #sib = soup_dump.find_all(\"div\" ,{'id': 'toc'}).next_sibling\n",
    "    other = {}\n",
    "\n",
    "    template_list = dict();\n",
    "    prev = None\n",
    "    prev_tag = None\n",
    "    prev_parent = None\n",
    "    prev_parent_tag = 2\n",
    "\n",
    "    for i, sibling in enumerate(soup_dump.find(id=\"toc\").next_siblings):\n",
    "\n",
    "        if prev_parent == 'Other':\n",
    "            break\n",
    "\n",
    "        if sibling.name == 'h2':\n",
    "            template_list[sibling.findChild().text] = {}\n",
    "            prev_parent = sibling.findChild().text\n",
    "            prev_tag = 2\n",
    "\n",
    "        if sibling.name == 'h3':\n",
    "            if prev_tag < 3:\n",
    "                template_list[prev_parent][sibling.findChild().text] = list()\n",
    "                prev_tag = 3\n",
    "                prev = sibling.findChild().text\n",
    "\n",
    "            if prev_tag == 3:\n",
    "                template_list[prev_parent][sibling.findChild().text] = list()\n",
    "                prev = sibling.findChild().text\n",
    "\n",
    "        if sibling.name == 'ul':\n",
    "            a = sibling.find_all('a', title=re.compile('^Template:Infobox'))\n",
    "            b = map(lambda x: regex.findall('(?<=Template:Infobox )(.*)(?i)', x.text.lower()), a)\n",
    "            c = reduce(lambda x,y: x+y, b, list())\n",
    "\n",
    "            if prev_tag >=3:\n",
    "                template_list[prev_parent][prev] = [y for x in [template_list[prev_parent][prev], list(c)] for y in x] \n",
    "            else:\n",
    "                template_list[prev_parent] = list(c)\n",
    "\n",
    "    \n",
    "    for k ,v in template_list.items():\n",
    "        if(k not in ['Person', \"Place\", 'Society and social science', \"Other\"]):\n",
    "            other.update({k:v})\n",
    "        elif k == 'Society and social science':\n",
    "            tmp = {}\n",
    "            for k2,v2 in v.items():\n",
    "                if k2 not in ['Business and economics', \"Organization\"]:\n",
    "                    tmp.update({k2:v2})\n",
    "            other.update({k:tmp})\n",
    "            \n",
    "    other = sum(sum((map(lambda x: list(x.values()) if isinstance(x, dict) else [x] ,list(other.values()))),[]),[])\n",
    "    persons = list(reduce(lambda x,y: x+y, template_list[\"Person\"].values()))\n",
    "    locations = list(reduce(lambda x,y: x+y, template_list[\"Place\"].values()))\n",
    "    companies = template_list['Society and social science']['Business and economics']\n",
    "    organizations = template_list['Society and social science']['Organization']\n",
    "    \n",
    "    return {'person': persons, 'location': locations, 'company': companies, 'org': organizations, 'other': other}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jednoduché funckie používané vo viacerých častaich projektu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPathFiles(path, endwith):\n",
    "    out_path = os.getcwd().rsplit('/', 1)[0]\n",
    "    files = f'{out_path}{path}/'\n",
    "    files = [files+file for file in os.listdir(files) if file.endswith(endwith)]\n",
    "    return files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapperCategories(arg):\n",
    "    switcher = {\n",
    "        'Person': 0,\n",
    "        'A_Person':0,\n",
    "        'C_Person':0,\n",
    "        'Company': 1,\n",
    "        'A_Company':1,\n",
    "        'C_Company':1,\n",
    "        'Organization':2,\n",
    "        'A_Organization':2,\n",
    "        'C_Organization':2,\n",
    "        'Location':3,\n",
    "        'A_Location':3,\n",
    "        'C_Location':3,\n",
    "        'Another':4\n",
    "    }\n",
    "    return switcher.get(arg,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapperCategoriesNames(arg):\n",
    "    switcher = {\n",
    "        'Person': 'Person',\n",
    "        'A_Person':'Person',\n",
    "        'C_Person':'Person',\n",
    "        'Company': 'Company',\n",
    "        'A_Company':'Company',\n",
    "        'C_Company':'Company',\n",
    "        'Organization':'Organization',\n",
    "        'A_Organization':'Organization',\n",
    "        'C_Organization':'Organization',\n",
    "        'Location':'Location',\n",
    "        'A_Location':'Location',\n",
    "        'C_Location':'Location',\n",
    "        'Another':'Another'\n",
    "    }\n",
    "    return switcher.get(arg,'Another')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readTsv(file):\n",
    "    output = []\n",
    "    with open(file) as f:\n",
    "        for line in csv.DictReader(f, delimiter='\\t'): \n",
    "            output.append(line)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readTsvGenerator(file):\n",
    "#     output = []\n",
    "    with open(file) as f:\n",
    "        for line in csv.DictReader(f, delimiter='\\t'): \n",
    "            yield line\n",
    "#     return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spustenie programu na spravocanie súboru, avšak nie je to hlavné spustenie. Tento prístup je na ukázanie funckionality pri spracovaní časti jedného súboru. Na spracovanie celého datasetu je vytvorená multiprocessorova alternatíva. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/xminarikd/Documents/VINF/data/sample_wiki_articles2.xml.bz2\n",
      "David Stagg <--> A_Person\n",
      "Amaranthus mantegazzianus <--> redirect::Amaranthus caudatus\n",
      "Amaranthus quitensis <--> redirect::Amaranthus hybridus\n",
      "Maud Queen of Norway <--> redirect::Maud of Wales\n",
      "Milligram per litre <--> redirect::Gram per litre\n",
      "Utica Psychiatric Center <--> Location\n",
      "Olean Wholesale Grocery <--> C_Company\n",
      "Queen Tiye <--> redirect::Tiye\n",
      "Queen Hatshepsut <--> redirect::Hatshepsut\n",
      "Clibanarii <--> Other/None\n",
      "Political documentary <--> redirect::Documentary film\n",
      "Final fantasy legends <--> redirect::Final Fantasy Dimensions\n",
      "Queen Marie Amelie Therese <--> redirect::Maria Amalia of Naples and Sicily\n",
      "Political documentaries <--> redirect::Documentary film\n",
      "E-767 <--> redirect::Boeing E-767\n",
      "Prince Edward-Lennox <--> redirect::Prince Edward\\xe2\\x80\\x94Lennox\n",
      "Arthur Hill (actor) <--> A_Person\n",
      "Periodic paralysis <--> Other\n",
      "Greenstripe <--> redirect::Amaranthus acanthochiton\n",
      "Amaranthus cruentus <--> C_Location\n",
      "Careless weed <--> redirect::Amaranthus palmeri\n",
      "Zamil idris <--> redirect::Ahmad Zamil\n",
      "Khada sag <--> redirect::Amaranthus dubius\n",
      "Million instructions per second <--> redirect::Instructions per second#Millions of instructions per second (MIPS)\n",
      "Ashtadiggajas <--> Other/None\n",
      "John C.Harsanyi <--> redirect::John Harsanyi\n",
      "Soci\\xc3\\xa9t\\xc3\\xa9 entomologique de France <--> C_Organization\n",
      "Sangorache <--> redirect::Amaranthus hybridus\n",
      "Joseph's coat <--> redirect::Coat of many colors\n",
      "Recipients of the Distinguished Service Award of the Order of the Arrow <--> Other/None\n",
      "Josephscoat <--> redirect::Amaranthus tricolor\n",
      "Joseph's-coat <--> redirect::Amaranthus tricolor\n",
      "URM Stores <--> C_Company\n",
      "General purpose technology <--> Other/None\n",
      "Rough skinned newt <--> redirect::Rough-skinned newt\n",
      "Queen Sophie Dorothea of Hanover <--> redirect::Sophia Dorothea of Hanover\n",
      "Cefepime <--> Other/None\n",
      "Larry Zerner <--> A_Person\n",
      "World of Illushions Starring Mickey Mouse and Donald Duck <--> redirect::World of Illusion\n",
      "United Retail Merchants <--> redirect::URM Stores\n",
      "Plassenburg <--> C_Location\n",
      "U.R.M. <--> redirect::URM Stores\n",
      "Turriaco <--> A_Location\n",
      "Piggly Wiggly Alabama Distributing Company <--> redirect::Piggly Wiggly\n",
      "Queen Eleanor of Aquitaine <--> redirect::Eleanor of Aquitaine\n",
      "Crenshaw Company <--> C_Company\n",
      "Osmia ribifloris <--> Other/None\n",
      "San Pier d'Isonzo <--> A_Location\n",
      "Early life of Hugo Ch\\xc3\\xa1vez <--> A_Person\n"
     ]
    }
   ],
   "source": [
    "data = parseWiki(test_sample=True, limit=0, save=False, evaluation=False)\n",
    "\n",
    "for i, x in enumerate(data):\n",
    "    if i > 150:\n",
    "        break\n",
    "    if x[1] == 'redirect':\n",
    "        print(x[0], '<-->', x[3])\n",
    "    else:\n",
    "        print(x[0], '<-->', x[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pomocou tohto prístupu (multiprocessing) bolo umožnené rýchlejšie spracovanie celého datasetu, ktorý je rozdelený na 58 súborov. \n",
    "Tento prístup bol testovaný na 4 jadrách, prčim ich unitilácia bola na úrovni 100%.\n",
    "Spracovanie celého datasetu v tomto prípade trvalo 1h 44min 51s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool \n",
    "from tqdm.notebook import tqdm\n",
    "from functools import partial\n",
    "import uuid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_dir = '/home/xminarikd/.keras/datasets/'\n",
    "dataset = [dataset_dir+file for file in os.listdir(dataset_dir)]\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97d5250900e543e698bdb3f5a54b5f86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=58.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/xminarikd/Documents/VINF/output/p23716198p25216197.tsv done\n",
      "CPU times: user 397 ms, sys: 81.3 ms, total: 478 ms\n",
      "Wall time: 1h 44min 51s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "pool = Pool(processes=4)\n",
    "results = []\n",
    "\n",
    "map_parser = partial(parseWiki, limit = 0, save = True,evaluation=False)\n",
    "\n",
    "for x in tqdm(pool.imap_unordered(map_parser, dataset), total = len(dataset)):\n",
    "    results.append(x)\n",
    "\n",
    "pool.close()\n",
    "pool.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vytvorenie indexu a následne vyhľadávanie v ňom. Na tento účel je použitý framework Elasticsearch. Komunikácia s týmto frameworkom prebiha pomocou python knižnice s rovnakých názvom. \n",
    "\n",
    "Vytvorený idex obsahuje 2 polia, title a category. Tieto polia sú typu text. Vytváranie indexu prebiaha pomocou bulk API, ktorá umožnuje zadanie viacero indexov v jednom kroku, bez použitia commit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":) Connect\n"
     ]
    }
   ],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "from tqdm.auto import tqdm as tq\n",
    "def connect_elasticsearch():\n",
    "    _es = None\n",
    "    _es = Elasticsearch([{'host': 'localhost', 'port': 9200}], timeout=120, max_retries=10, retry_on_timeout=True)\n",
    "    if _es.ping():\n",
    "        print(':) Connect')\n",
    "    else:\n",
    "        print(':( could not connect!')\n",
    "    return _es\n",
    "\n",
    "es = connect_elasticsearch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vytvorenie indexu, zadefinovanie počtu polí a ich typov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_index(es_object, index_name='wiki'):\n",
    "    created = False\n",
    "    # index settings\n",
    "    settings = {\n",
    "        \"settings\": {\n",
    "            \"number_of_shards\": 1,\n",
    "            \"number_of_replicas\": 1,\n",
    "        }\n",
    "        \"mappings\": {\n",
    "          \"properties\": {\n",
    "            \"category\": {\n",
    "              \"type\": \"text\",\n",
    "              \"fields\": {\n",
    "                \"keyword\": {\n",
    "                  \"type\": \"keyword\",\n",
    "                  \"ignore_above\": 256\n",
    "                }\n",
    "              }\n",
    "            },\n",
    "            \"title\": {\n",
    "              \"type\": \"text\",\n",
    "              \"fields\": {\n",
    "                \"keyword\": {\n",
    "                  \"type\": \"keyword\",\n",
    "                  \"ignore_above\": 256\n",
    "                }\n",
    "              }\n",
    "            }\n",
    "          }\n",
    "        }\n",
    "    }\n",
    " \n",
    "    try:\n",
    "        if not es_object.indices.exists(index_name):\n",
    "            # Ignore 400 means to ignore \"Index Already Exist\" error.\n",
    "            es_object.indices.create(index=index_name, body=settings)\n",
    "            print('Created Index')\n",
    "        created = True\n",
    "    except Exception as ex:\n",
    "        print(str(ex))\n",
    "    finally:\n",
    "        return created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created Index\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_index(es,'wiki')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funkcia na pridanie nového indexu. Použitie tejto metódy na pridanie viacerých záznamov súčastne nie je odporučané z dôvodu vysokej časovej náročnosti."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def toElastic(files, elastic):\n",
    "    for x in tq(files):\n",
    "        data = readTsv(x)\n",
    "        for item in tq(data):\n",
    "            res = elastic.index(index='wiki', id=uuid.uuid4(), body={'title': item['Title'], 'category': mapperCategories(item['Category'])})\n",
    "            if res['result'] != 'created':\n",
    "                print('Warning, Error', res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Searching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jednoduché vyhľadávanie, v ktorom sa vyhľadáva súčastne v oboch poliach. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "681c20aa256549dfbabf812c57d6ed17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Text(value='', description='query'), Output()), _dom_classes=('widget-interact',))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@interact(query=\"\")\n",
    "def searchByTitle(query):\n",
    "    res = es.search(index='wiki', body={\n",
    "        \"query\":{\n",
    "            \"multi_match\":{\n",
    "                \"query\": query,\n",
    "                \"type\": \"cross_fields\",\n",
    "                \"analyzer\" : \"standard\",\n",
    "                \"fields\": [\"title\",\"category^3\"]\n",
    "            }\n",
    "        }\n",
    "    })\n",
    "    return list(map(lambda x: x['_source'],res['hits']['hits']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Možnosť vytvorenia vyhľadávania s presným definovaním požadovanej kategórie a následne vyhľdávanie len na základe parametra title. Taktiež ponúka možnosť definovania počtu požadovaných výsledkov."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "821ef5875ae848428bcd03b1621af9c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Text(value='', description='title'), Dropdown(description='category', options=('Person',…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@interact(title=\"\", category=[\"Person\", \"Location\", \"Company\", \"Organization\"], i=widgets.IntSlider(value=10, description='Limit', max=100, min=1))\n",
    "def searchByTitle2(title, category,i):\n",
    "    res = es.search(index='wiki', body={\n",
    "        \"from\" : 0,\n",
    "        \"size\" : i,\n",
    "#         \"min_score\": 0.8,\n",
    "        \"query\":{\n",
    "            \"bool\": {\n",
    "                \"must\": {\n",
    "                    \"bool\": {\n",
    "                        \"should\": {\n",
    "                            \"match\":{\n",
    "                                \"title\": title\n",
    "                            }\n",
    "                        },\n",
    "                        \"must\": {\n",
    "                            \"match\": {\"category\": category}\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    })\n",
    "    df = pd.DataFrame(list(map(lambda x: x['_source'],res['hits']['hits'])), columns=[\"title\", \"category\"])\n",
    "    df.set_index('title', inplace=True)\n",
    "    with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n",
    "        print(df)\n",
    "#     return list(map(lambda x: x['_source'],res['hits']['hits']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0879705bbab34f5ba98c8f44c572d38d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Text(value='', description='title'), Output()), _dom_classes=('widget-interact',))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<function __main__.searchExactMatchByTitle(title)>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interact(searchExactMatchByTitle,title=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{'title': 'X Æ A-XII', 'category': 'Person'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def searchExactMatchByTitle(title):\n",
    "    res = es.search(index=\"wiki\",body=\n",
    "    {\n",
    "       \"size\": 1,\n",
    "       \"query\" : {\n",
    "          \"term\" : {\n",
    "             \"title.keyword\" : title\n",
    "          }\n",
    "       }\n",
    "    })\n",
    "    if res['hits']['hits']:\n",
    "        return res['hits']['hits'][0]['_source']\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'took': 4, 'timed_out': False, '_shards': {'total': 1, 'successful': 1, 'skipped': 0, 'failed': 0}, 'hits': {'total': {'value': 0, 'relation': 'eq'}, 'max_score': None, 'hits': []}}\n"
     ]
    }
   ],
   "source": [
    "res= es.search(index='wiki',body={\n",
    "   \"query\" : {\n",
    "      \"term\" : {\n",
    "         \"Title.keyword\" : \"Andy\"\n",
    "      }\n",
    "   }\n",
    "})\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "## Spracovanie redirectov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "redirect_files = getPathFiles('/output','-redirects')\n",
    "redirect_files = redirect_files\n",
    "len(redirect_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_dir = os.getcwd().rsplit('/', 1)[0]\n",
    "output_file = f'{output_dir}/output/redirects/processedRedirects.tsv'\n",
    "f1 = open(output_file, 'w+', newline='\\n')\n",
    "f1.truncate(0)\n",
    "writer1 = csv.writer(f1, delimiter='\\t')\n",
    "writer1.writerow([\"Title\",\"Category\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 µs, sys: 1e+03 ns, total: 4 µs\n",
      "Wall time: 6.2 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for file in redirect_files:\n",
    "    data = readTsv(file)\n",
    "    resSearch = es.msearch(body=multiSearch(file))\n",
    "    for item, res in zip(data, resSearch[\"responses\"]):\n",
    "        if res[\"hits\"][\"hits\"]:\n",
    "            writer1.writerow([item[\"Title\"],res[\"hits\"][\"hits\"][0][\"_source\"][\"category\"]])\n",
    "#             founded.append({\"Title\": item[\"Title\"], \"Category\": res[\"hits\"][\"hits\"][0][\"_source\"][\"category\"] })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3060367"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(founded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = os.getcwd().rsplit('/', 1)[0]\n",
    "output_file = f'{output_dir}/output/redirects/processedRedirects.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7968615"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(readTsv(output_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def bulkRedirectData(file):\n",
    "    metadata = '{ \"index\": { \"_index\": \"wiki\" }}'\n",
    "    for item in readTsvGenerator(file):\n",
    "        if mapperCategoriesNames(item[\"Category\"]) in [\"Person\",'Company','Organization','Location']:\n",
    "            curr = {\"title\": item[\"Title\"], \"category\": mapperCategoriesNames(item[\"Category\"])}\n",
    "            yield f'{metadata}{os.linesep}{json.dumps(curr)}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunks(iterable, size=10):\n",
    "    iterator = iter(iterable)\n",
    "    for first in iterator:\n",
    "        yield chain([first], islice(iterator, size - 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Elasticsearch ma obmedznie na maximálnu veľkosť bulk requestu na vytvorenie indexu. Preto sa dáta zadávajú po skupiných (chunk) veľkostí 500 000 záznamov."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 1s, sys: 3.09 s, total: 1min 4s\n",
      "Wall time: 4min 28s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for chunk in chunks(bulkRedirectData(output_file), size=500000):\n",
    "    res = es.bulk(index='wiki', body= chunk)\n",
    "    if res['errors']:\n",
    "        print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiSearch(file):\n",
    "    header = '{\"index\": \"wiki\"}'\n",
    "    for item in readTsvGenerator(file):\n",
    "        curr = {\"size\": 1, \"query\" : { \"term\" : {\"title.keyword\" : item['Source'] }} }\n",
    "        yield f'{header}{os.linesep}{json.dumps(curr)}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vytvorenie indexu zo súboru"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Na vyriešenie problému vložania celého spravocaného datasetu do indexu bolo nájdene riešenia využitia bulk api, ktoré vykoná viacero operácií na elesticom bez použitia priebžných commitov. Tento prístup mnohonásobne zrýchlil vytvorenie indexu opriti postupnému vkladaniu jednotlivých záznamov."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bulk api potrebuje presne definovanú štruktúru na vykonanie query. Táto funkcia, teda generátor, vytvára túto štruktúru."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bulkMyData(file):\n",
    "    metadata = '{ \"index\": { \"_index\": \"wiki\" }}'\n",
    "#     data = readTsv(file)\n",
    "    for item in readTsvGenerator(file):\n",
    "        curr = {\"title\": item[\"Title\"], \"category\": mapperCategoriesNames(item[\"Category\"])}\n",
    "        yield f'{metadata}{os.linesep}{json.dumps(curr)}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = getPathFiles('/output','.tsv')\n",
    "test_data = test_data\n",
    "len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in test_data:\n",
    "    res = es.bulk(index='wiki', body=bulkMyData(file))\n",
    "    if res['errors']:\n",
    "        print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delete all records\n",
    "\n",
    "vymazanie všetkých indexov v elasticsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deleteIndex(elastic, index):\n",
    "    if elastic.indices.exists(index=index):\n",
    "        elastic.indices.delete(index=index)\n",
    "        print(f'Deleted index {index}')\n",
    "    else:\n",
    "        print(f'Index {index} not exist')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "deleteIndex(es, 'wiki')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding common categories\n",
    "\n",
    "V tejto časti je vykonávaná analáza kategórií článkov s cieľom zistenia kľučových slov v kategóriach pre naše kategórie. Na vykonanie tejto analýzy je použitá technika tf-idf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "import nltk\n",
    "from collections import Counter\n",
    "import threading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "def process_whole_sentence(text):\n",
    "    sen = ' '.join(w for w in text.split() if w not in stopwords)\n",
    "    sen = re.sub(r'\\W', ' ', str(sen))\n",
    "    sen = re.sub(r'\\s+', ' ', sen, flags=re.I)\n",
    "    sen = sen.lower()\n",
    "    return sen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bigrams(text):\n",
    "    bigrams = []\n",
    "    for sen in text:\n",
    "        token = nltk.word_tokenize(sen)\n",
    "        bigrams.append(list(map(lambda x: ' '.join(x),list(ngrams(token,2)))))\n",
    "    return bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_bigrams(data_cat):\n",
    "    categories_processed = []\n",
    "    \n",
    "    cat_per = list(reduce(lambda x,y: x+y[2]['categories'],filter(lambda x: x[3] == 'Person',data_cat),[]))\n",
    "    cat_com = list(reduce(lambda x,y: x+y[2]['categories'],filter(lambda x: x[3] in ['Company'],data_cat),[]))\n",
    "    cat_org = list(reduce(lambda x,y: x+y[2]['categories'],filter(lambda x: x[3] in ['Organization'],data_cat),[]))\n",
    "    cat_loc = list(reduce(lambda x,y: x+y[2]['categories'],filter(lambda x: x[3] in ['Location'],data_cat),[]))\n",
    "    \n",
    "    categories_processed.append(sum(get_bigrams(stemming(map(lambda x: process_whole_sentence(x),cat_per))),[]))\n",
    "    categories_processed.append(sum(get_bigrams(stemming(map(lambda x: process_whole_sentence(x),cat_com))),[]))\n",
    "    categories_processed.append(sum(get_bigrams(stemming(map(lambda x: process_whole_sentence(x),cat_org))),[]))\n",
    "    categories_processed.append(sum(get_bigrams(stemming(map(lambda x: process_whole_sentence(x),cat_loc))),[]))\n",
    "    \n",
    "    return categories_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words(data):\n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "    return [w for w in data if w not in stopwords]\n",
    "\n",
    "\n",
    "def tokenize(data):\n",
    "    symbols = symbols = \"!\\\"#$%&()*+'-./:;,<=>?@[\\]^_`{|}~\\n\"\n",
    "    tokens = word_tokenize(data)\n",
    "    tokens = [token.lower() for token in tokens if token not in list(symbols)]\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def stemming(data):\n",
    "    stemmer= PorterStemmer()\n",
    "    return [stemmer.stem(token) for token in data]\n",
    "\n",
    "\n",
    "def preprocess(data):\n",
    "    data = tokenize(data)\n",
    "    data = remove_stop_words(data)\n",
    "    data = stemming(data)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dfcount(data):\n",
    "    df = {}\n",
    "    for i in range(len(data)):\n",
    "        for token in data[i]:\n",
    "            try:\n",
    "                df[token].add(i)\n",
    "            except:\n",
    "                df[token] = {i}\n",
    "    for i in df:\n",
    "        df[i] = len(df[i])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_idf(data, doc_freq):\n",
    "    tfidf = {}\n",
    "    for i in range(len(data)):\n",
    "        counter = Counter(data[i])\n",
    "        count_w = len(data[i])\n",
    "        for token in np.unique(data[i]):\n",
    "            tf = counter[token]/count_w\n",
    "            df = doc_freq[token]\n",
    "            idf = np.log((len(data)+1)/(df+1))\n",
    "            tfidf[i, token] = tf*idf\n",
    "    return tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bigrams(text):\n",
    "    bigrams = []\n",
    "    for sen in text:\n",
    "        token = nltk.word_tokenize(sen)\n",
    "        bigrams.append(list(map(lambda x: ' '.join(x),list(ngrams(token,2)))))\n",
    "    return bigrams\n",
    "\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "def process_whole_sentence(text):\n",
    "    sen = ' '.join(w for w in text.split() if w not in stopwords)\n",
    "    sen = re.sub(r'\\W', ' ', str(sen))\n",
    "    sen = re.sub(r'\\s+', ' ', sen, flags=re.I)\n",
    "    sen = sen.lower()\n",
    "    return sen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spojenie predchádzajúcich funkcií spracovania textu, viet, slov a vypočítania tf-idf skóre, do jednej spúštatelnej funkcie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSignificanteCategories(limit=2000, write=True):\n",
    "    categories = []\n",
    "\n",
    "    data_cat = parseWiki(limit=limit ,test_sample=False, save=False)\n",
    "    categories.append(reduce(lambda x,y: x+preprocess(y),reduce(lambda x,y: x+y[2]['categories'],filter(lambda x: x[3] == 'Person',data_cat),[]),[]))\n",
    "    categories.append(reduce(lambda x,y: x+preprocess(y),reduce(lambda x,y: x+y[2]['categories'],filter(lambda x: x[3] in ['Company'],data_cat),[]),[]))\n",
    "    categories.append(reduce(lambda x,y: x+preprocess(y),reduce(lambda x,y: x+y[2]['categories'],filter(lambda x: x[3] in ['Organization'],data_cat),[]),[]))\n",
    "    categories.append(reduce(lambda x,y: x+preprocess(y),reduce(lambda x,y: x+y[2]['categories'],filter(lambda x: x[3] in ['Location'],data_cat),[]),[]))\n",
    "    \n",
    "    del data_cat\n",
    "    \n",
    "    DF = dfcount(categories)\n",
    "    tfidf = tf_idf(categories, DF)\n",
    "    \n",
    "    c_person = None\n",
    "    c_company = None\n",
    "    c_org = None\n",
    "    c_location = None\n",
    "    \n",
    "    def task1():\n",
    "        c_person = {term:x for (doc, term), x in tfidf.items() if doc == 0}\n",
    "        c_person = sorted(c_person, key=c_person.__getitem__,reverse=True)\n",
    "        print('Person: ', c_person[:20])\n",
    "\n",
    "    def task2():\n",
    "        c_company = {term:x for (doc, term), x in tfidf.items() if doc == 1}\n",
    "        c_company = sorted(c_company, key=c_company.__getitem__,reverse=True)\n",
    "        print('Company: ', c_company[:20])\n",
    "\n",
    "    def task3():\n",
    "        c_org = {term:x for (doc, term), x in tfidf.items() if doc == 2}\n",
    "        c_org = sorted(c_org, key=c_org.__getitem__,reverse=True)\n",
    "        print('Organisation: ', c_org[:20])\n",
    "\n",
    "    def task4():\n",
    "        c_location = {term:x for (doc, term), x in tfidf.items() if doc == 3}\n",
    "        c_location = sorted(c_location, key=c_location.__getitem__, reverse=True)\n",
    "        print('Location: ', c_location[:20])\n",
    "    \n",
    "    t1 = threading.Thread(target=task1, name='t1') \n",
    "    t2 = threading.Thread(target=task2, name='t2') \n",
    "    t3 = threading.Thread(target=task3, name='t3') \n",
    "    t4 = threading.Thread(target=task4, name='t4')\n",
    "    \n",
    "    t1.start()\n",
    "    t2.start()\n",
    "    t3.start()\n",
    "    t4.start()\n",
    "    \n",
    "    \n",
    "    t1.join()\n",
    "    t2.join()\n",
    "    t3.join()\n",
    "    t4.join()\n",
    "    \n",
    "    if write:\n",
    "        print('Person: ', c_person[:20])\n",
    "        print('')\n",
    "        print('Company: ', c_company[:20])\n",
    "        print('')\n",
    "        print('Organisation: ', c_org[:20])\n",
    "        print('')\n",
    "        print('Location: ', c_location[:20])\n",
    "    \n",
    "    return {'person': c_person, 'company': c_company, 'org': c_org, 'location': c_location}\n",
    "\n",
    "\n",
    "def getSignificanteCategoriesBigrams(limit=2000, write=True):\n",
    "    data_cat = parseWiki(limit=limit ,test_sample=False, save=False)\n",
    "    categories_processed = []\n",
    "    \n",
    "    cat_per = list(reduce(lambda x,y: x+y[2]['categories'],filter(lambda x: x[3] == 'Person',data_cat),[]))\n",
    "    cat_com = list(reduce(lambda x,y: x+y[2]['categories'],filter(lambda x: x[3] in ['Company'],data_cat),[]))\n",
    "    cat_org = list(reduce(lambda x,y: x+y[2]['categories'],filter(lambda x: x[3] in ['Organization'],data_cat),[]))\n",
    "    cat_loc = list(reduce(lambda x,y: x+y[2]['categories'],filter(lambda x: x[3] in ['Location'],data_cat),[]))\n",
    "    \n",
    "    categories_processed.append(sum(get_bigrams(stemming(map(lambda x: process_whole_sentence(x),cat_per))),[]))\n",
    "    categories_processed.append(sum(get_bigrams(stemming(map(lambda x: process_whole_sentence(x),cat_com))),[]))\n",
    "    categories_processed.append(sum(get_bigrams(stemming(map(lambda x: process_whole_sentence(x),cat_org))),[]))\n",
    "    categories_processed.append(sum(get_bigrams(stemming(map(lambda x: process_whole_sentence(x),cat_loc))),[]))\n",
    "    \n",
    "    del data_cat\n",
    "    \n",
    "    DF = dfcount(categories_processed)\n",
    "    tfidf = tf_idf(categories_processed, DF)\n",
    "    \n",
    "    #filtrovanie na základe požadovanej kategórie a usporiadanie na základe tf-idf skóre\n",
    "    c_person = {term:x for (doc, term), x in tfidf.items() if doc == 0}\n",
    "    c_person = sorted(c_person, key=c_person.__getitem__,reverse=True)\n",
    "\n",
    "    c_company = {term:x for (doc, term), x in tfidf.items() if doc == 1}\n",
    "    c_company = sorted(c_company, key=c_company.__getitem__,reverse=True)\n",
    "\n",
    "    c_org = {term:x for (doc, term), x in tfidf.items() if doc == 2}\n",
    "    c_org = sorted(c_org, key=c_org.__getitem__,reverse=True)\n",
    "\n",
    "    c_location = {term:x for (doc, term), x in tfidf.items() if doc == 3}\n",
    "    c_location = sorted(c_location, key=c_location.__getitem__, reverse=True)\n",
    "    \n",
    "    if write:\n",
    "        print('Person: ', c_person[:20])\n",
    "        print('')\n",
    "        print('Company: ', c_company[:20])\n",
    "        print('')\n",
    "        print('Organisation: ', c_org[:20])\n",
    "        print('')\n",
    "        print('Location: ', c_location[:20])\n",
    "    \n",
    "    return {'person': c_person, 'company': c_company, 'org': c_org, 'location': c_location}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Person:  ['living peopl', '20th century', 'f c', 'c play', 'century american', '21st century', 'american male', 'association football', 'league play', 'expatriate footballers', 'expatriate sportspeople', 'cup play', 'international footbal', 'rugby league', 'university alumni', 'fc play', 'musical groups', 'ice hockey', 'world cup', 'american people']\n",
      "\n",
      "Company:  ['companies established', 'companies based', 'companies united', 'services companies', 'financial services', 'mergers acquisit', 'american companies', 'stock exchanges', 'video game', 'game companies', 'manufacturing companies', 'chains united', 'companies listed', 'exchanges africa', 'restaurants established', 'stock exchang', 'companies disestablished', 'mass media', 'media companies', 'restaurant chains']\n",
      "\n",
      "Organisation:  ['based united', 'non profit', 'learned societies', 'profit organizations', 'organizations established', 'organisations based', 'organizations based', '3 organ', '501 c', 'associations based', 'c 3', 'charities based', 'consultative status', 'established 1946', 'professional associations', 'psychology organizations', 'relief organ', 'societies canada', 'status united', 'english football']\n",
      "\n",
      "Location:  ['pyrénées atlantiqu', 'communes pyrénées', 'articles needing', 'atlantiques communes', 'communes articles', 'french wikipedia', 'needing translation', 'pyrénées atlantiques', 'translation french', 'lower navarr', 'populated places', 'register historic', 'national register', 'historic places', 'towns luxembourg', 'unincorporated communities', 'civil parishes', 'buildings structures', 'neighborhoods pittsburgh', 'sports venues']\n"
     ]
    }
   ],
   "source": [
    "bires = getSignificanteCategoriesBigrams(limit=10000,write=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Person:  ['player', 'birth', 'male', 'death', 'expatri', 'peopl', 'alumni', 'live', 'sportspeopl', 'actor', 'writer', 'descent', 'footbal', '21st-centuri', 'cricket', '20th-centuri', 'singer', 'politician', 'musician', 'actress']\n",
      "Company:  ['exchang', 'brand', 'acquisit', 'merger', 'defunct', 'manufactur', 'softwar', 'label', 'cloth', 'vehicl', 'retail', 'disestablish', 'restaur', 'video', 'fast-food', 'nasdaq', 'onlin', 'publish', 'chain', 'stock']\n",
      "Organisation: Location:  ['station', 'build', 'pyrénées-atlantiqu', 'regist', 'popul', 'complet', 'place', 'airport', 'venu', 'town', 'school', 'aerodrom', 'need', 'counti', 'railway', 'villag', 'unincorpor', 'great', 'mountain', 'open']\n",
      " ['sahara', 'scout', 'youth', '501', 'gang', 'non-profit', 'polisario', 'chariti', 'c', 'learn', 'polit', 'societi', 'advocaci', 'ambul', 'anti-christian', 'anti-vaccin', 'child-rel', 'kazakhstan', 'multi-sport', 'non-government']\n"
     ]
    }
   ],
   "source": [
    "res = getSignificanteCategories(limit=30000, write=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printResults(result, limit):\n",
    "    print('Person: ',result['person'][:limit])\n",
    "    print('')\n",
    "    print('Company: ', result['company'][:limit])\n",
    "    print('')\n",
    "    print('Organisation: ',result['org'][:limit])\n",
    "    print('')\n",
    "    print('Location: ', result['location'][:limit])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Person:  ['living peopl', '20th century', 'f c', 'c play', 'century american', '21st century', 'american male', 'association football', 'league play', 'expatriate footballers', 'expatriate sportspeople', 'cup play', 'international footbal', 'rugby league', 'university alumni', 'fc play', 'musical groups', 'ice hockey', 'world cup', 'american people', 'fifa world', 'male actors', 'football league', 'male actor', 'expatriate footbal', 'military personnel', 'people educated', 'hockey players', 'male writ', 'records artist', 'draft pick', 'century indian', 'football manag', 'male television', 'film actor', 'uk mps', 'male film', 'soccer play', 'television actor', 'united f', 'year birth', 'living people', 'winter olymp', 'birth missing', 'football midfield', 'missing living', 'major league', 'school alumni', '19th century', 'new zealand']\n",
      "\n",
      "Company:  ['companies established', 'companies based', 'companies united', 'services companies', 'financial services', 'mergers acquisit', 'american companies', 'stock exchanges', 'video game', 'game companies', 'manufacturing companies', 'chains united', 'companies listed', 'exchanges africa', 'restaurants established', 'stock exchang', 'companies disestablished', 'mass media', 'media companies', 'restaurant chains', 'retail companies', 'companies filed', 'defunct companies', 'filed chapter', 'internet properties', 'properties established', 'retailers united', '11 bankruptcy', 'chapter 11', 'companies canada', 'companies formerly', 'established 1960', 'established 1995', 'fast food', 'formerly listed', 'listed new', 'manufacturers united', 'york stock', 'established 1989', 'establishments california', 'based austin', 'british companies', 'clothing companies', 'companies england', 'companies isle', 'development compani', 'established 1950', 'established 1974', 'established 2003', 'food chains']\n",
      "\n",
      "Organisation:  ['based united', 'non profit', 'learned societies', 'profit organizations', 'organizations established', 'organisations based', 'organizations based', '3 organ', '501 c', 'associations based', 'c 3', 'charities based', 'consultative status', 'established 1946', 'professional associations', 'psychology organizations', 'relief organ', 'societies canada', 'status united', 'english football', 'establishments united', 'youth organizations', 'united nations', '1845 establishments', '1859 establishments', '1864 establishments', '1907 establishments', '1908 establishments', '1959 establishments', '1982 establishments', '1996 establishments', '19th centuri', 'academy financial', 'advocacy organ', 'aid organ', 'air ambulance', 'ambulance servic', 'ambulance services', 'american council', 'american organized', 'ancient near', 'awards h', 'banks texa', 'bar associ', 'based geneva', 'based hong', 'based montr', 'based surrey', 'based switzerland', 'based tyne']\n",
      "\n",
      "Location:  ['pyrénées atlantiqu', 'communes pyrénées', 'articles needing', 'atlantiques communes', 'communes articles', 'french wikipedia', 'needing translation', 'pyrénées atlantiques', 'translation french', 'lower navarr', 'populated places', 'register historic', 'national register', 'historic places', 'towns luxembourg', 'unincorporated communities', 'civil parishes', 'buildings structures', 'neighborhoods pittsburgh', 'sports venues', 'western australia', 'parishes leicestershir', 'villages leicestershir', 'cities towns', 'protected areas', 'borough charnwood', 'places established', 'suburbs perth', 'west virginia', 'tourist attractions', 'buildings completed', 'city rockingham', 'suburbs city', 'shopping malls', 'new jersey', 'county west', 'historic house', 'house museums', 'venues completed', 'county california', 'county virginia', 'alzette canton', 'communes esch', 'county massachusett', 'esch sur', 'former communes', 'mountains hills', 'rhode island', 'road bridges', 'sur alzette']\n"
     ]
    }
   ],
   "source": [
    "printResults(bires,50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "300 000 articles cca 45 minutes need refactoring\n",
    "\n",
    "Person=['player', 'male', 'actor', 'sportspeopl', 'medalist', 'actress', 'expatri', 'singer', 'musician', 'live', 'writer', 'politician', 'f.c', 'alumni', 'personnel', 'olymp', '20th-centuri', 'faculti', 'coach', 'guitarist']\n",
    "\n",
    "Company=['brand', 'merger', 'retail', 'exchang', 'stock', 'label', 'nasdaq', 'multin', 'subsidiari', 'acquisit', 'onlin', 'offer', 'held', 'conglomer', 'drink', 'vehicl', 'softwar', 'equip', 'store', 'bankruptci']\n",
    "\n",
    "Organisation=['scout', 'think', 'non-profit', 'girl', 'gang', 'multi-sport', 'event', 'recur', 'religi', 'tank', 'child-rel', 'non-align', 'non-government', 'critic', 'right', 'chess', 'evangel', 'movement|', 'yakuza', 'advocaci']\n",
    "\n",
    "Location=['regist', 'unincorpor', 'station', 'popul', 'complet', 'aerodrom', 'villag', 'town', 'landform', 'parish', 'river', 'seaplan', 'open', 'census-design', 'mountain', 'attract', 'neighbourhood', 'suburb', 'rang', 'airport']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "300 000 first 100\n",
    "Person:  ['player', 'male', 'actor', 'sportspeopl', 'medalist', 'actress', 'expatri', 'singer', 'musician', 'live', 'writer', 'politician', 'f.c', 'alumni', 'personnel', 'olymp', '20th-centuri', 'faculti', 'coach', 'guitarist', 'basebal', 'novelist', 'emigr', 'descent', 'cup', '21st-centuri', 'mp', 'painter', 'femal', 'journalist', 'poet', 'compos', 'draft', 'pick', 'repres', '19th-centuri', 'summer', 'champion', 'screenwrit', 'lawyer', 'director', 'swimmer', 'soccer', 'forward', 'skater', 'burial', 'midfield', 'field', 'ice', 'gold', 'non-fict', 'basketbal', 'winter', 'recipi', 'comedian', 'fifa', 'filipino', 'businesspeopl', 'defend', 'senat', 'silver', 'major', 'songwrit', 'scientist', 'minist', 'medal', 'fc', 'medallist', 'staff', 'singer-songwrit', 'voic', 'scholar', 'fellow', 'boxer', 'wrestler', 'historian', 'pan', 'drummer', 'universiad', 'rock', 'figur', 'bundesliga', 'cemeteri', 'rugbi', 'bronz', 'pianist', 'dramatist', 'merit', 'playwright', 'cyclist', 'stage', 'inducte', 'mayor', 'under-21', 'activist', 'xi', 'republican', 'first', 'governor', 'presid']\n",
    "\n",
    "Company:  ['brand', 'merger', 'retail', 'exchang', 'stock', 'label', 'nasdaq', 'multin', 'subsidiari', 'acquisit', 'onlin', 'offer', 'held', 'conglomer', 'drink', 'vehicl', 'softwar', 'equip', 'store', 'bankruptci', 'file', 'cloth', 'non-renew', 'chapter', 'shoe', 'supermarket', 'initi', 'formerli', 'properti', 'publish', 'portfolio', 'chain', 'supplier', 'chocol', 'luxuri', 'tokyo', 'equiti', 'phone', 'applianc', 'part', 'ga', 'motor', 'truck', 'bakeri', 'group|', 'midwestern', 'toy', 'housebuild', 'web', 'hold', 'fashion', 'headquart', 'studio', 'breweri', '11', 'government-own', 'snack', 'spin-off', 'energi', 'fast-food', 'oil', 'pharmaceut', 'amplifi', 'eyewear', 'nationalis', 'encyclopedia', '2010', 'resourc', 'discontinu', 'euronext', 'outsourc', 'r.a', 're-establish', 'guitar', 'colorado', '2017', 'magazin', 'mobil', 'firearm', 'googl', 'warrant', '2008', 'indiana', 'pipelin', 'provid', 'chaebol', 'condiment', 'dairi', 'discount', 'index', 'mortgag', 'poultri', 'coffe', 'cosmet', 'distribut', 'fuel', '2020', 'consult', 'rock', 'station']\n",
    "\n",
    "Organisation:  ['scout', 'think', 'non-profit', 'girl', 'gang', 'multi-sport', 'event', 'recur', 'religi', 'tank', 'child-rel', 'non-align', 'non-government', 'critic', 'right', 'chess', 'evangel', 'movement|', 'yakuza', 'advocaci', 'patronag', 'usa', 'games|', 'sahara', 'accreditor', 'america|', 'associations|', 'association|', 'hispanic-american', 'ioc-recognis', 'lobbi', 'metalwork', 'polisario', 'supraorgan', '501', 'bolivia', 'femin', 'intergovernment', 'secret', 'traffick', 'learn', 'asian', 'publish', 'ambul', 'anti-abort', 'anti-vaccin', 'consortia', 'feminist', 'parachurch', 'shelter', 'veteran', 'diego', 'adi', 'advaita', 'anti-vivisect', 'awards|', 'caloust', 'churches|thailand', 'education|', 'federation|', 'foundation|', 'genet', 'gmb', 'gulbenkian', 'irredent', 'metric', 'pageants|california', 'philanthrop', 'positiv', 'puri', 'shankara', 'shankaracharya', 'states–european', 'sub-confeder', 'taxat', 'treati', 'trust|', 'vedanta', 'vexillolog', 'center', 'confeder', 'local', 'nebraska', 'olymp', 'anglican', 'denomin', 'labor', 'missionari', 'scientolog', 'welfar', '1778', 'activist', 'anti-christian', 'biblic', 'carpent', 'certif', 'combat', 'emerg', 'homeless', 'israeli–palestinian']\n",
    "\n",
    "Location:  ['regist', 'unincorpor', 'station', 'popul', 'complet', 'aerodrom', 'villag', 'town', 'landform', 'parish', 'river', 'seaplan', 'open', 'census-design', 'mountain', 'attract', 'neighbourhood', 'suburb', 'rang', 'airport', 'certifi', 'secondari', 'district|', 'site', 'skyscrap', 'pyrénées-atlantiqu', 'basketbal', 'stadium', 'demolish', 'need', 'vaud', 'coast', 'tributari', 'arena', 'neighborhood', 'dam', 'tunnel', 'saskatchewan', 'monument', 'serv', 'multi-purpos', 'mall', 'lighthous', 'pradesh', 'locat', 'volcano', 'norfolk', 'coastal', 'mojav', 'territori', 'canton', 'township', 'subprefectur', 'desert', 'volleybal', 'derbyshir', 'grassland', 'hill', 'censu', 'castl', 'casino', 'landmark', 'governor', 'voivodeship', 'glacier', 'line', 'valley', 'residenti', 'subway', 'nova', 'colorado', 'close', 'scotia', 'princ', 'reservoir', 'grade', 'offic', 'properti', 'abellio', 'scotrail', 'local', 'indoor', 'lrt', 'uninhabit', 'metropolitan', 'oklahoma', 'suffolk', 'wikipedia', 'montana', 'translat', 'cumbria', 'indiana', 'dioces', 'sculptur', 'divis', 'punggol', 'navarr', 'instal', 'reserv', 'verd']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Person:  ['living peopl', '20th century', 'f c', 'c play', 'century american', '21st century', 'american male', 'association football', 'league play', 'expatriate footballers', 'expatriate sportspeople', 'cup play', 'international footbal', 'rugby league', 'university alumni', 'fc play', 'musical groups', 'ice hockey', 'world cup', 'american people', 'fifa world', 'male actors', 'football league', 'male actor', 'expatriate footbal', 'military personnel', 'people educated', 'hockey players', 'male writ', 'records artist', 'draft pick', 'century indian', 'football manag', 'male television', 'film actor', 'uk mps', 'male film', 'soccer play', 'television actor', 'united f', 'year birth', 'living people', 'winter olymp', 'birth missing', 'football midfield', 'missing living', 'major league', 'school alumni', '19th century', 'new zealand']\n",
    "\n",
    "Company:  ['companies established', 'companies based', 'companies united', 'services companies', 'financial services', 'mergers acquisit', 'american companies', 'stock exchanges', 'video game', 'game companies', 'manufacturing companies', 'chains united', 'companies listed', 'exchanges africa', 'restaurants established', 'stock exchang', 'companies disestablished', 'mass media', 'media companies', 'restaurant chains', 'retail companies', 'companies filed', 'defunct companies', 'filed chapter', 'internet properties', 'properties established', 'retailers united', '11 bankruptcy', 'chapter 11', 'companies canada', 'companies formerly', 'established 1960', 'established 1995', 'fast food', 'formerly listed', 'listed new', 'manufacturers united', 'york stock', 'established 1989', 'establishments california', 'based austin', 'british companies', 'clothing companies', 'companies england', 'companies isle', 'development compani', 'established 1950', 'established 1974', 'established 2003', 'food chains']\n",
    "\n",
    "Organisation:  ['based united', 'non profit', 'learned societies', 'profit organizations', 'organizations established', 'organisations based', 'organizations based', '3 organ', '501 c', 'associations based', 'c 3', 'charities based', 'consultative status', 'established 1946', 'professional associations', 'psychology organizations', 'relief organ', 'societies canada', 'status united', 'english football', 'establishments united', 'youth organizations', 'united nations', '1845 establishments', '1859 establishments', '1864 establishments', '1907 establishments', '1908 establishments', '1959 establishments', '1982 establishments', '1996 establishments', '19th centuri', 'academy financial', 'advocacy organ', 'aid organ', 'air ambulance', 'ambulance servic', 'ambulance services', 'american council', 'american organized', 'ancient near', 'awards h', 'banks texa', 'bar associ', 'based geneva', 'based hong', 'based montr', 'based surrey', 'based switzerland', 'based tyne']\n",
    "\n",
    "Location:  ['pyrénées atlantiqu', 'communes pyrénées', 'articles needing', 'atlantiques communes', 'communes articles', 'french wikipedia', 'needing translation', 'pyrénées atlantiques', 'translation french', 'lower navarr', 'populated places', 'register historic', 'national register', 'historic places', 'towns luxembourg', 'unincorporated communities', 'civil parishes', 'buildings structures', 'neighborhoods pittsburgh', 'sports venues', 'western australia', 'parishes leicestershir', 'villages leicestershir', 'cities towns', 'protected areas', 'borough charnwood', 'places established', 'suburbs perth', 'west virginia', 'tourist attractions', 'buildings completed', 'city rockingham', 'suburbs city', 'shopping malls', 'new jersey', 'county west', 'historic house', 'house museums', 'venues completed', 'county california', 'county virginia', 'alzette canton', 'communes esch', 'county massachusett', 'esch sur', 'former communes', 'mountains hills', 'rhode island', 'road bridges', 'sur alzette']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overenie pridelovania kategorii"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[79537     5     6     9  1207]\n",
      " [   13  4924    69    20   682]\n",
      " [   37   164  1143    40   625]\n",
      " [  721   414   230 45134 10983]\n",
      " [ 6153  2428  1923  8684 73095]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.98      0.95     80764\n",
      "           1       0.62      0.86      0.72      5708\n",
      "           2       0.34      0.57      0.42      2009\n",
      "           3       0.84      0.79      0.81     57482\n",
      "           4       0.84      0.79      0.82     92283\n",
      "\n",
      "    accuracy                           0.86    238246\n",
      "   macro avg       0.71      0.80      0.75    238246\n",
      "weighted avg       0.86      0.86      0.86    238246\n",
      "\n",
      "0.8555568613953645\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Získanie súborov, ktoré boli spracované, nachádzajúce sa v adresári ./output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_path = os.getcwd().rsplit('/', 1)[0]\n",
    "data_files_original = f'{out_path}/output/'\n",
    "data_files_original = [file for file in os.listdir(data_files_original) if file.endswith('.tsv')]\n",
    "len(data_files_original)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vytváranie testovacieho datasetu spôsobom filtrovania záznamov, ktoré sú považované za ground truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = []\n",
    "y_pred = []\n",
    "\n",
    "for x in data_files_original:\n",
    "    original = readTsv(f'{out_path}/output/{x}')\n",
    "    evaluated = readTsv(f'{out_path}/output/eval/{x}')\n",
    "    for o, e in zip(original, evaluated):\n",
    "        if o['Category'] in [\"Person\",'Company','Organization','Location','Another']:\n",
    "            y_test.append(mapperCategories(o['Category']))\n",
    "            y_pred.append(mapperCategories(e['Category']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Celková veľkosť testovacieho setu je 238 246 záznamov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "238246"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "V tejto časti sú zobrazené výsledky z viacerých spôsobov/variacií prideľovania kategórií"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Pridelovanie kategórií na základe bigramov a unigramov súčastne, t.j. zhoda v jednom alebo druhom.\n",
    "\n",
    "\n",
    "[[79693     6    17    23  1025]\n",
    " [   22  5196    57    25   408]\n",
    " [   63   174  1220    41   511]\n",
    " [  744   420   262 50426  5630]\n",
    " [ 9813  2925  5506 11094 62945]]\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.88      0.99      0.93     80764\n",
    "           1       0.60      0.91      0.72      5708\n",
    "           2       0.17      0.61      0.27      2009\n",
    "           3       0.82      0.88      0.85     57482\n",
    "           4       0.89      0.68      0.77     92283\n",
    "\n",
    "    accuracy                           0.84    238246\n",
    "   macro avg       0.67      0.81      0.71    238246\n",
    "weighted avg       0.86      0.84      0.84    238246\n",
    "\n",
    "0.8372858306120564\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Pridelovanie kategórií použitím \"len\" unigramov.\n",
    "\n",
    "\n",
    "[[76889    19    26    21  3809]\n",
    " [    9  4810    36    34   819]\n",
    " [   33    33   765    49  1129]\n",
    " [   48   244   127 50533  6530]\n",
    " [ 5034   771  4869 10830 70779]]\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.94      0.95      0.94     80764\n",
    "           1       0.82      0.84      0.83      5708\n",
    "           2       0.13      0.38      0.20      2009\n",
    "           3       0.82      0.88      0.85     57482\n",
    "           4       0.85      0.77      0.81     92283\n",
    "\n",
    "    accuracy                           0.86    238246\n",
    "   macro avg       0.71      0.76      0.73    238246\n",
    "weighted avg       0.87      0.86      0.86    238246\n",
    "\n",
    "0.8553176128875196\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Pridelovanie kategórií \"len\" na základe bigramov.\n",
    "\n",
    "\n",
    "[[79537     5     6     9  1207]\n",
    " [   13  4924    69    20   682]\n",
    " [   37   164  1143    40   625]\n",
    " [  721   414   230 45134 10983]\n",
    " [ 6153  2428  1923  8684 73095]]\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.92      0.98      0.95     80764\n",
    "           1       0.62      0.86      0.72      5708\n",
    "           2       0.34      0.57      0.42      2009\n",
    "           3       0.84      0.79      0.81     57482\n",
    "           4       0.84      0.79      0.82     92283\n",
    "\n",
    "    accuracy                           0.86    238246\n",
    "   macro avg       0.71      0.80      0.75    238246\n",
    "weighted avg       0.86      0.86      0.86    238246\n",
    "\n",
    "0.8555568613953645\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pri týchto testovaniach sa netestovala kategória \"other\", preto tieto výsledky nemožno považovať za 100% správne"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Vysledok ako predosle, ale bez pridelovania na zaklade zoznamu vytvorenemu kategori\n",
    "\n",
    "[[72236     0     8     0  8520]\n",
    " [    0  4206    19     6  1477]\n",
    " [    0    13   377    23  1596]\n",
    " [    2   231    79 39294 17876]\n",
    " [    0     0     0     0     0]]\n",
    "\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       1.00      0.89      0.94     80764\n",
    "           1       0.95      0.74      0.83      5708\n",
    "           2       0.78      0.19      0.30      2009\n",
    "           3       1.00      0.68      0.81     57482\n",
    "           4       0.00      0.00      0.00         0\n",
    "\n",
    "    accuracy                           0.80    145963\n",
    "   macro avg       0.74      0.50      0.58    145963\n",
    "weighted avg       0.99      0.80      0.88    145963\n",
    "\n",
    "0.795496118879442\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "V7sledok ako predosle, ale categorie pridelovanie na zaklade bigrams\n",
    "\n",
    "[[79537     5     6     9  1207]\n",
    " [   13  4924    69    20   682]\n",
    " [   37   164  1143    40   625]\n",
    " [  721   414   230 45134 10983]\n",
    " [    0     0     0     0     0]]\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.99      0.98      0.99     80764\n",
    "           1       0.89      0.86      0.88      5708\n",
    "           2       0.79      0.57      0.66      2009\n",
    "           3       1.00      0.79      0.88     57482\n",
    "           4       0.00      0.00      0.00         0\n",
    "\n",
    "    accuracy                           0.90    145963\n",
    "   macro avg       0.73      0.64      0.68    145963\n",
    "weighted avg       0.99      0.90      0.94    145963\n",
    "\n",
    "0.8956927440515747\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Celkový počet počet spracovaných článkov je: 15 669 453, z toho je 6 124 163 článkov a 9 545 290 redirectov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6124163\n",
      "9545290\n"
     ]
    }
   ],
   "source": [
    "size = 0\n",
    "redirectSize = 0\n",
    "files = getPathFiles('/output','.tsv')\n",
    "redirects = getPathFiles('/output','-redirects')\n",
    "for file in files:\n",
    "    size += len(readTsv(file))\n",
    "for file in redirects:\n",
    "    redirectSize += len(readTsv(file))\n",
    "print(size)\n",
    "print(redirectSize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kategória Person je počet: 1 834 543\n",
    "\n",
    "Kategória Location je počet: 1 038 893\n",
    "\n",
    "Kategória Organization je počet: 69 638\n",
    "\n",
    "Kategória Company je počet: 116 931\n",
    "\n",
    "Ostatne: 3 064 158"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Person': 1834543, 'Organization': 69638, 'Location': 1038893, 'Company': 116931, 'Another': 3064158}\n"
     ]
    }
   ],
   "source": [
    "results = { \"Person\": 0, \"Organization\": 0, \"Location\": 0, \"Company\": 0, \"Another\": 0 }\n",
    "\n",
    "files = getPathFiles('/output','.tsv')\n",
    "for file in files:\n",
    "    for item in readTsv(file):\n",
    "        results[mapperCategoriesNames(item['Category'])] += 1\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Najviac redirectov má:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "redirecttargets = []\n",
    "redirects = getPathFiles('/output','-redirects')\n",
    "for file in redirects:\n",
    "    for item in readTsv(file):\n",
    "        redirecttargets.append(item[\"Source\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = Counter(redirecttargets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hangul \t 7072\n",
      "Private Use Areas \t 6410\n",
      "Category:World Current Research Publishing academic journals \t 3948\n",
      "OMICS Publishing Group \t 2848\n",
      "Category:British Open Research Publications academic journals \t 2172\n",
      "Category:European Union Research Publishing academic journals \t 1850\n",
      "Category:Eurasian Research Publishing academic journals \t 1816\n",
      "Category:North American Research Publishing academic journals \t 1810\n",
      "Category:Academic Knowledge and Research Publishing academic journals \t 1785\n",
      "Category:American Research Publications academic journals \t 1695\n",
      "Category:Academic and Scientific Publishing academic journals \t 1616\n",
      "Category:Canadian Research Publication academic journals \t 1594\n",
      "Category:Asian and American Research Publishing Group academic journals \t 1513\n",
      "Category:Science and Technology Publishing academic journals \t 1445\n",
      "Category:Research and Knowledge Publication academic journals \t 1380\n",
      "Science Publishing Group \t 1164\n",
      "Habeas corpus petitions of Guantanamo Bay detainees \t 1087\n",
      "Chlaenius \t 897\n",
      "Scientific Research Publishing \t 859\n",
      "List of Saudi detainees at Guantanamo Bay \t 698\n"
     ]
    }
   ],
   "source": [
    "for title, count in counter.most_common(20):\n",
    "    print(f'{title} \\t {count}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Najbežnejšie písmená su a,e,i"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "  \t 11842687\n",
    "a \t 11630400\n",
    "e \t 10625800\n",
    "i \t 9309190\n",
    "o \t 8057637\n",
    "n \t 7823001\n",
    "r \t 7786106\n",
    "s \t 7026456\n",
    "t \t 6838753\n",
    "l \t 6223618\n",
    "c \t 4196236\n",
    "h \t 3673594\n",
    "m \t 3637044\n",
    "d \t 3505580\n",
    "u \t 3369260\n",
    "p \t 2783301\n",
    "g \t 2534380\n",
    "b \t 2150971\n",
    "f \t 1896078\n",
    "y \t 1862938\n",
    "k \t 1837845\n",
    "w \t 1454121\n",
    "v \t 1270818\n",
    "j \t 766927\n",
    "( \t 701948\n",
    ") \t 701937\n",
    "1 \t 638944\n",
    "0 \t 531615\n",
    ". \t 458932\n",
    "z \t 447911\n",
    "2 \t 430345\n",
    ", \t 417407\n",
    "- \t 388987\n",
    "9 \t 366907\n",
    ": \t 350662\n",
    "/ \t 302285\n",
    "x \t 233740\n",
    "' \t 193193\n",
    "8 \t 172797\n",
    "q \t 161575\n",
    "– \t 149396\n",
    "7 \t 141223\n",
    "3 \t 138191\n",
    "6 \t 136593\n",
    "5 \t 136586\n",
    "4 \t 133466\n",
    "é \t 80878\n",
    "á \t 48949\n",
    "ó \t 40028\n",
    "í \t 31623\n",
    "ö \t 23485\n",
    "ü \t 21968\n",
    "& \t 21726\n",
    "ł \t 20167\n",
    "ć \t 17688\n",
    "š \t 17379\n",
    "ä \t 13312\n",
    "č \t 13036\n",
    "ç \t 12403\n",
    "ō \t 11614\n",
    "è \t 11584\n",
    "ı \t 11448\n",
    "! \t 11181\n",
    "ø \t 9463\n",
    "ñ \t 8079\n",
    "ú \t 7439\n",
    "ã \t 7097\n",
    "ž \t 6810\n",
    "â \t 6491\n",
    "ş \t 5982\n",
    "ă \t 5891\n",
    "\" \t 5780\n",
    "å \t 5589\n",
    "ę \t 5516\n",
    "ś \t 5200\n",
    "ń \t 4890\n",
    "ș \t 4690\n",
    "? \t 4387\n",
    "ż \t 4124\n",
    "ë \t 3877\n",
    "ğ \t 3777\n",
    "à \t 3523\n",
    "ô \t 3276\n",
    "ą \t 3258\n",
    "ū \t 3216\n",
    "ə \t 3161\n",
    "ř \t 3027\n",
    "ā \t 2979\n",
    "đ \t 2571\n",
    "ß \t 2548\n",
    "ê \t 2441\n",
    "ț \t 2298\n",
    "ý \t 2270\n",
    "æ \t 2245\n",
    "+ \t 2237\n",
    "× \t 2233\n",
    "ź \t 1955\n",
    "ě \t 1886\n",
    "ï \t 1806\n",
    "ð \t 1596\n",
    "̇ \t 1539\n",
    "õ \t 1416\n",
    "î \t 1318\n",
    "ė \t 1314\n",
    "ő \t 1188\n",
    "— \t 1115\n",
    "ò \t 1033"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = Counter()\n",
    "files = getPathFiles('/output','.tsv')\n",
    "for file in files:\n",
    "    for item in readTsv(file):\n",
    "        counter.update(list(item[\"Title\"].lower()))\n",
    "for char, count in counter.most_common():\n",
    "    print(f'{char} \\t {count}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = Counter()\n",
    "files = getPathFiles('/output','.tsv')\n",
    "for file in files:\n",
    "    for item in readTsv(file):\n",
    "        counter.update(tokenize(item[\"Title\"]))\n",
    "for word, count in counter.most_common(20):\n",
    "    print(f'{word} \\t {count}')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "of \t 448800\n",
    "the \t 303773\n",
    "wikipedia \t 153080\n",
    "in \t 144755\n",
    "'s \t 131279\n",
    "file \t 118292\n",
    "list \t 109030\n",
    "and \t 98020\n",
    "wikiproject \t 72218\n",
    "film \t 58601\n",
    "de \t 57711\n",
    "– \t 54951\n",
    "station \t 54933\n",
    "district \t 54087\n",
    "county \t 52967\n",
    "for \t 51847\n",
    "at \t 51578\n",
    "john \t 51021\n",
    "album \t 50338\n",
    "season \t 48882\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter3 = Counter()\n",
    "files = getPathFiles('/output','.tsv')\n",
    "for file in files:\n",
    "    for item in readTsv(file):\n",
    "        if(mapperCategoriesNames(item[\"Category\"]) == \"Person\"):\n",
    "            counter3.update(tokenize(item[\"Title\"]))\n",
    "for word, count in counter3.most_common(20):\n",
    "    print(f'{word} \\t {count}')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "john \t 42646\n",
    "of \t 39000\n",
    "footballer \t 28234\n",
    "william \t 24680\n",
    "de \t 21756\n",
    "james \t 18804\n",
    "politician \t 17272\n",
    "david \t 17224\n",
    "robert \t 16892\n",
    "george \t 16851\n",
    "thomas \t 16070\n",
    "the \t 15931\n",
    "born \t 15802\n",
    "charles \t 14344\n",
    "in \t 11954\n",
    "michael \t 11895\n",
    "henry \t 11267\n",
    "richard \t 11173\n",
    "band \t 11157\n",
    "peter \t 11144"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
