{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vytvorenie slovníka dvojíc pre účely Named Entity Recognizing\n",
    "#### Creating a dictionary of pairs for the purposes of Named Entity Recognizing: Wiki page - type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Projekt je momentalne rozdeleny do 2 časti.\n",
    "\n",
    "1. časť tvorí stahovanie potrebných súborov(wikipedia dump) na účely spracovania v projekte.\n",
    "2. časť tvorí parsovanie súborov spolu s priradením kategorie jednotlivym clankom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Part : Downloading Wikipedia articles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import re\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stiahnutie dát zo stránky wikipédie. Vyfiltrovanie všetkých súborov, ktoré obsahujú v názve \"pages-articles\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<!DOCTYPE html '"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_url = 'https://dumps.wikimedia.org/enwiki/20201001/'\n",
    "base_html = requests.get(base_url).text\n",
    "base_html[:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<li class=\"file\"><a href=\"/enwiki/20201001/enwiki-20201001-pages-articles-multistream.xml.bz2\">enwiki-20201001-pages-articles-multistream.xml.bz2</a> 17.5 GB</li>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup_dump = BeautifulSoup(base_html, 'html.parser')\n",
    "soup_dump.find_all('li', {'class': 'file'}, limit = 10)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('enwiki-20201001-pages-articles-multistream.xml.bz2', ['17.5', 'GB']),\n",
       " ('enwiki-20201001-pages-articles-multistream-index.txt.bz2', ['215.8', 'MB']),\n",
       " ('enwiki-20201001-pages-articles-multistream1.xml-p1p41242.bz2',\n",
       "  ['231.7', 'MB']),\n",
       " ('enwiki-20201001-pages-articles-multistream-index1.txt-p1p41242.bz2',\n",
       "  ['222', 'KB']),\n",
       " ('enwiki-20201001-pages-articles-multistream2.xml-p41243p151573.bz2',\n",
       "  ['313.2', 'MB'])]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files = []\n",
    "for file in soup_dump.find_all('li', {'class': 'file'}):\n",
    "    text = file.text\n",
    "    if 'pages-articles' in text:\n",
    "        files.append((text.split()[0], text.split()[1:]))\n",
    "files[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['enwiki-20201001-pages-articles1.xml-p1p41242.bz2',\n",
       " 'enwiki-20201001-pages-articles2.xml-p41243p151573.bz2',\n",
       " 'enwiki-20201001-pages-articles3.xml-p151574p311329.bz2',\n",
       " 'enwiki-20201001-pages-articles4.xml-p311330p558391.bz2',\n",
       " 'enwiki-20201001-pages-articles5.xml-p558392p958045.bz2']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files_to_download = [file[0] for file in files if re.search('pages-articles\\d{1,2}.xml-p',file[0])]\n",
    "files_to_download[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Použitie knižnice keras na stiahnutie týchto súborov/datasetu. Stiahnú sa len tie súbory, ktoré ešte nie sú stahnuté"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from keras.utils import get_file\n",
    "directory = '/home/xminarikd/.keras/datasets/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading\n",
      "Downloading data from https://dumps.wikimedia.org/enwiki/20201001/enwiki-20201001-pages-articles1.xml-p1p41242.bz2\n",
      "242098176/242093817 [==============================] - 330s 1us/step\n",
      "Downloading\n",
      "Downloading data from https://dumps.wikimedia.org/enwiki/20201001/enwiki-20201001-pages-articles2.xml-p41243p151573.bz2\n",
      "324780032/324777650 [==============================] - 445s 1us/step\n",
      "Downloading\n",
      "Downloading data from https://dumps.wikimedia.org/enwiki/20201001/enwiki-20201001-pages-articles3.xml-p151574p311329.bz2\n",
      "352124928/352119906 [==============================] - 313s 1us/step\n",
      "Downloading\n",
      "Downloading data from https://dumps.wikimedia.org/enwiki/20201001/enwiki-20201001-pages-articles4.xml-p311330p558391.bz2\n",
      "389988352/389987127 [==============================] - 372s 1us/step\n",
      "Downloading\n",
      "Downloading data from https://dumps.wikimedia.org/enwiki/20201001/enwiki-20201001-pages-articles5.xml-p558392p958045.bz2\n",
      "420814848/420806959 [==============================] - 471s 1us/step\n",
      "Downloading\n",
      "Downloading data from https://dumps.wikimedia.org/enwiki/20201001/enwiki-20201001-pages-articles6.xml-p958046p1483661.bz2\n",
      "450748416/450745879 [==============================] - 553s 1us/step\n",
      "Downloading\n",
      "Downloading data from https://dumps.wikimedia.org/enwiki/20201001/enwiki-20201001-pages-articles7.xml-p1483662p2134111.bz2\n",
      "462512128/462504094 [==============================] - 385s 1us/step\n",
      "Downloading\n",
      "Downloading data from https://dumps.wikimedia.org/enwiki/20201001/enwiki-20201001-pages-articles8.xml-p2134112p2936260.bz2\n",
      "471654400/471652241 [==============================] - 344s 1us/step\n",
      "Downloading\n",
      "Downloading data from https://dumps.wikimedia.org/enwiki/20201001/enwiki-20201001-pages-articles9.xml-p2936261p4045402.bz2\n",
      "512147456/512145263 [==============================] - 485s 1us/step\n",
      "Downloading\n",
      "Downloading data from https://dumps.wikimedia.org/enwiki/20201001/enwiki-20201001-pages-articles10.xml-p4045403p5399366.bz2\n",
      "502456320/502449820 [==============================] - 357s 1us/step\n",
      "Downloading\n",
      "Downloading data from https://dumps.wikimedia.org/enwiki/20201001/enwiki-20201001-pages-articles11.xml-p5399367p6899366.bz2\n",
      "486776832/486770345 [==============================] - 393s 1us/step\n",
      "Downloading\n",
      "Downloading data from https://dumps.wikimedia.org/enwiki/20201001/enwiki-20201001-pages-articles11.xml-p6899367p7054859.bz2\n",
      "46866432/46859027 [==============================] - 69s 1us/step\n",
      "Downloading\n",
      "Downloading data from https://dumps.wikimedia.org/enwiki/20201001/enwiki-20201001-pages-articles12.xml-p7054860p8554859.bz2\n",
      "404406272/404401272 [==============================] - 624s 2us/step\n",
      "Downloading\n",
      "Downloading data from https://dumps.wikimedia.org/enwiki/20201001/enwiki-20201001-pages-articles12.xml-p8554860p9172788.bz2\n",
      "164061184/164054426 [==============================] - 131s 1us/step\n",
      "Downloading\n",
      "Downloading data from https://dumps.wikimedia.org/enwiki/20201001/enwiki-20201001-pages-articles13.xml-p9172789p10672788.bz2\n",
      "330563584/330555755 [==============================] - 278s 1us/step\n",
      "Downloading\n",
      "Downloading data from https://dumps.wikimedia.org/enwiki/20201001/enwiki-20201001-pages-articles13.xml-p10672789p11659682.bz2\n",
      "229384192/229381099 [==============================] - 205s 1us/step\n",
      "Downloading\n",
      "Downloading data from https://dumps.wikimedia.org/enwiki/20201001/enwiki-20201001-pages-articles14.xml-p11659683p13159682.bz2\n",
      "393928704/393922075 [==============================] - 348s 1us/step\n",
      "Downloading\n",
      "Downloading data from https://dumps.wikimedia.org/enwiki/20201001/enwiki-20201001-pages-articles14.xml-p13159683p14324602.bz2\n",
      "273661952/273655291 [==============================] - 271s 1us/step\n",
      "Downloading\n",
      "Downloading data from https://dumps.wikimedia.org/enwiki/20201001/enwiki-20201001-pages-articles15.xml-p14324603p15824602.bz2\n",
      "355713024/355706040 [==============================] - 263s 1us/step\n",
      "Downloading\n",
      "Downloading data from https://dumps.wikimedia.org/enwiki/20201001/enwiki-20201001-pages-articles15.xml-p15824603p17324602.bz2\n",
      "307257344/307257124 [==============================] - 317s 1us/step\n",
      "Downloading\n",
      "Downloading data from https://dumps.wikimedia.org/enwiki/20201001/enwiki-20201001-pages-articles15.xml-p17324603p17460152.bz2\n",
      "28254208/28249729 [==============================] - 111s 4us/step\n",
      "Downloading\n",
      "Downloading data from https://dumps.wikimedia.org/enwiki/20201001/enwiki-20201001-pages-articles16.xml-p17460153p18960152.bz2\n",
      "336871424/336865577 [==============================] - 308s 1us/step\n",
      "Downloading\n",
      "Downloading data from https://dumps.wikimedia.org/enwiki/20201001/enwiki-20201001-pages-articles16.xml-p18960153p20460152.bz2\n",
      "314253312/314246115 [==============================] - 205s 1us/step\n",
      "Downloading\n",
      "Downloading data from https://dumps.wikimedia.org/enwiki/20201001/enwiki-20201001-pages-articles16.xml-p20460153p20570392.bz2\n",
      "22953984/22949874 [==============================] - 35s 2us/step\n",
      "Downloading\n",
      "Downloading data from https://dumps.wikimedia.org/enwiki/20201001/enwiki-20201001-pages-articles17.xml-p20570393p22070392.bz2\n",
      "351920128/351918604 [==============================] - 228s 1us/step\n",
      "Downloading\n",
      "Downloading data from https://dumps.wikimedia.org/enwiki/20201001/enwiki-20201001-pages-articles17.xml-p22070393p23570392.bz2\n",
      "362340352/362336803 [==============================] - 221s 1us/step\n",
      "Downloading\n",
      "Downloading data from https://dumps.wikimedia.org/enwiki/20201001/enwiki-20201001-pages-articles17.xml-p23570393p23716197.bz2\n",
      "40402944/40402767 [==============================] - 27s 1us/step\n",
      "Downloading\n",
      "Downloading data from https://dumps.wikimedia.org/enwiki/20201001/enwiki-20201001-pages-articles18.xml-p23716198p25216197.bz2\n",
      "375750656/375742870 [==============================] - 295s 1us/step\n",
      "Downloading\n",
      "Downloading data from https://dumps.wikimedia.org/enwiki/20201001/enwiki-20201001-pages-articles18.xml-p25216198p26716197.bz2\n",
      "347947008/347946542 [==============================] - 251s 1us/step\n",
      "Downloading\n",
      "Downloading data from https://dumps.wikimedia.org/enwiki/20201001/enwiki-20201001-pages-articles18.xml-p26716198p27121850.bz2\n",
      "87384064/87377512 [==============================] - 119s 1us/step\n",
      "Downloading\n",
      "Downloading data from https://dumps.wikimedia.org/enwiki/20201001/enwiki-20201001-pages-articles19.xml-p27121851p28621850.bz2\n",
      "337952768/337946504 [==============================] - 229s 1us/step\n",
      "Downloading\n",
      "Downloading data from https://dumps.wikimedia.org/enwiki/20201001/enwiki-20201001-pages-articles19.xml-p28621851p30121850.bz2\n",
      "297205760/297201089 [==============================] - 213s 1us/step\n",
      "Downloading\n",
      "Downloading data from https://dumps.wikimedia.org/enwiki/20201001/enwiki-20201001-pages-articles19.xml-p30121851p31308442.bz2\n",
      "281026560/281023102 [==============================] - 159s 1us/step\n",
      "Downloading\n",
      "Downloading data from https://dumps.wikimedia.org/enwiki/20201001/enwiki-20201001-pages-articles20.xml-p31308443p32808442.bz2\n",
      "383336448/383334873 [==============================] - 241s 1us/step\n",
      "Downloading\n",
      "Downloading data from https://dumps.wikimedia.org/enwiki/20201001/enwiki-20201001-pages-articles20.xml-p32808443p34308442.bz2\n",
      "349700096/349699080 [==============================] - 394s 1us/step\n",
      "Downloading\n",
      "Downloading data from https://dumps.wikimedia.org/enwiki/20201001/enwiki-20201001-pages-articles20.xml-p34308443p35522432.bz2\n",
      "259284992/259278058 [==============================] - 153s 1us/step\n",
      "Downloading\n",
      "Downloading data from https://dumps.wikimedia.org/enwiki/20201001/enwiki-20201001-pages-articles21.xml-p35522433p37022432.bz2\n",
      "351600640/351597304 [==============================] - 515s 1us/step\n",
      "Downloading\n",
      "Downloading data from https://dumps.wikimedia.org/enwiki/20201001/enwiki-20201001-pages-articles21.xml-p37022433p38522432.bz2\n",
      "340942848/340936052 [==============================] - 502s 1us/step\n",
      "Downloading\n",
      "Downloading data from https://dumps.wikimedia.org/enwiki/20201001/enwiki-20201001-pages-articles21.xml-p38522433p39996245.bz2\n",
      "345997312/345996096 [==============================] - 232s 1us/step\n",
      "Downloading\n",
      "Downloading data from https://dumps.wikimedia.org/enwiki/20201001/enwiki-20201001-pages-articles22.xml-p39996246p41496245.bz2\n",
      "340557824/340555294 [==============================] - 231s 1us/step\n",
      "Downloading\n",
      "Downloading data from https://dumps.wikimedia.org/enwiki/20201001/enwiki-20201001-pages-articles22.xml-p41496246p42996245.bz2\n",
      "351027200/351022715 [==============================] - 304s 1us/step\n",
      "Downloading\n",
      "Downloading data from https://dumps.wikimedia.org/enwiki/20201001/enwiki-20201001-pages-articles22.xml-p42996246p44496245.bz2\n",
      "354238464/354232150 [==============================] - 347s 1us/step\n",
      "Downloading\n",
      "Downloading data from https://dumps.wikimedia.org/enwiki/20201001/enwiki-20201001-pages-articles22.xml-p44496246p44788941.bz2\n",
      "55885824/55884326 [==============================] - 60s 1us/step\n",
      "Downloading\n",
      "Downloading data from https://dumps.wikimedia.org/enwiki/20201001/enwiki-20201001-pages-articles23.xml-p44788942p46288941.bz2\n",
      "228384768/228383839 [==============================] - 217s 1us/step\n",
      "Downloading\n",
      "Downloading data from https://dumps.wikimedia.org/enwiki/20201001/enwiki-20201001-pages-articles23.xml-p46288942p47788941.bz2\n",
      "362102784/362097862 [==============================] - 626s 2us/step\n",
      "Downloading\n",
      "Downloading data from https://dumps.wikimedia.org/enwiki/20201001/enwiki-20201001-pages-articles23.xml-p47788942p49288941.bz2\n",
      "304455680/304454625 [==============================] - 328s 1us/step\n",
      "Downloading\n",
      "Downloading data from https://dumps.wikimedia.org/enwiki/20201001/enwiki-20201001-pages-articles23.xml-p49288942p50564553.bz2\n",
      "235069440/235065016 [==============================] - 175s 1us/step\n",
      "Downloading\n",
      "Downloading data from https://dumps.wikimedia.org/enwiki/20201001/enwiki-20201001-pages-articles24.xml-p50564554p52064553.bz2\n",
      "322166784/322159926 [==============================] - 269s 1us/step\n",
      "Downloading\n",
      "Downloading data from https://dumps.wikimedia.org/enwiki/20201001/enwiki-20201001-pages-articles24.xml-p52064554p53564553.bz2\n",
      "322420736/322418062 [==============================] - 255s 1us/step\n",
      "Downloading\n",
      "Downloading data from https://dumps.wikimedia.org/enwiki/20201001/enwiki-20201001-pages-articles24.xml-p53564554p55064553.bz2\n",
      "306626560/306624664 [==============================] - 192s 1us/step\n",
      "Downloading\n",
      "Downloading data from https://dumps.wikimedia.org/enwiki/20201001/enwiki-20201001-pages-articles24.xml-p55064554p56564553.bz2\n",
      "321986560/321985394 [==============================] - 189s 1us/step\n",
      "Downloading\n",
      "Downloading data from https://dumps.wikimedia.org/enwiki/20201001/enwiki-20201001-pages-articles24.xml-p56564554p57025655.bz2\n",
      "101703680/101699893 [==============================] - 63s 1us/step\n",
      "Downloading\n",
      "Downloading data from https://dumps.wikimedia.org/enwiki/20201001/enwiki-20201001-pages-articles25.xml-p57025656p58525655.bz2\n",
      "335503360/335498693 [==============================] - 193s 1us/step\n",
      "Downloading\n",
      "Downloading data from https://dumps.wikimedia.org/enwiki/20201001/enwiki-20201001-pages-articles25.xml-p58525656p60025655.bz2\n",
      "296157184/296156960 [==============================] - 233s 1us/step\n",
      "Downloading\n",
      "Downloading data from https://dumps.wikimedia.org/enwiki/20201001/enwiki-20201001-pages-articles25.xml-p60025656p61525655.bz2\n",
      "329154560/329146942 [==============================] - 217s 1us/step\n",
      "Downloading\n",
      "Downloading data from https://dumps.wikimedia.org/enwiki/20201001/enwiki-20201001-pages-articles25.xml-p61525656p62585850.bz2\n",
      "231440384/231434326 [==============================] - 145s 1us/step\n",
      "Downloading\n",
      "Downloading data from https://dumps.wikimedia.org/enwiki/20201001/enwiki-20201001-pages-articles26.xml-p62585851p63975909.bz2\n",
      "344973312/344965297 [==============================] - 321s 1us/step\n",
      "Downloading\n",
      "Downloading data from https://dumps.wikimedia.org/enwiki/20201001/enwiki-20201001-pages-articles27.xml-p63975910p65475424.bz2\n",
      "310239232/310232346 [==============================] - 275s 1us/step\n"
     ]
    }
   ],
   "source": [
    "data_paths = []\n",
    "file_info = []\n",
    "\n",
    "for file in files_to_download:\n",
    "    path = directory + file\n",
    "    \n",
    "    if not os.path.exists(directory):\n",
    "        print('neexistuje')\n",
    "    # downaload only when file dont exist\n",
    "    if not os.path.exists(directory + file):\n",
    "        print('Downloading')\n",
    "        data_paths.append(get_file(file, base_url + file))\n",
    "        file_size = os.stat(path).st_size / 1e6\n",
    "        \n",
    "        # Find the number of articles\n",
    "        file_articles = int(file.split('p')[-1].split('.')[-2]) - int(file.split('p')[-2])\n",
    "        file_info.append((file, file_size, file_articles))\n",
    "        \n",
    "    # when file already exist\n",
    "    else:\n",
    "        data_paths.append(path)\n",
    "        file_size = os.stat(path).st_size / 1e6\n",
    "        \n",
    "        file_number = int(file.split('p')[-1].split('.')[-2]) - int(file.split('p')[-2])\n",
    "        file_info.append((file.split('-')[-1], file_size, file_number))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('enwiki-20201001-pages-articles9.xml-p2936261p4045402.bz2',\n",
       "  512.145263,\n",
       "  1109141),\n",
       " ('enwiki-20201001-pages-articles10.xml-p4045403p5399366.bz2',\n",
       "  502.44982,\n",
       "  1353963),\n",
       " ('enwiki-20201001-pages-articles11.xml-p5399367p6899366.bz2',\n",
       "  486.770345,\n",
       "  1499999),\n",
       " ('enwiki-20201001-pages-articles8.xml-p2134112p2936260.bz2',\n",
       "  471.652241,\n",
       "  802148),\n",
       " ('enwiki-20201001-pages-articles7.xml-p1483662p2134111.bz2',\n",
       "  462.504094,\n",
       "  650449)]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(file_info, key = lambda x: x[1], reverse = True)[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Part Parsing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parsovanie prebieha postupne na všetkých súboroch v kompresovanom tvare. Na tento účel je použitý podproces \"bzcat\", ktorý číta a dodáva súbor po jednotlivých riadkoch. Na spracovanie týchto dát je použitý XML SAX parser. Tento parser obsahuje metódu ContentHandler, ktorá zabezpečuje uchovanie riadkov v buffery, pričom sa hľadajú tagy (page, title, text). Po nájdeni ukončovacieho znaku tagu page prebieha spracovanie celého článku.\n",
    "\n",
    "Z článku sú pomocou regulárnych výrazov extrahované informácie:\n",
    "* **infobox**\n",
    "    * atribúty infoboxu\n",
    "    * typ infoboxu\n",
    "* **kategórie čklánku**\n",
    "\n",
    "Následne na základe týchto informácií je určená kategória článku."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import xml.sax\n",
    "import regex\n",
    "import pandas as pd\n",
    "from functools import reduce\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import csv\n",
    "import json\n",
    "import os\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "import nltk\n",
    "import gc\n",
    "from nltk.util import ngrams\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Momentálne sú priradzované kategórie: Person, Company, Organisation, Place.\n",
    "Priradzovanie prebieha podľa vyššieho poradia na základe parametrov v poradí:\n",
    "* **typ infoboxu** - či sa v zozname danej kategorie nachádza infobox daného článku\n",
    "* **atribúty infoboxu:**\n",
    "    * **person** - birth_date\n",
    "    * **company** - industry, trade_name, products, brands\n",
    "    * **organisation** - zatiaľ žiadne\n",
    "    * **place** - coordinates, locations _|neobsahuje|_ date, founded, founder, founders\n",
    "* **kategorie článku:**\n",
    "    * **organisation** - obsahuje v kategóriach slovo organisaion/s\n",
    "* **text článku** - zatiaľ nepoužité, ale plánované pre prípady, kedy článok neobsahuje infobox a kategórie neposkytnú žiadnu informáciu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get Infobox and Infobox type from article text\n",
    "def ArticleHandler(infobox_types=None, evaluation=None):\n",
    "    #source:(stackof) https://regex101.com/r/kT1jF4/1\n",
    "    infobox_regex = '(?=\\{Infobox )(\\{([^{}]|(?1))*\\})'\n",
    "    inf_type_regex = '(?<=Infobox)(.*?)(?=(\\||\\n|<!-|<--))'\n",
    "    #https://regex101.com/r/1vJlms/1\n",
    "    inf_parameters = '(?(?<=\\|)|(?<=\\|\\s))(\\w*)\\s*=\\s*[\\w{\\[]'\n",
    "    #https://regex101.com/r/fl5hAw/1 https://regex101.com/r/Xj0fM3/1\n",
    "    redirect_title = '(?<=\\[\\[)(.*)(?=\\]\\])'\n",
    "    categories = '(?<=\\[\\[Category:)([^\\]]*)(?=\\]\\])'\n",
    "    testing = evaluation\n",
    "    \n",
    "    Person=['player', 'male', 'actor', 'sportspeopl', 'medalist', 'actress', 'expatri', 'singer', 'musician', 'live', 'writer', 'politician', 'f.c', 'alumni', 'personnel', 'olymp', '20th-centuri', 'faculti', 'coach', 'guitarist']\n",
    "    Company=['brand', 'merger', 'retail', 'exchang', 'stock', 'label', 'nasdaq', 'multin', 'subsidiari', 'acquisit', 'onlin', 'offer', 'held', 'conglomer', 'store', 'bankruptci']\n",
    "    Organisation=['scout', 'think', 'non-profit', 'gang', 'event', 'recur', 'religi', 'child-rel', 'non-align', 'non-government', 'critic', 'evangel', 'yakuza', 'advocaci']\n",
    "    Location=['regist', 'unincorpor', 'station', 'popul', 'complet', 'aerodrom', 'villag', 'town', 'landform', 'parish', 'river', 'seaplan', 'open', 'census-design', 'mountain', 'attract', 'neighbourhood', 'suburb', 'rang', 'airport']\n",
    "    \n",
    "    PersonBi=['living peopl', '20th century', 'f c', 'c play', 'century american', '21st century', 'american male', 'association football', 'league play', 'expatriate footballers', 'expatriate sportspeople', 'cup play', 'international footbal', 'rugby league', 'university alumni', 'fc play', 'musical groups', 'ice hockey', 'world cup', 'american people', 'fifa world', 'male actors', 'football league', 'male actor', 'expatriate footbal', 'military personnel', 'people educated', 'hockey players', 'male writ', 'records artist', 'draft pick', 'century indian', 'football manag', 'male television', 'film actor', 'uk mps', 'male film', 'soccer play', 'television actor', 'united f', 'year birth', 'living people', 'winter olymp', 'birth missing', 'football midfield', 'missing living', 'major league', 'school alumni', '19th century', 'new zealand']\n",
    "    CompanyBi=['companies established', 'companies based', 'companies united', 'services companies', 'financial services', 'mergers acquisit', 'american companies', 'stock exchanges', 'video game', 'game companies', 'manufacturing companies', 'chains united', 'companies listed', 'exchanges africa', 'restaurants established', 'stock exchang', 'companies disestablished', 'mass media', 'media companies', 'restaurant chains', 'retail companies', 'companies filed', 'defunct companies', 'filed chapter', 'internet properties', 'properties established', 'retailers united', '11 bankruptcy', 'chapter 11', 'companies canada', 'companies formerly', 'established 1960', 'established 1995', 'fast food', 'formerly listed', 'listed new', 'manufacturers united', 'york stock', 'established 1989', 'establishments california', 'based austin', 'british companies', 'clothing companies', 'companies england', 'companies isle', 'development compani', 'established 1950', 'established 1974', 'established 2003', 'food chains']\n",
    "    OrganisationBi=['based united', 'non profit', 'learned societies', 'profit organizations', 'organizations established', 'organisations based', 'organizations based', '3 organ', '501 c', 'associations based', 'c 3', 'charities based', 'consultative status', 'established 1946', 'professional associations', 'psychology organizations', 'relief organ', 'societies canada', 'status united', 'english football', 'establishments united', 'youth organizations', 'united nations', '1845 establishments', '1859 establishments', '1864 establishments', '1907 establishments', '1908 establishments', '1959 establishments', '1982 establishments', '1996 establishments', '19th centuri', 'academy financial', 'advocacy organ', 'aid organ', 'air ambulance', 'ambulance servic', 'ambulance services', 'american council', 'american organized', 'ancient near', 'awards h', 'banks texa', 'bar associ', 'based geneva', 'based hong', 'based montr', 'based surrey', 'based switzerland', 'based tyne']\n",
    "    LocationBi=['pyrénées atlantiqu', 'communes pyrénées', 'articles needing', 'atlantiques communes', 'communes articles', 'french wikipedia', 'needing translation', 'pyrénées atlantiques', 'translation french', 'lower navarr', 'populated places', 'register historic', 'national register', 'historic places', 'towns luxembourg', 'unincorporated communities', 'civil parishes', 'buildings structures', 'neighborhoods pittsburgh', 'sports venues', 'western australia', 'parishes leicestershir', 'villages leicestershir', 'cities towns', 'protected areas', 'borough charnwood', 'places established', 'suburbs perth', 'west virginia', 'tourist attractions', 'buildings completed', 'city rockingham', 'suburbs city', 'shopping malls', 'new jersey', 'county west', 'historic house', 'house museums', 'venues completed', 'county california', 'county virginia', 'alzette canton', 'communes esch', 'county massachusett', 'esch sur', 'former communes', 'mountains hills', 'rhode island', 'road bridges', 'sur alzette']\n",
    "    \n",
    "    \n",
    "    # infobox_types = getInfoboxTypesList()\n",
    "    \n",
    "    def getCategories(text):\n",
    "        return regex.findall(categories, text)\n",
    "    \n",
    "    \n",
    "    def getArticleAtributes(infobox,text):\n",
    "        i_par = regex.findall(inf_parameters, infobox)\n",
    "        i_type = regex.search(inf_type_regex, infobox)\n",
    "        i_type = i_type.group(0).strip() if i_type is not None else \"none\"\n",
    "        return {'type': i_type.lower(), 'parameters': i_par, 'categories': list(getCategories(text))}\n",
    "    \n",
    "    \n",
    "    def remove_stop_words(data):\n",
    "        stopwords = nltk.corpus.stopwords.words('english')\n",
    "        return [w for w in data if w not in stopwords]\n",
    "\n",
    "\n",
    "    def tokenize(data):\n",
    "        symbols = symbols = \"!\\\"#$%&()*+'-./:;,|<=>?@[\\]^_`{}~\\n\"\n",
    "        tokens = word_tokenize(data)\n",
    "        tokens = [token.lower() for token in tokens if token not in list(symbols)]\n",
    "        return tokens\n",
    "\n",
    "\n",
    "    def stemming(data):\n",
    "        stemmer= PorterStemmer()\n",
    "        return [stemmer.stem(token) for token in data]\n",
    "        \n",
    "    def processCategories(data):\n",
    "        data = tokenize(data)\n",
    "        data = remove_stop_words(data)\n",
    "        data = stemming(data)\n",
    "        return data\n",
    "    \n",
    "    \n",
    "    def get_bigrams(text):\n",
    "        bigrams = []\n",
    "        for sen in text:\n",
    "            token = nltk.word_tokenize(sen)\n",
    "            bigrams.append(list(map(lambda x: ' '.join(x),list(ngrams(token,2)))))\n",
    "        return bigrams\n",
    "\n",
    "\n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "    def process_whole_sentence(text):\n",
    "        sen = ' '.join(w for w in text.split() if w not in stopwords)\n",
    "        sen = re.sub(r'\\W', ' ', str(sen))\n",
    "        sen = re.sub(r'\\s+', ' ', sen, flags=re.I)\n",
    "        sen = sen.lower()\n",
    "        return sen\n",
    "        \n",
    "        \n",
    "    def isRedirect(text):\n",
    "        return regex.search(\"^#redirect\\s*\\[\\[(?i)\", text)\n",
    "        \n",
    "        \n",
    "    def getInfobox(text):\n",
    "        infobox = regex.search(infobox_regex, text)\n",
    "        return infobox.group() if infobox is not None else \"redirect\" if isRedirect(text) is not None else \"no infobox/redirect\"\n",
    "    \n",
    "    \n",
    "    def categoryBy_infoboxType(info):\n",
    "        if info['type'] in infobox_types['person']:\n",
    "            return 'Person'\n",
    "        elif info['type'] in infobox_types['company']:\n",
    "            return 'Company'\n",
    "        elif info['type'] in infobox_types['org']:\n",
    "            return 'Organization'\n",
    "        elif info['type'] in infobox_types['location']:\n",
    "            return 'Location'\n",
    "        else:\n",
    "            return None\n",
    "  \n",
    "\n",
    "    def anotherCategoryBy_infoboxType(info):\n",
    "        if info['type'] in infobox_types['other']:\n",
    "            return 'Another'\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "        \n",
    "    def categoryBy_atributes(info):\n",
    "        if 'birth_date' in info['parameters']:\n",
    "            return \"A_Person\"\n",
    "        elif any(i in info['parameters'] for i in ['industry', 'trade_name', 'products', 'brands']):\n",
    "            return 'A_Company'\n",
    "        elif any(i in info['parameters'] for i in ['coordinates', 'locations']) and not(any(i in info['parameters'] for i in ['date', 'founded', 'founder', 'founders'])):\n",
    "            return 'A_Location'\n",
    "        else:\n",
    "            return None\n",
    "        \n",
    "        \n",
    "    def categoryBy_categories(info):\n",
    "        stemmed_categories = reduce(lambda x,y: x+y,map(lambda x: processCategories(x), info['categories']),[])\n",
    "        bigramCategories = sum(get_bigrams(stemming(map(lambda x: process_whole_sentence(x), info['categories']))),[])\n",
    "        \n",
    "#         if any(i in PersonBi for i in bigramCategories) or any(i in Person for i in stemmed_categories):\n",
    "#             return 'C_Person'\n",
    "#         elif any(i in CompanyBi for i in bigramCategories) or any(i in Company for i in stemmed_categories):\n",
    "#             return 'C_Company'\n",
    "#         elif any(i in OrganisationBi for i in bigramCategories) or any(i in Organisation for i in stemmed_categories):\n",
    "#             return 'C_Organization'\n",
    "#         elif any(i in LocationBi for i in bigramCategories) or any(i in Location for i in stemmed_categories):\n",
    "#             return 'C_Location'\n",
    "        \n",
    "#         if any(i in Person for i in stemmed_categories):\n",
    "#             return 'C_Person'\n",
    "#         elif any(i in Company for i in stemmed_categories):\n",
    "#             return 'C_Company'\n",
    "#         elif any(i in Organisation for i in stemmed_categories):\n",
    "#             return 'C_Organization'\n",
    "#         elif any(i in Location for i in stemmed_categories):\n",
    "#             return 'C_Location'\n",
    "\n",
    "        if any(i in PersonBi for i in bigramCategories):\n",
    "            return 'C_Person'\n",
    "        elif any(i in CompanyBi for i in bigramCategories):\n",
    "            return 'C_Company'\n",
    "        elif any(i in OrganisationBi for i in bigramCategories):\n",
    "            return 'C_Organization'\n",
    "        elif any(i in LocationBi for i in bigramCategories):\n",
    "            return 'C_Location'\n",
    "        \n",
    "        elif list(filter(lambda x: regex.search('^\\d*\\sbirths*(?i)', x), info['categories'])):\n",
    "            return 'C_Person'\n",
    "        elif list(filter(lambda x: regex.search('\\b(compan(y|ies))\\b(?i)', x), info['categories'])):\n",
    "            return 'C_Company'\n",
    "        elif list(filter(lambda x: regex.search('(organisations*)(?i)', x), info['categories'])):\n",
    "            return 'C_Organization'\n",
    "        else:\n",
    "            return None\n",
    "    \n",
    "    \n",
    "    def first_true(iterable,data=None, default='Other'):\n",
    "        return next((item(data) for item in iterable if item(data) is not None), default)\n",
    "    \n",
    "    \n",
    "    def predictCategory(infobox, info):\n",
    "        if infobox not in ['redirect', 'no infobox/redirect']:\n",
    "            if not(testing):\n",
    "                return first_true([categoryBy_infoboxType,categoryBy_atributes, anotherCategoryBy_infoboxType, categoryBy_categories], info)\n",
    "            else:\n",
    "                if categoryBy_infoboxType(info) is not None or anotherCategoryBy_infoboxType(info) is not None: \n",
    "                    return first_true([categoryBy_atributes,categoryBy_categories], info)\n",
    "                else:\n",
    "                    return None\n",
    "            #tieto clanky maju len kategorie\n",
    "        elif infobox == 'no infobox/redirect':\n",
    "            return first_true([categoryBy_categories], info,\"Other/None\")\n",
    "        else:\n",
    "            return 'redirect::'+info\n",
    "\n",
    "    \n",
    "    def processArticle(title, text):\n",
    "        infobox = getInfobox(text)\n",
    "        \n",
    "        if infobox == \"redirect\":\n",
    "            info = regex.search(redirect_title, text).group(0)\n",
    "        \n",
    "        elif infobox == 'no infobox/redirect':\n",
    "            info = {'categories': list(getCategories(text))}\n",
    "            if info['categories'] == []:\n",
    "                return None\n",
    "        else:\n",
    "            info = getArticleAtributes(infobox, text)\n",
    "\n",
    "        return (title, infobox, info, predictCategory(infobox, info))\n",
    "    return processArticle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ano\n"
     ]
    }
   ],
   "source": [
    "sss = {'categories':[]}\n",
    "if sss['categories'] == []:\n",
    "    print('ano')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#docs: https://docs.python.org/3.8/library/xml.sax.handler.html\n",
    "class ContentHandler(xml.sax.handler.ContentHandler):\n",
    "    def __init__(self, testing=None):\n",
    "        xml.sax.handler.ContentHandler.__init__(self)\n",
    "        self._buf = None\n",
    "        self._last_tag = None\n",
    "        self._parts = {}\n",
    "        self.output = []\n",
    "        self.evaluation = testing\n",
    "        self.article_process = ArticleHandler(infobox_types=getInfoboxTypesList(),evaluation=self.evaluation)\n",
    "\n",
    "    def characters(self, content):\n",
    "        if self._last_tag:\n",
    "            self._buf.append(content)\n",
    "\n",
    "    def startElement(self, name, attrs):\n",
    "        if name == 'page':\n",
    "            self._parts = {}\n",
    "        if name in ('title', 'text'):\n",
    "            self._last_tag = name\n",
    "            self._buf = []\n",
    "\n",
    "    def endElement(self, name):\n",
    "        if name == self._last_tag:\n",
    "            self._parts[name] = ''.join(self._buf)\n",
    "        \n",
    "        #whole article\n",
    "        if name == 'page':\n",
    "            data = self.article_process(**self._parts)\n",
    "            if data is not None:\n",
    "                self.output.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def parseWiki(data=None, limit = 200, save = True, test_sample=False, evaluation=False):\n",
    "    \n",
    "    if test_sample:\n",
    "        data = os.getcwd().rsplit('/', 1)[0]\n",
    "        data = f'{data}/data/sample_wiki_articles2.xml.bz2'\n",
    "        print(data)\n",
    "    elif data is None:\n",
    "        data = '/home/xminarikd/.keras/datasets/enwiki-20201001-pages-articles9.xml-p2936261p4045402.bz2'\n",
    "    \n",
    "    handler = ContentHandler(testing=evaluation)\n",
    "\n",
    "    parser = xml.sax.make_parser()\n",
    "    parser.setContentHandler(handler)\n",
    "\n",
    "    for i, line in enumerate(subprocess.Popen(['bzcat'], \n",
    "                             stdin = open(data), \n",
    "                             stdout = subprocess.PIPE).stdout):\n",
    "\n",
    "#         if (i + 1) % 10000 == 0:\n",
    "#             print(f'Spracovanych {i + 1} riadkov.', end = '\\r')\n",
    "#             print('')\n",
    "        try:\n",
    "            parser.feed(line)\n",
    "        except StopIteration:\n",
    "            break\n",
    "        \n",
    "        # get only some results\n",
    "        if len(handler.output) >= limit:\n",
    "            break\n",
    "        \n",
    "    if save:\n",
    "        output_dir = os.getcwd().rsplit('/', 1)[0]\n",
    "        partition_name = data.split('/')[-1].split('-')[-1].split('.')[0]\n",
    "        if not(evaluation):\n",
    "            output_file = f'{output_dir}/output/{partition_name}.tsv'\n",
    "        else:\n",
    "            output_file = f'{output_dir}/output/eval/{partition_name}.tsv'\n",
    "\n",
    "        \n",
    "        f1 = open(output_file, 'w+', newline='\\n')\n",
    "        f2 = open(f'{output_file}-redirects', 'w+', newline='\\n')\n",
    "        \n",
    "        writer1 = csv.writer(f1, delimiter='\\t')\n",
    "        writer2 = csv.writer(f2, delimiter='\\t')\n",
    "        writer1.writerow([\"Title\",\"Category\"])\n",
    "        writer2.writerow([\"Title\",\"Source\"])\n",
    "        \n",
    "        for x in handler.output:\n",
    "            if x[1] == 'redirect':\n",
    "                writer2.writerow([x[0],x[2] or 'None'])\n",
    "            else:\n",
    "                writer1.writerow([x[0],x[3] or 'None'])\n",
    "        \n",
    "            \n",
    "#         with open(output_file, 'w+', newline='\\n') as file:\n",
    "#             writer = csv.writer(file, delimiter='\\t')\n",
    "#             writer.writerow([\"Title\", \"Category\"])\n",
    "#             for x in handler.output:\n",
    "#                 writer.writerow([x[0],x[3] or 'None'])\n",
    "\n",
    "#         with open(output_file, 'w+', newline='\\n') as file:\n",
    "#             for x in handler.output:\n",
    "#                 file.write(json.dumps({\"title\": x[0], \"category\":x[3]}))\n",
    "        \n",
    "        print(f'{output_file} done', end='\\r')\n",
    "        del handler\n",
    "        del parser\n",
    "        gc.collect()\n",
    "        return None\n",
    "    else:\n",
    "        return handler.output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stiahnutie a parsovanie stránky wikipédie, ktorá obsahuje zoznam typov infoboxov. Tento zoznam obsahuje aj členeie týchto typov do rôznych kategórií. Vďaka tomuto je možné jednoducho získať všetky infoboxy, ktoré sú spojené napríklad s osobami."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getInfoboxTypesList():\n",
    "    infobox_list_url = 'https://en.wikipedia.org/wiki/Wikipedia:List_of_infoboxes'\n",
    "    infobox_list_html = requests.get(infobox_list_url).text\n",
    "    soup_dump = BeautifulSoup(infobox_list_html, 'html.parser')\n",
    "    #sib = soup_dump.find_all(\"div\" ,{'id': 'toc'}).next_sibling\n",
    "    other = {}\n",
    "\n",
    "    template_list = dict();\n",
    "    prev = None\n",
    "    prev_tag = None\n",
    "    prev_parent = None\n",
    "    prev_parent_tag = 2\n",
    "\n",
    "    for i, sibling in enumerate(soup_dump.find(id=\"toc\").next_siblings):\n",
    "\n",
    "        if prev_parent == 'Other':\n",
    "            break\n",
    "\n",
    "        if sibling.name == 'h2':\n",
    "            template_list[sibling.findChild().text] = {}\n",
    "            prev_parent = sibling.findChild().text\n",
    "            prev_tag = 2\n",
    "\n",
    "        if sibling.name == 'h3':\n",
    "            if prev_tag < 3:\n",
    "                template_list[prev_parent][sibling.findChild().text] = list()\n",
    "                prev_tag = 3\n",
    "                prev = sibling.findChild().text\n",
    "\n",
    "            if prev_tag == 3:\n",
    "                template_list[prev_parent][sibling.findChild().text] = list()\n",
    "                prev = sibling.findChild().text\n",
    "\n",
    "        if sibling.name == 'ul':\n",
    "            a = sibling.find_all('a', title=re.compile('^Template:Infobox'))\n",
    "            b = map(lambda x: regex.findall('(?<=Template:Infobox )(.*)(?i)', x.text.lower()), a)\n",
    "            c = reduce(lambda x,y: x+y, b, list())\n",
    "\n",
    "            if prev_tag >=3:\n",
    "                template_list[prev_parent][prev] = [y for x in [template_list[prev_parent][prev], list(c)] for y in x] \n",
    "            else:\n",
    "                template_list[prev_parent] = list(c)\n",
    "\n",
    "    \n",
    "    for k ,v in template_list.items():\n",
    "        if(k not in ['Person', \"Place\", 'Society and social science', \"Other\"]):\n",
    "            other.update({k:v})\n",
    "        elif k == 'Society and social science':\n",
    "            tmp = {}\n",
    "            for k2,v2 in v.items():\n",
    "                if k2 not in ['Business and economics', \"Organization\"]:\n",
    "                    tmp.update({k2:v2})\n",
    "            other.update({k:tmp})\n",
    "            \n",
    "    other = sum(sum((map(lambda x: list(x.values()) if isinstance(x, dict) else [x] ,list(other.values()))),[]),[])\n",
    "    persons = list(reduce(lambda x,y: x+y, template_list[\"Person\"].values()))\n",
    "    locations = list(reduce(lambda x,y: x+y, template_list[\"Place\"].values()))\n",
    "    companies = template_list['Society and social science']['Business and economics']\n",
    "    organizations = template_list['Society and social science']['Organization']\n",
    "    \n",
    "    return {'person': persons, 'location': locations, 'company': companies, 'org': organizations, 'other': other}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPathFiles(path, endwith):\n",
    "    out_path = os.getcwd().rsplit('/', 1)[0]\n",
    "    files = f'{out_path}{path}/'\n",
    "    files = [files+file for file in os.listdir(files) if file.endswith(endwith)]\n",
    "    return files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapperCategories(arg):\n",
    "    switcher = {\n",
    "        'Person': 0,\n",
    "        'A_Person':0,\n",
    "        'C_Person':0,\n",
    "        'Company': 1,\n",
    "        'A_Company':1,\n",
    "        'C_Company':1,\n",
    "        'Organization':2,\n",
    "        'A_Organization':2,\n",
    "        'C_Organization':2,\n",
    "        'Location':3,\n",
    "        'A_Location':3,\n",
    "        'C_Location':3,\n",
    "        'Another':4\n",
    "    }\n",
    "    return switcher.get(arg,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readTsv(file):\n",
    "    output = []\n",
    "    with open(file) as f:\n",
    "        for line in csv.DictReader(f, delimiter='\\t'): \n",
    "            output.append(line)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spustenie funkcie na spracovanie súborov."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "David Stagg <--> Person\n",
      "Amaranthus mantegazzianus <--> redirect::Amaranthus caudatus\n",
      "Amaranthus quitensis <--> redirect::Amaranthus hybridus\n",
      "Maud Queen of Norway <--> redirect::Maud of Wales\n",
      "Milligram per litre <--> redirect::Gram per litre\n",
      "Utica Psychiatric Center <--> Location\n",
      "Olean Wholesale Grocery <--> C_Company\n",
      "Queen Tiye <--> redirect::Tiye\n",
      "Queen Hatshepsut <--> redirect::Hatshepsut\n",
      "Clibanarii <--> Other/None\n",
      "Political documentary <--> redirect::Documentary film\n",
      "Final fantasy legends <--> redirect::Final Fantasy Dimensions\n",
      "Queen Marie Amelie Therese <--> redirect::Maria Amalia of Naples and Sicily\n",
      "Political documentaries <--> redirect::Documentary film\n",
      "E-767 <--> redirect::Boeing E-767\n",
      "Prince Edward-Lennox <--> redirect::Prince Edward—Lennox\n",
      "Arthur Hill (actor) <--> Person\n",
      "Periodic paralysis <--> Other\n",
      "Greenstripe <--> redirect::Amaranthus acanthochiton\n",
      "Amaranthus cruentus <--> C_Location\n",
      "Careless weed <--> redirect::Amaranthus palmeri\n",
      "Zamil idris <--> redirect::Ahmad Zamil\n",
      "Khada sag <--> redirect::Amaranthus dubius\n",
      "Million instructions per second <--> redirect::Instructions per second#Millions of instructions per second (MIPS)\n",
      "Ashtadiggajas <--> Other/None\n",
      "John C.Harsanyi <--> redirect::John Harsanyi\n",
      "Société entomologique de France <--> C_Organization\n",
      "Sangorache <--> redirect::Amaranthus hybridus\n",
      "Joseph's coat <--> redirect::Coat of many colors\n",
      "Recipients of the Distinguished Service Award of the Order of the Arrow <--> Other/None\n",
      "Josephscoat <--> redirect::Amaranthus tricolor\n",
      "Joseph's-coat <--> redirect::Amaranthus tricolor\n",
      "URM Stores <--> C_Company\n",
      "General purpose technology <--> Other/None\n",
      "Rough skinned newt <--> redirect::Rough-skinned newt\n",
      "Queen Sophie Dorothea of Hanover <--> redirect::Sophia Dorothea of Hanover\n",
      "Cefepime <--> Other/None\n",
      "Larry Zerner <--> Person\n",
      "World of Illushions Starring Mickey Mouse and Donald Duck <--> redirect::World of Illusion\n",
      "Category:Aberdeen, Hong Kong <--> Other/None\n",
      "United Retail Merchants <--> redirect::URM Stores\n",
      "Plassenburg <--> C_Location\n",
      "U.R.M. <--> redirect::URM Stores\n",
      "Turriaco <--> Location\n",
      "Piggly Wiggly Alabama Distributing Company <--> redirect::Piggly Wiggly\n",
      "Queen Eleanor of Aquitaine <--> redirect::Eleanor of Aquitaine\n",
      "Crenshaw Company <--> C_Company\n",
      "Osmia ribifloris <--> Other/None\n",
      "San Pier d'Isonzo <--> Location\n",
      "Early life of Hugo Chávez <--> Person\n",
      "1974–75 NHL season <--> Another\n",
      "Demorrio Williams <--> A_Person\n",
      "Contact explosive <--> Other/None\n",
      "Bartley Secondary School <--> A_Location\n",
      "Tom Ayrton <--> Another\n",
      "Система <--> redirect::Systema (disambiguation)\n",
      "Bedloe <--> redirect::Liberty Island\n",
      "Cape Camarón <--> Other/None\n",
      "Reece Williams <--> Person\n",
      "Amariah Brigham <--> C_Person\n",
      "Microsoft MultiPlan <--> redirect::Multiplan\n",
      "Asadullah Rahman <--> redirect::List of Afghan detainees at Guantanamo Bay\n",
      "IMac DV <--> redirect::IMac G3\n",
      "Gute Bücher für Alle <--> C_Organization\n",
      "Howard Koh <--> Person\n",
      "Cefotaxime <--> Other/None\n",
      "The Ghost of the Grotto <--> Another\n",
      "Cwm-yr-Eglwys <--> Location\n",
      "Land Beneath the Ground! <--> Another\n",
      "The Immortals (band) <--> Other/None\n",
      "Category:Spanish handball players <--> Other/None\n",
      "Michael Sullivan (rugby league) <--> Person\n",
      "Polyisocyanurate <--> Other/None\n",
      "Blueberry Bee <--> redirect::Blueberry bee\n",
      "Littlejohn adaptor <--> Other/None\n",
      "Step Up (pricing game) <--> redirect::List of The Price Is Right pricing games\n",
      "Canon T-80 <--> redirect::Canon T80\n",
      "Grossauheim <--> redirect::Großauheim\n",
      "Yasuo Hamanaka <--> C_Person\n",
      "Rubel Castle <--> Location\n",
      "Columbus before the Council of Salamanca <--> redirect::Christopher Columbus Before the Council of Salamanca\n",
      "Gag names <--> redirect::Gag name\n",
      "David Simmons (rugby league) <--> Person\n",
      "Save the Best for Last <--> Another\n",
      "Ken Sagoes <--> Person\n",
      "Buedingen, Hesse <--> redirect::Büdingen\n",
      "Excise Tax <--> redirect::Excise\n",
      "5th Marine Regiment <--> Another\n",
      "Eve Johnstone <--> Person\n",
      "Emergencies in India <--> redirect::President of India#Emergency powers\n",
      "Ray of Creation <--> Other/None\n",
      "Swap Meet (pricing game) <--> redirect::List of The Price Is Right pricing games\n",
      "State Emergency in India <--> redirect::President of India#Emergency powers\n",
      "List of presidents of Mississippi State University <--> Other/None\n",
      "National Emergency in India <--> redirect::President of India#Emergency powers\n",
      "Financial Emergency in India <--> redirect::President of India#Financial emergency\n",
      "Fish disks <--> redirect::Fred Fish\n",
      "Gross-Gerau <--> redirect::Groß-Gerau\n",
      "Gag name <--> Other/None\n",
      "Chaharbagh, Isfahan <--> C_Location\n",
      "Marcomms <--> redirect::Marketing communications\n",
      "Montgomery Convention <--> redirect::Provisional Congress of the Confederate States\n",
      "Gross-Umstadt <--> redirect::Groß-Umstadt\n",
      "The colophon <--> redirect::The Colophon, A Book Collectors' Quarterly\n",
      "Purely High School for Boys <--> redirect::Purley High School for Boys\n",
      "Danny Nutley <--> Person\n",
      "Amvescap <--> redirect::Invesco\n",
      "Etxebarri, Anteiglesia de San Esteban - Etxebarri Doneztebeko Elizatea <--> redirect::Etxebarri\n",
      "Johnny mosley <--> redirect::Jonny Moseley\n",
      "Chahar bagh <--> redirect::Charbagh\n",
      "Ruesselsheim <--> redirect::Rüsselsheim am Main\n",
      "Pont Alexandre III <--> Location\n",
      "Replacement value <--> Other/None\n",
      "ABAP programming language <--> redirect::ABAP\n",
      "Galbraith <--> Other/None\n",
      "Royal Bank Building (Toronto) <--> C_Location\n",
      "Norbert Lammert <--> Person\n",
      "MM-UGM <--> redirect::Gadjah Mada University\n",
      "Switch? (The Price Is Right) <--> redirect::List of The Price Is Right pricing games\n",
      "Manipuri (Meiteilon Language) <--> redirect::Meitei language\n",
      "Fountain of life <--> redirect::Fountain of Life\n",
      "British NVC community SD1 <--> Other/None\n",
      "Meetei <--> redirect::Meitei people\n",
      "MM UGM <--> redirect::Gadjah Mada University\n",
      "MMUGM <--> redirect::Gadjah Mada University\n",
      "Category:Politics of Cameroon <--> Other/None\n",
      "Piccolo Coro <--> redirect::Piccolo Coro dell'Antoniano\n",
      "Greatest Native Americans <--> redirect::List of indigenous people of the Americas\n",
      "Hapoel Tel Aviv B.C. <--> Another\n",
      "Psychology/rewrite <--> redirect::Psychology\n",
      "The Wave (Arizona) <--> Other/None\n",
      "Category:Cameroon-related lists <--> Other/None\n",
      "Ault Hucknall <--> Location\n",
      "Frontier Middle School shooting <--> Another\n",
      "Turf house <--> redirect::Earth shelter\n",
      "Claris Impact <--> redirect::Claris#Pro_series\n",
      "Quannah Parker <--> redirect::Quanah Parker\n",
      "Switcheroo (pricing game) <--> redirect::List of The Price Is Right pricing games\n",
      "Moon dog <--> Other/None\n",
      "Rodney Eastman <--> Person\n",
      "Correggio, Emilia-Romagna <--> Location\n",
      "Peters Export and Import <--> redirect::Crenshaw Company\n",
      "Wah Ching <--> Organization\n",
      "Oasis of the Zombies <--> Another\n",
      "Sihl <--> Location\n",
      "Smith & Mighty <--> Person\n",
      "Westinghouse Astronuclear Laboratory <--> C_Company\n",
      "Ovruch <--> Location\n",
      "Brother Alois <--> C_Person\n",
      "John Hannah (U.S. National Security Aide) <--> redirect::John P. Hannah\n",
      "Category:Spanish handball clubs <--> Other/None\n"
     ]
    }
   ],
   "source": [
    "data = parseWiki(test_sample=False, limit=300, save=False, evaluation=False)\n",
    "\n",
    "for i, x in enumerate(data):\n",
    "    if i > 150:\n",
    "        break\n",
    "    if x[1] == 'redirect':\n",
    "        print(x[0], '<-->', x[3])\n",
    "    else:\n",
    "        print(x[0], '<-->', x[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool \n",
    "from tqdm.notebook import tqdm\n",
    "from functools import partial\n",
    "import uuid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_dir = '/home/xminarikd/.keras/datasets/'\n",
    "dataset = [dataset_dir+file for file in os.listdir(dataset_dir)]\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a0f09f6a657472a98db8d2f82eb475e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=58.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/xminarikd/Documents/VINF/output/p1p41242.tsv done7.tsv done\n",
      "CPU times: user 278 ms, sys: 55.6 ms, total: 333 ms\n",
      "Wall time: 9min 42s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "pool = Pool(processes=4)\n",
    "results = []\n",
    "\n",
    "map_parser = partial(parseWiki, limit = 20000, save = True,evaluation=False)\n",
    "\n",
    "for x in tqdm(pool.imap_unordered(map_parser, dataset), total = len(dataset)):\n",
    "    results.append(x)\n",
    "\n",
    "pool.close()\n",
    "pool.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":) Connect\n"
     ]
    }
   ],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "from tqdm.auto import tqdm as tq\n",
    "def connect_elasticsearch():\n",
    "    _es = None\n",
    "    _es = Elasticsearch([{'host': 'localhost', 'port': 9200}])\n",
    "    if _es.ping():\n",
    "        print(':) Connect')\n",
    "    else:\n",
    "        print(':( could not connect!')\n",
    "    return _es\n",
    "\n",
    "es = connect_elasticsearch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_index(es_object, index_name='wiki'):\n",
    "    created = False\n",
    "    # index settings\n",
    "    settings = {\n",
    "        \"settings\": {\n",
    "            \"number_of_shards\": 1,\n",
    "            \"number_of_replicas\": 1,\n",
    "            'refresh_interval': '2s'\n",
    "        }\n",
    "#         \"mappings\": {\n",
    "#             \"properties\": {\n",
    "#                 \"title\": {\n",
    "#                     \"type\": \"text\"\n",
    "#                 },\n",
    "#                 \"category\": {\n",
    "#                     \"type\": \"long\"\n",
    "#                 }\n",
    "#             }            \n",
    "#         }\n",
    "    }\n",
    " \n",
    "    try:\n",
    "        if not es_object.indices.exists(index_name):\n",
    "            # Ignore 400 means to ignore \"Index Already Exist\" error.\n",
    "            es_object.indices.create(index=index_name, body=settings)\n",
    "            print('Created Index')\n",
    "        created = True\n",
    "    except Exception as ex:\n",
    "        print(str(ex))\n",
    "    finally:\n",
    "        return created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created Index\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_index(es,'wiki')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/xminarikd/Documents/VINF/output/p47788942p49288941.tsv',\n",
       " '/home/xminarikd/Documents/VINF/output/p31308443p32808442.tsv',\n",
       " '/home/xminarikd/Documents/VINF/output/p32808443p34308442.tsv',\n",
       " '/home/xminarikd/Documents/VINF/output/p28621851p30121850.tsv',\n",
       " '/home/xminarikd/Documents/VINF/output/p20460153p20570392.tsv',\n",
       " '/home/xminarikd/Documents/VINF/output/p49288942p50564553.tsv',\n",
       " '/home/xminarikd/Documents/VINF/output/p7054860p8554859.tsv',\n",
       " '/home/xminarikd/Documents/VINF/output/p15824603p17324602.tsv',\n",
       " '/home/xminarikd/Documents/VINF/output/p558392p958045.tsv',\n",
       " '/home/xminarikd/Documents/VINF/output/p52064554p53564553.tsv',\n",
       " '/home/xminarikd/Documents/VINF/output/p35522433p37022432.tsv',\n",
       " '/home/xminarikd/Documents/VINF/output/p23716198p25216197.tsv',\n",
       " '/home/xminarikd/Documents/VINF/output/p61525656p62585850.tsv',\n",
       " '/home/xminarikd/Documents/VINF/output/p13159683p14324602.tsv',\n",
       " '/home/xminarikd/Documents/VINF/output/p56564554p57025655.tsv',\n",
       " '/home/xminarikd/Documents/VINF/output/p63975910p65475424.tsv',\n",
       " '/home/xminarikd/Documents/VINF/output/p20570393p22070392.tsv',\n",
       " '/home/xminarikd/Documents/VINF/output/p11659683p13159682.tsv',\n",
       " '/home/xminarikd/Documents/VINF/output/p9172789p10672788.tsv',\n",
       " '/home/xminarikd/Documents/VINF/output/p8554860p9172788.tsv',\n",
       " '/home/xminarikd/Documents/VINF/output/p17324603p17460152.tsv',\n",
       " '/home/xminarikd/Documents/VINF/output/p53564554p55064553.tsv',\n",
       " '/home/xminarikd/Documents/VINF/output/p37022433p38522432.tsv',\n",
       " '/home/xminarikd/Documents/VINF/output/p958046p1483661.tsv',\n",
       " '/home/xminarikd/Documents/VINF/output/p58525656p60025655.tsv']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_files = getPathFiles('/output','.tsv')\n",
    "data_files = data_files[:25]\n",
    "data_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Other/None'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = readTsv(data_files[0])\n",
    "data[0]['Category']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def toElastic(files, elastic):\n",
    "    for x in tq(files):\n",
    "        data = readTsv(x)\n",
    "        for item in tq(data):\n",
    "            res = elastic.index(index='wiki', id=uuid.uuid4(), body={'title': item['Title'], 'category': mapperCategories(item['Category'])})\n",
    "            if res['result'] != 'created':\n",
    "                print('Warning, Error', res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dae9f0f4ae674fafb94480c1ae58d831",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=25.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbd38ef0596149e6bdb0e088581cc120",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=8847.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b6eaba88dea46b7affea574f0e1b958",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10114.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da87dda251bc4529a71aa8c47c74a708",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=9778.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b415013780e414097b35d8268e4c883",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=9247.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "895fc5800fdb4fd0abde9618ae64632d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=9622.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5f1ade1423f45cc9a515237c9997c3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=7721.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66704a4030754eb8b37635942f31b66b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e73cf85a78c04335a155ca7e27ad5e31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=9917.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8b02b7b4c1e45e3bd1f3115ae61321b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=9873.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51b01be766ab485cbf4ceaea5cf912b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10246.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf6e4cea341b47c086d24df26cf9c52b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=9916.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37aa02c89eae4a3ab4e35a813d64f55c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=9087.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "169b3941755c405e8a244a7da6f8d209",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=8226.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab2455c792fa4f6cbf7e7a0287d3bd5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=8455.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57c8655842b640cf9f02b3efd338686a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=8439.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a290284ee4694016b34eab3f2645b5df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=9505.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dab9aa4a7e9d47efa4200857ecfe1d8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=9713.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c74c3c7c6b984e6d9749bdbbe542921a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=6989.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07934071370d428f887164b3940dd99c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=9543.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "134f68d388c645e1bb50533274169c34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=9420.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4a7cc8af348431bb2cf8680fd38069d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=9567.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f19c37b8f864b67acb203a71a11e065",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=9848.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d50821a30c64bb38f5305f114e470f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=8953.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2502d6df36944879acecb097ecc7e0c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10678.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a5d6abd4f8a4a28bc9e6a3d9b6bd4e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=8890.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "toElastic(data_files,es)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sample searching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'title': 'Peter Vig', 'category': 4},\n",
       " {'title': 'Peter Vandermeer', 'category': 4},\n",
       " {'title': 'Peter Dertliev', 'category': 4},\n",
       " {'title': 'Peter Lavenda', 'category': 4},\n",
       " {'title': 'Peter Harker', 'category': 0},\n",
       " {'title': 'Peter Chasseaud', 'category': 0},\n",
       " {'title': 'Peter Onumanyi', 'category': 0},\n",
       " {'title': 'Peter Rowell', 'category': 0},\n",
       " {'title': 'Peter Mazan', 'category': 0},\n",
       " {'title': 'Peter Reichhardt', 'category': 0}]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "searchByTitle('Peter',es)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43ee4f06e29744b1b3892ece3d33daff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Text(value='', description='title'), Output()), _dom_classes=('widget-interact',))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@interact(title='')\n",
    "def searchByTitle(title):\n",
    "    res = es.search(index='wiki', body={'query':{'match':{'title': title}}})\n",
    "    return list(map(lambda x: x['_source'],res['hits']['hits']))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "res= es.search(index='wiki',body={'query':{'match':{'title':'Peter'}}})\n",
    "print(res['hits']['hits'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "redirect_files = getPathFiles('/output','-redirects')\n",
    "redirect_files = redirect_files[:25]\n",
    "len(redirect_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pridanych 22442 articles z poctu 263326\r"
     ]
    }
   ],
   "source": [
    "added = 0\n",
    "allarticles = 0\n",
    "for f in redirect_files:\n",
    "    data = readTsv(f)\n",
    "    allarticles += len(data)\n",
    "    for item in data:\n",
    "        res = searchExactMatchByTitle(item['Source'],es)\n",
    "        if res is not None:\n",
    "            es.index(\n",
    "                index='wiki', \n",
    "                id=uuid.uuid4(), \n",
    "                body={'title': item['Title'], 'category': mapperCategories(res['category'])}\n",
    "            )\n",
    "            added +=1 \n",
    "            print(f'Pridanych {added} articles z poctu {allarticles}', end = '\\r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'Peter Weatherson', 'category': 0}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "searchExactMatchByTitle(\"Peter Weatherson\",es)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def searchExactMatchByTitle(title, elastic, index='wiki'):\n",
    "    res = elastic.search(index=index,body=\n",
    "    {\n",
    "       \"query\" : {\n",
    "          \"term\" : {\n",
    "             \"title.keyword\" : title\n",
    "          }\n",
    "       }\n",
    "    })\n",
    "    if res['hits']['hits']:\n",
    "        return res['hits']['hits'][0]['_source']\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'took': 0, 'timed_out': False, '_shards': {'total': 1, 'successful': 1, 'skipped': 0, 'failed': 0}, 'hits': {'total': {'value': 0, 'relation': 'eq'}, 'max_score': None, 'hits': []}}\n"
     ]
    }
   ],
   "source": [
    "res= es.search(index='wiki',body={\n",
    "   \"query\" : {\n",
    "      \"term\" : {\n",
    "         \"Title.keyword\" : \"Andy\"\n",
    "      }\n",
    "   }\n",
    "})\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Delete all records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deleteIndex(elastic, index):\n",
    "    if elastic.indices.exists(index=index):\n",
    "        elastic.indices.delete(index=index)\n",
    "        print(f'Deleted index {index}')\n",
    "    else:\n",
    "        print(f'Index {index} not exist')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "deleteIndex(es, 'wiki')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding common categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "import nltk\n",
    "from collections import Counter\n",
    "import threading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "def process_whole_sentence(text):\n",
    "    sen = ' '.join(w for w in text.split() if w not in stopwords)\n",
    "    sen = re.sub(r'\\W', ' ', str(sen))\n",
    "    sen = re.sub(r'\\s+', ' ', sen, flags=re.I)\n",
    "    sen = sen.lower()\n",
    "    return sen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bigrams(text):\n",
    "    bigrams = []\n",
    "    for sen in text:\n",
    "        token = nltk.word_tokenize(sen)\n",
    "        bigrams.append(list(map(lambda x: ' '.join(x),list(ngrams(token,2)))))\n",
    "    return bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_bigrams(data_cat):\n",
    "    categories_processed = []\n",
    "    \n",
    "    cat_per = list(reduce(lambda x,y: x+y[2]['categories'],filter(lambda x: x[3] == 'Person',data_cat),[]))\n",
    "    cat_com = list(reduce(lambda x,y: x+y[2]['categories'],filter(lambda x: x[3] in ['Company'],data_cat),[]))\n",
    "    cat_org = list(reduce(lambda x,y: x+y[2]['categories'],filter(lambda x: x[3] in ['Organization'],data_cat),[]))\n",
    "    cat_loc = list(reduce(lambda x,y: x+y[2]['categories'],filter(lambda x: x[3] in ['Location'],data_cat),[]))\n",
    "    \n",
    "    categories_processed.append(sum(get_bigrams(stemming(map(lambda x: process_whole_sentence(x),cat_per))),[]))\n",
    "    categories_processed.append(sum(get_bigrams(stemming(map(lambda x: process_whole_sentence(x),cat_com))),[]))\n",
    "    categories_processed.append(sum(get_bigrams(stemming(map(lambda x: process_whole_sentence(x),cat_org))),[]))\n",
    "    categories_processed.append(sum(get_bigrams(stemming(map(lambda x: process_whole_sentence(x),cat_loc))),[]))\n",
    "    \n",
    "    return categories_processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words(data):\n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "    return [w for w in data if w not in stopwords]\n",
    "\n",
    "\n",
    "def tokenize(data):\n",
    "    symbols = symbols = \"!\\\"#$%&()*+'-./:;,<=>?@[\\]^_`{|}~\\n\"\n",
    "    tokens = word_tokenize(data)\n",
    "    tokens = [token.lower() for token in tokens if token not in list(symbols)]\n",
    "    return tokens\n",
    "\n",
    "\n",
    "def stemming(data):\n",
    "    stemmer= PorterStemmer()\n",
    "    return [stemmer.stem(token) for token in data]\n",
    "\n",
    "\n",
    "def preprocess(data):\n",
    "    data = tokenize(data)\n",
    "    data = remove_stop_words(data)\n",
    "    data = stemming(data)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dfcount(data):\n",
    "    df = {}\n",
    "    for i in range(len(data)):\n",
    "        for token in data[i]:\n",
    "            try:\n",
    "                df[token].add(i)\n",
    "            except:\n",
    "                df[token] = {i}\n",
    "    for i in df:\n",
    "        df[i] = len(df[i])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf_idf(data, doc_freq):\n",
    "    tfidf = {}\n",
    "    for i in range(len(data)):\n",
    "        counter = Counter(data[i])\n",
    "        count_w = len(data[i])\n",
    "        for token in np.unique(data[i]):\n",
    "            tf = counter[token]/count_w\n",
    "            df = doc_freq[token]\n",
    "            idf = np.log((len(data)+1)/(df+1))\n",
    "            tfidf[i, token] = tf*idf\n",
    "    return tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bigrams(text):\n",
    "    bigrams = []\n",
    "    for sen in text:\n",
    "        token = nltk.word_tokenize(sen)\n",
    "        bigrams.append(list(map(lambda x: ' '.join(x),list(ngrams(token,2)))))\n",
    "    return bigrams\n",
    "\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "def process_whole_sentence(text):\n",
    "    sen = ' '.join(w for w in text.split() if w not in stopwords)\n",
    "    sen = re.sub(r'\\W', ' ', str(sen))\n",
    "    sen = re.sub(r'\\s+', ' ', sen, flags=re.I)\n",
    "    sen = sen.lower()\n",
    "    return sen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSignificanteCategories(limit=2000, write=True):\n",
    "    categories = []\n",
    "\n",
    "    data_cat = parseWiki(limit=limit ,test_sample=False, save=False)\n",
    "    categories.append(reduce(lambda x,y: x+preprocess(y),reduce(lambda x,y: x+y[2]['categories'],filter(lambda x: x[3] == 'Person',data_cat),[]),[]))\n",
    "    categories.append(reduce(lambda x,y: x+preprocess(y),reduce(lambda x,y: x+y[2]['categories'],filter(lambda x: x[3] in ['Company'],data_cat),[]),[]))\n",
    "    categories.append(reduce(lambda x,y: x+preprocess(y),reduce(lambda x,y: x+y[2]['categories'],filter(lambda x: x[3] in ['Organization'],data_cat),[]),[]))\n",
    "    categories.append(reduce(lambda x,y: x+preprocess(y),reduce(lambda x,y: x+y[2]['categories'],filter(lambda x: x[3] in ['Location'],data_cat),[]),[]))\n",
    "    \n",
    "    del data_cat\n",
    "    \n",
    "    DF = dfcount(categories)\n",
    "    tfidf = tf_idf(categories, DF)\n",
    "    \n",
    "    c_person = None\n",
    "    c_company = None\n",
    "    c_org = None\n",
    "    c_location = None\n",
    "    \n",
    "    def task1():\n",
    "        c_person = {term:x for (doc, term), x in tfidf.items() if doc == 0}\n",
    "        c_person = sorted(c_person, key=c_person.__getitem__,reverse=True)\n",
    "        print('Person: ', c_person[:20])\n",
    "\n",
    "    def task2():\n",
    "        c_company = {term:x for (doc, term), x in tfidf.items() if doc == 1}\n",
    "        c_company = sorted(c_company, key=c_company.__getitem__,reverse=True)\n",
    "        print('Company: ', c_company[:20])\n",
    "\n",
    "    def task3():\n",
    "        c_org = {term:x for (doc, term), x in tfidf.items() if doc == 2}\n",
    "        c_org = sorted(c_org, key=c_org.__getitem__,reverse=True)\n",
    "        print('Organisation: ', c_org[:20])\n",
    "\n",
    "    def task4():\n",
    "        c_location = {term:x for (doc, term), x in tfidf.items() if doc == 3}\n",
    "        c_location = sorted(c_location, key=c_location.__getitem__, reverse=True)\n",
    "        print('Location: ', c_location[:20])\n",
    "    \n",
    "    t1 = threading.Thread(target=task1, name='t1') \n",
    "    t2 = threading.Thread(target=task2, name='t2') \n",
    "    t3 = threading.Thread(target=task3, name='t3') \n",
    "    t4 = threading.Thread(target=task4, name='t4')\n",
    "    \n",
    "    t1.start()\n",
    "    t2.start()\n",
    "    t3.start()\n",
    "    t4.start()\n",
    "    \n",
    "    \n",
    "    t1.join()\n",
    "    t2.join()\n",
    "    t3.join()\n",
    "    t4.join()\n",
    "    \n",
    "    if write:\n",
    "        print('Person: ', c_person[:20])\n",
    "        print('')\n",
    "        print('Company: ', c_company[:20])\n",
    "        print('')\n",
    "        print('Organisation: ', c_org[:20])\n",
    "        print('')\n",
    "        print('Location: ', c_location[:20])\n",
    "    \n",
    "    return {'person': c_person, 'company': c_company, 'org': c_org, 'location': c_location}\n",
    "\n",
    "\n",
    "def getSignificanteCategoriesBigrams(limit=2000, write=True):\n",
    "    data_cat = parseWiki(limit=limit ,test_sample=False, save=False)\n",
    "    categories_processed = []\n",
    "    \n",
    "    cat_per = list(reduce(lambda x,y: x+y[2]['categories'],filter(lambda x: x[3] == 'Person',data_cat),[]))\n",
    "    cat_com = list(reduce(lambda x,y: x+y[2]['categories'],filter(lambda x: x[3] in ['Company'],data_cat),[]))\n",
    "    cat_org = list(reduce(lambda x,y: x+y[2]['categories'],filter(lambda x: x[3] in ['Organization'],data_cat),[]))\n",
    "    cat_loc = list(reduce(lambda x,y: x+y[2]['categories'],filter(lambda x: x[3] in ['Location'],data_cat),[]))\n",
    "    \n",
    "    categories_processed.append(sum(get_bigrams(stemming(map(lambda x: process_whole_sentence(x),cat_per))),[]))\n",
    "    categories_processed.append(sum(get_bigrams(stemming(map(lambda x: process_whole_sentence(x),cat_com))),[]))\n",
    "    categories_processed.append(sum(get_bigrams(stemming(map(lambda x: process_whole_sentence(x),cat_org))),[]))\n",
    "    categories_processed.append(sum(get_bigrams(stemming(map(lambda x: process_whole_sentence(x),cat_loc))),[]))\n",
    "    \n",
    "    del data_cat\n",
    "    \n",
    "    DF = dfcount(categories_processed)\n",
    "    tfidf = tf_idf(categories_processed, DF)\n",
    "    \n",
    "    c_person = {term:x for (doc, term), x in tfidf.items() if doc == 0}\n",
    "    c_person = sorted(c_person, key=c_person.__getitem__,reverse=True)\n",
    "\n",
    "    c_company = {term:x for (doc, term), x in tfidf.items() if doc == 1}\n",
    "    c_company = sorted(c_company, key=c_company.__getitem__,reverse=True)\n",
    "\n",
    "    c_org = {term:x for (doc, term), x in tfidf.items() if doc == 2}\n",
    "    c_org = sorted(c_org, key=c_org.__getitem__,reverse=True)\n",
    "\n",
    "    c_location = {term:x for (doc, term), x in tfidf.items() if doc == 3}\n",
    "    c_location = sorted(c_location, key=c_location.__getitem__, reverse=True)\n",
    "    \n",
    "    if write:\n",
    "        print('Person: ', c_person[:20])\n",
    "        print('')\n",
    "        print('Company: ', c_company[:20])\n",
    "        print('')\n",
    "        print('Organisation: ', c_org[:20])\n",
    "        print('')\n",
    "        print('Location: ', c_location[:20])\n",
    "    \n",
    "    return {'person': c_person, 'company': c_company, 'org': c_org, 'location': c_location}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Person:  ['living peopl', '20th century', 'f c', 'c play', 'century american', '21st century', 'american male', 'association football', 'league play', 'expatriate footballers', 'expatriate sportspeople', 'cup play', 'international footbal', 'rugby league', 'university alumni', 'fc play', 'musical groups', 'ice hockey', 'world cup', 'american people']\n",
      "\n",
      "Company:  ['companies established', 'companies based', 'companies united', 'services companies', 'financial services', 'mergers acquisit', 'american companies', 'stock exchanges', 'video game', 'game companies', 'manufacturing companies', 'chains united', 'companies listed', 'exchanges africa', 'restaurants established', 'stock exchang', 'companies disestablished', 'mass media', 'media companies', 'restaurant chains']\n",
      "\n",
      "Organisation:  ['based united', 'non profit', 'learned societies', 'profit organizations', 'organizations established', 'organisations based', 'organizations based', '3 organ', '501 c', 'associations based', 'c 3', 'charities based', 'consultative status', 'established 1946', 'professional associations', 'psychology organizations', 'relief organ', 'societies canada', 'status united', 'english football']\n",
      "\n",
      "Location:  ['pyrénées atlantiqu', 'communes pyrénées', 'articles needing', 'atlantiques communes', 'communes articles', 'french wikipedia', 'needing translation', 'pyrénées atlantiques', 'translation french', 'lower navarr', 'populated places', 'register historic', 'national register', 'historic places', 'towns luxembourg', 'unincorporated communities', 'civil parishes', 'buildings structures', 'neighborhoods pittsburgh', 'sports venues']\n"
     ]
    }
   ],
   "source": [
    "bires = getSignificanteCategoriesBigrams(limit=10000,write=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Person:  ['player', 'birth', 'male', 'death', 'expatri', 'peopl', 'alumni', 'live', 'sportspeopl', 'actor', 'writer', 'descent', 'footbal', '21st-centuri', 'cricket', '20th-centuri', 'singer', 'politician', 'musician', 'actress']\n",
      "Company:  ['exchang', 'brand', 'acquisit', 'merger', 'defunct', 'manufactur', 'softwar', 'label', 'cloth', 'vehicl', 'retail', 'disestablish', 'restaur', 'video', 'fast-food', 'nasdaq', 'onlin', 'publish', 'chain', 'stock']\n",
      "Organisation: Location:  ['station', 'build', 'pyrénées-atlantiqu', 'regist', 'popul', 'complet', 'place', 'airport', 'venu', 'town', 'school', 'aerodrom', 'need', 'counti', 'railway', 'villag', 'unincorpor', 'great', 'mountain', 'open']\n",
      " ['sahara', 'scout', 'youth', '501', 'gang', 'non-profit', 'polisario', 'chariti', 'c', 'learn', 'polit', 'societi', 'advocaci', 'ambul', 'anti-christian', 'anti-vaccin', 'child-rel', 'kazakhstan', 'multi-sport', 'non-government']\n"
     ]
    }
   ],
   "source": [
    "res = getSignificanteCategories(limit=30000, write=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printResults(result, limit):\n",
    "    print('Person: ',result['person'][:limit])\n",
    "    print('')\n",
    "    print('Company: ', result['company'][:limit])\n",
    "    print('')\n",
    "    print('Organisation: ',result['org'][:limit])\n",
    "    print('')\n",
    "    print('Location: ', result['location'][:limit])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Person:  ['living peopl', '20th century', 'f c', 'c play', 'century american', '21st century', 'american male', 'association football', 'league play', 'expatriate footballers', 'expatriate sportspeople', 'cup play', 'international footbal', 'rugby league', 'university alumni', 'fc play', 'musical groups', 'ice hockey', 'world cup', 'american people', 'fifa world', 'male actors', 'football league', 'male actor', 'expatriate footbal', 'military personnel', 'people educated', 'hockey players', 'male writ', 'records artist', 'draft pick', 'century indian', 'football manag', 'male television', 'film actor', 'uk mps', 'male film', 'soccer play', 'television actor', 'united f', 'year birth', 'living people', 'winter olymp', 'birth missing', 'football midfield', 'missing living', 'major league', 'school alumni', '19th century', 'new zealand']\n",
      "\n",
      "Company:  ['companies established', 'companies based', 'companies united', 'services companies', 'financial services', 'mergers acquisit', 'american companies', 'stock exchanges', 'video game', 'game companies', 'manufacturing companies', 'chains united', 'companies listed', 'exchanges africa', 'restaurants established', 'stock exchang', 'companies disestablished', 'mass media', 'media companies', 'restaurant chains', 'retail companies', 'companies filed', 'defunct companies', 'filed chapter', 'internet properties', 'properties established', 'retailers united', '11 bankruptcy', 'chapter 11', 'companies canada', 'companies formerly', 'established 1960', 'established 1995', 'fast food', 'formerly listed', 'listed new', 'manufacturers united', 'york stock', 'established 1989', 'establishments california', 'based austin', 'british companies', 'clothing companies', 'companies england', 'companies isle', 'development compani', 'established 1950', 'established 1974', 'established 2003', 'food chains']\n",
      "\n",
      "Organisation:  ['based united', 'non profit', 'learned societies', 'profit organizations', 'organizations established', 'organisations based', 'organizations based', '3 organ', '501 c', 'associations based', 'c 3', 'charities based', 'consultative status', 'established 1946', 'professional associations', 'psychology organizations', 'relief organ', 'societies canada', 'status united', 'english football', 'establishments united', 'youth organizations', 'united nations', '1845 establishments', '1859 establishments', '1864 establishments', '1907 establishments', '1908 establishments', '1959 establishments', '1982 establishments', '1996 establishments', '19th centuri', 'academy financial', 'advocacy organ', 'aid organ', 'air ambulance', 'ambulance servic', 'ambulance services', 'american council', 'american organized', 'ancient near', 'awards h', 'banks texa', 'bar associ', 'based geneva', 'based hong', 'based montr', 'based surrey', 'based switzerland', 'based tyne']\n",
      "\n",
      "Location:  ['pyrénées atlantiqu', 'communes pyrénées', 'articles needing', 'atlantiques communes', 'communes articles', 'french wikipedia', 'needing translation', 'pyrénées atlantiques', 'translation french', 'lower navarr', 'populated places', 'register historic', 'national register', 'historic places', 'towns luxembourg', 'unincorporated communities', 'civil parishes', 'buildings structures', 'neighborhoods pittsburgh', 'sports venues', 'western australia', 'parishes leicestershir', 'villages leicestershir', 'cities towns', 'protected areas', 'borough charnwood', 'places established', 'suburbs perth', 'west virginia', 'tourist attractions', 'buildings completed', 'city rockingham', 'suburbs city', 'shopping malls', 'new jersey', 'county west', 'historic house', 'house museums', 'venues completed', 'county california', 'county virginia', 'alzette canton', 'communes esch', 'county massachusett', 'esch sur', 'former communes', 'mountains hills', 'rhode island', 'road bridges', 'sur alzette']\n"
     ]
    }
   ],
   "source": [
    "printResults(bires,50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "300 000 articles cca 45 minutes need refactoring\n",
    "\n",
    "Person=['player', 'male', 'actor', 'sportspeopl', 'medalist', 'actress', 'expatri', 'singer', 'musician', 'live', 'writer', 'politician', 'f.c', 'alumni', 'personnel', 'olymp', '20th-centuri', 'faculti', 'coach', 'guitarist']\n",
    "\n",
    "Company=['brand', 'merger', 'retail', 'exchang', 'stock', 'label', 'nasdaq', 'multin', 'subsidiari', 'acquisit', 'onlin', 'offer', 'held', 'conglomer', 'drink', 'vehicl', 'softwar', 'equip', 'store', 'bankruptci']\n",
    "\n",
    "Organisation=['scout', 'think', 'non-profit', 'girl', 'gang', 'multi-sport', 'event', 'recur', 'religi', 'tank', 'child-rel', 'non-align', 'non-government', 'critic', 'right', 'chess', 'evangel', 'movement|', 'yakuza', 'advocaci']\n",
    "\n",
    "Location=['regist', 'unincorpor', 'station', 'popul', 'complet', 'aerodrom', 'villag', 'town', 'landform', 'parish', 'river', 'seaplan', 'open', 'census-design', 'mountain', 'attract', 'neighbourhood', 'suburb', 'rang', 'airport']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "300 000 first 100\n",
    "Person:  ['player', 'male', 'actor', 'sportspeopl', 'medalist', 'actress', 'expatri', 'singer', 'musician', 'live', 'writer', 'politician', 'f.c', 'alumni', 'personnel', 'olymp', '20th-centuri', 'faculti', 'coach', 'guitarist', 'basebal', 'novelist', 'emigr', 'descent', 'cup', '21st-centuri', 'mp', 'painter', 'femal', 'journalist', 'poet', 'compos', 'draft', 'pick', 'repres', '19th-centuri', 'summer', 'champion', 'screenwrit', 'lawyer', 'director', 'swimmer', 'soccer', 'forward', 'skater', 'burial', 'midfield', 'field', 'ice', 'gold', 'non-fict', 'basketbal', 'winter', 'recipi', 'comedian', 'fifa', 'filipino', 'businesspeopl', 'defend', 'senat', 'silver', 'major', 'songwrit', 'scientist', 'minist', 'medal', 'fc', 'medallist', 'staff', 'singer-songwrit', 'voic', 'scholar', 'fellow', 'boxer', 'wrestler', 'historian', 'pan', 'drummer', 'universiad', 'rock', 'figur', 'bundesliga', 'cemeteri', 'rugbi', 'bronz', 'pianist', 'dramatist', 'merit', 'playwright', 'cyclist', 'stage', 'inducte', 'mayor', 'under-21', 'activist', 'xi', 'republican', 'first', 'governor', 'presid']\n",
    "\n",
    "Company:  ['brand', 'merger', 'retail', 'exchang', 'stock', 'label', 'nasdaq', 'multin', 'subsidiari', 'acquisit', 'onlin', 'offer', 'held', 'conglomer', 'drink', 'vehicl', 'softwar', 'equip', 'store', 'bankruptci', 'file', 'cloth', 'non-renew', 'chapter', 'shoe', 'supermarket', 'initi', 'formerli', 'properti', 'publish', 'portfolio', 'chain', 'supplier', 'chocol', 'luxuri', 'tokyo', 'equiti', 'phone', 'applianc', 'part', 'ga', 'motor', 'truck', 'bakeri', 'group|', 'midwestern', 'toy', 'housebuild', 'web', 'hold', 'fashion', 'headquart', 'studio', 'breweri', '11', 'government-own', 'snack', 'spin-off', 'energi', 'fast-food', 'oil', 'pharmaceut', 'amplifi', 'eyewear', 'nationalis', 'encyclopedia', '2010', 'resourc', 'discontinu', 'euronext', 'outsourc', 'r.a', 're-establish', 'guitar', 'colorado', '2017', 'magazin', 'mobil', 'firearm', 'googl', 'warrant', '2008', 'indiana', 'pipelin', 'provid', 'chaebol', 'condiment', 'dairi', 'discount', 'index', 'mortgag', 'poultri', 'coffe', 'cosmet', 'distribut', 'fuel', '2020', 'consult', 'rock', 'station']\n",
    "\n",
    "Organisation:  ['scout', 'think', 'non-profit', 'girl', 'gang', 'multi-sport', 'event', 'recur', 'religi', 'tank', 'child-rel', 'non-align', 'non-government', 'critic', 'right', 'chess', 'evangel', 'movement|', 'yakuza', 'advocaci', 'patronag', 'usa', 'games|', 'sahara', 'accreditor', 'america|', 'associations|', 'association|', 'hispanic-american', 'ioc-recognis', 'lobbi', 'metalwork', 'polisario', 'supraorgan', '501', 'bolivia', 'femin', 'intergovernment', 'secret', 'traffick', 'learn', 'asian', 'publish', 'ambul', 'anti-abort', 'anti-vaccin', 'consortia', 'feminist', 'parachurch', 'shelter', 'veteran', 'diego', 'adi', 'advaita', 'anti-vivisect', 'awards|', 'caloust', 'churches|thailand', 'education|', 'federation|', 'foundation|', 'genet', 'gmb', 'gulbenkian', 'irredent', 'metric', 'pageants|california', 'philanthrop', 'positiv', 'puri', 'shankara', 'shankaracharya', 'states–european', 'sub-confeder', 'taxat', 'treati', 'trust|', 'vedanta', 'vexillolog', 'center', 'confeder', 'local', 'nebraska', 'olymp', 'anglican', 'denomin', 'labor', 'missionari', 'scientolog', 'welfar', '1778', 'activist', 'anti-christian', 'biblic', 'carpent', 'certif', 'combat', 'emerg', 'homeless', 'israeli–palestinian']\n",
    "\n",
    "Location:  ['regist', 'unincorpor', 'station', 'popul', 'complet', 'aerodrom', 'villag', 'town', 'landform', 'parish', 'river', 'seaplan', 'open', 'census-design', 'mountain', 'attract', 'neighbourhood', 'suburb', 'rang', 'airport', 'certifi', 'secondari', 'district|', 'site', 'skyscrap', 'pyrénées-atlantiqu', 'basketbal', 'stadium', 'demolish', 'need', 'vaud', 'coast', 'tributari', 'arena', 'neighborhood', 'dam', 'tunnel', 'saskatchewan', 'monument', 'serv', 'multi-purpos', 'mall', 'lighthous', 'pradesh', 'locat', 'volcano', 'norfolk', 'coastal', 'mojav', 'territori', 'canton', 'township', 'subprefectur', 'desert', 'volleybal', 'derbyshir', 'grassland', 'hill', 'censu', 'castl', 'casino', 'landmark', 'governor', 'voivodeship', 'glacier', 'line', 'valley', 'residenti', 'subway', 'nova', 'colorado', 'close', 'scotia', 'princ', 'reservoir', 'grade', 'offic', 'properti', 'abellio', 'scotrail', 'local', 'indoor', 'lrt', 'uninhabit', 'metropolitan', 'oklahoma', 'suffolk', 'wikipedia', 'montana', 'translat', 'cumbria', 'indiana', 'dioces', 'sculptur', 'divis', 'punggol', 'navarr', 'instal', 'reserv', 'verd']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Person:  ['living peopl', '20th century', 'f c', 'c play', 'century american', '21st century', 'american male', 'association football', 'league play', 'expatriate footballers', 'expatriate sportspeople', 'cup play', 'international footbal', 'rugby league', 'university alumni', 'fc play', 'musical groups', 'ice hockey', 'world cup', 'american people', 'fifa world', 'male actors', 'football league', 'male actor', 'expatriate footbal', 'military personnel', 'people educated', 'hockey players', 'male writ', 'records artist', 'draft pick', 'century indian', 'football manag', 'male television', 'film actor', 'uk mps', 'male film', 'soccer play', 'television actor', 'united f', 'year birth', 'living people', 'winter olymp', 'birth missing', 'football midfield', 'missing living', 'major league', 'school alumni', '19th century', 'new zealand']\n",
    "\n",
    "Company:  ['companies established', 'companies based', 'companies united', 'services companies', 'financial services', 'mergers acquisit', 'american companies', 'stock exchanges', 'video game', 'game companies', 'manufacturing companies', 'chains united', 'companies listed', 'exchanges africa', 'restaurants established', 'stock exchang', 'companies disestablished', 'mass media', 'media companies', 'restaurant chains', 'retail companies', 'companies filed', 'defunct companies', 'filed chapter', 'internet properties', 'properties established', 'retailers united', '11 bankruptcy', 'chapter 11', 'companies canada', 'companies formerly', 'established 1960', 'established 1995', 'fast food', 'formerly listed', 'listed new', 'manufacturers united', 'york stock', 'established 1989', 'establishments california', 'based austin', 'british companies', 'clothing companies', 'companies england', 'companies isle', 'development compani', 'established 1950', 'established 1974', 'established 2003', 'food chains']\n",
    "\n",
    "Organisation:  ['based united', 'non profit', 'learned societies', 'profit organizations', 'organizations established', 'organisations based', 'organizations based', '3 organ', '501 c', 'associations based', 'c 3', 'charities based', 'consultative status', 'established 1946', 'professional associations', 'psychology organizations', 'relief organ', 'societies canada', 'status united', 'english football', 'establishments united', 'youth organizations', 'united nations', '1845 establishments', '1859 establishments', '1864 establishments', '1907 establishments', '1908 establishments', '1959 establishments', '1982 establishments', '1996 establishments', '19th centuri', 'academy financial', 'advocacy organ', 'aid organ', 'air ambulance', 'ambulance servic', 'ambulance services', 'american council', 'american organized', 'ancient near', 'awards h', 'banks texa', 'bar associ', 'based geneva', 'based hong', 'based montr', 'based surrey', 'based switzerland', 'based tyne']\n",
    "\n",
    "Location:  ['pyrénées atlantiqu', 'communes pyrénées', 'articles needing', 'atlantiques communes', 'communes articles', 'french wikipedia', 'needing translation', 'pyrénées atlantiques', 'translation french', 'lower navarr', 'populated places', 'register historic', 'national register', 'historic places', 'towns luxembourg', 'unincorporated communities', 'civil parishes', 'buildings structures', 'neighborhoods pittsburgh', 'sports venues', 'western australia', 'parishes leicestershir', 'villages leicestershir', 'cities towns', 'protected areas', 'borough charnwood', 'places established', 'suburbs perth', 'west virginia', 'tourist attractions', 'buildings completed', 'city rockingham', 'suburbs city', 'shopping malls', 'new jersey', 'county west', 'historic house', 'house museums', 'venues completed', 'county california', 'county virginia', 'alzette canton', 'communes esch', 'county massachusett', 'esch sur', 'former communes', 'mountains hills', 'rhode island', 'road bridges', 'sur alzette']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1983 birth',\n",
       " 'australian rugby',\n",
       " 'rugby league',\n",
       " 'league play',\n",
       " 'rugby league',\n",
       " 'league players',\n",
       " 'players queensland',\n",
       " 'brisbane broncos',\n",
       " 'broncos play',\n",
       " 'canterbury bankstown',\n",
       " 'bankstown bulldogs',\n",
       " 'bulldogs play',\n",
       " 'queensland rugby',\n",
       " 'rugby league',\n",
       " 'league state',\n",
       " 'state origin',\n",
       " 'origin play',\n",
       " 'rugby league',\n",
       " 'league five',\n",
       " 'five eighth',\n",
       " 'rugby league',\n",
       " 'league centr',\n",
       " 'rugby league',\n",
       " 'league lock',\n",
       " 'people educated',\n",
       " 'educated padua',\n",
       " 'padua college',\n",
       " 'college brisbane',\n",
       " 'sportspeople townsvil',\n",
       " 'rugby league',\n",
       " 'league second',\n",
       " 'second row',\n",
       " 'wynnum manly',\n",
       " 'manly seagulls',\n",
       " 'seagulls play',\n",
       " 'living peopl']"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(get_bigrams(stemming(map(lambda x: process_whole_sentence(x), data[0][2]['categories']))),[])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overenie pridelovania kategorii"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evalueation_dataset(data=None):\n",
    "    if data is None:\n",
    "        data = '/home/xminarikd/.keras/datasets/enwiki-20201001-pages-articles9.xml-p2936261p4045402.bz2'\n",
    "    parser = xml.sax.make_parser()\n",
    "    parser.setContentHandler(handler,evaluation=True)\n",
    "\n",
    "    for i, line in enumerate(subprocess.Popen(['bzcat'], \n",
    "                                 stdin = open(data), \n",
    "                                 stdout = subprocess.PIPE).stdout):\n",
    "\n",
    "        try:\n",
    "            parser.feed(line)\n",
    "        except StopIteration:\n",
    "            break\n",
    "\n",
    "        # get only some results\n",
    "        if len(handler.output) >= limit:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation = parseWiki(limit=20000 ,test_sample=False, save=False, evaluation=True)\n",
    "# original = parseWiki(limit=20000 ,test_sample=False, save=False, evaluation=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapperCategories(arg):\n",
    "    switcher = {\n",
    "        'Person': 0,\n",
    "        'A_Person':0,\n",
    "        'C_Person':0,\n",
    "        'Company': 1,\n",
    "        'A_Company':1,\n",
    "        'C_Company':1,\n",
    "        'Organization':2,\n",
    "        'A_Organization':2,\n",
    "        'C_Organization':2,\n",
    "        'Location':3,\n",
    "        'A_Location':3,\n",
    "        'C_Location':3,\n",
    "        'Another':4\n",
    "    }\n",
    "    return switcher.get(arg,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'evaluation' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-108-e3ad100bd990>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"Person\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Company'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Organization'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Location'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0my_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapperCategories\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'evaluation' is not defined"
     ]
    }
   ],
   "source": [
    "y_test = []\n",
    "y_pred = []\n",
    "\n",
    "for e, o in zip(evaluation, original):\n",
    "    if o[3] in [\"Person\",'Company','Organization','Location','Another']:\n",
    "        y_test.append(mapperCategories(o[3]))\n",
    "        y_pred.append(mapperCategories(e[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2904"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "105780"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(filter(lambda x: x == 4,y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[79537     5     6     9  1207]\n",
      " [   13  4924    69    20   682]\n",
      " [   37   164  1143    40   625]\n",
      " [  721   414   230 45134 10983]\n",
      " [ 6153  2428  1923  8684 73095]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.98      0.95     80764\n",
      "           1       0.62      0.86      0.72      5708\n",
      "           2       0.34      0.57      0.42      2009\n",
      "           3       0.84      0.79      0.81     57482\n",
      "           4       0.84      0.79      0.82     92283\n",
      "\n",
      "    accuracy                           0.86    238246\n",
      "   macro avg       0.71      0.80      0.75    238246\n",
      "weighted avg       0.86      0.86      0.86    238246\n",
      "\n",
      "0.8555568613953645\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_path = os.getcwd().rsplit('/', 1)[0]\n",
    "data_files_original = f'{out_path}/output/'\n",
    "data_files_original = [file for file in os.listdir(data_files_original) if file.endswith('.tsv')]\n",
    "len(data_files_original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = []\n",
    "y_pred = []\n",
    "\n",
    "for x in data_files_original:\n",
    "    original = readTsv(f'{out_path}/output/{x}')\n",
    "    evaluated = readTsv(f'{out_path}/output/eval/{x}')\n",
    "    for o, e in zip(original, evaluated):\n",
    "        if o['Category'] in [\"Person\",'Company','Organization','Location','Another']:\n",
    "            y_test.append(mapperCategories(o['Category']))\n",
    "            y_pred.append(mapperCategories(e['Category']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "238246"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Vysledok 20 000 z ka6deho suboru, 145 963 filtorvany clankov, \n",
    "Unigrams\n",
    "V tomto je chyba, kedže nebolo testovanie kategórie 4,tj. other\n",
    "\n",
    "[[76558    19    32    21  4134]\n",
    " [    9  4810    37    34   818]\n",
    " [   33    33   799    49  1095]\n",
    " [   47   244   134 50528  6529]\n",
    " [    0     0     0     0     0]]\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       1.00      0.95      0.97     80764\n",
    "           1       0.94      0.84      0.89      5708\n",
    "           2       0.80      0.40      0.53      2009\n",
    "           3       1.00      0.88      0.93     57482\n",
    "           4       0.00      0.00      0.00         0\n",
    "\n",
    "    accuracy                           0.91    145963\n",
    "   macro avg       0.75      0.61      0.67    145963\n",
    "weighted avg       0.99      0.91      0.95    145963\n",
    "\n",
    "0.9091002514335825\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Bigrams a unigrams spolu\n",
    "[[79693     6    17    23  1025]\n",
    " [   22  5196    57    25   408]\n",
    " [   63   174  1220    41   511]\n",
    " [  744   420   262 50426  5630]\n",
    " [ 9813  2925  5506 11094 62945]]\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.88      0.99      0.93     80764\n",
    "           1       0.60      0.91      0.72      5708\n",
    "           2       0.17      0.61      0.27      2009\n",
    "           3       0.82      0.88      0.85     57482\n",
    "           4       0.89      0.68      0.77     92283\n",
    "\n",
    "    accuracy                           0.84    238246\n",
    "   macro avg       0.67      0.81      0.71    238246\n",
    "weighted avg       0.86      0.84      0.84    238246\n",
    "\n",
    "0.8372858306120564\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Unigrams ....\n",
    "[[76889    19    26    21  3809]\n",
    " [    9  4810    36    34   819]\n",
    " [   33    33   765    49  1129]\n",
    " [   48   244   127 50533  6530]\n",
    " [ 5034   771  4869 10830 70779]]\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.94      0.95      0.94     80764\n",
    "           1       0.82      0.84      0.83      5708\n",
    "           2       0.13      0.38      0.20      2009\n",
    "           3       0.82      0.88      0.85     57482\n",
    "           4       0.85      0.77      0.81     92283\n",
    "\n",
    "    accuracy                           0.86    238246\n",
    "   macro avg       0.71      0.76      0.73    238246\n",
    "weighted avg       0.87      0.86      0.86    238246\n",
    "\n",
    "0.8553176128875196\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "BIGRAMS Only\n",
    "[[79537     5     6     9  1207]\n",
    " [   13  4924    69    20   682]\n",
    " [   37   164  1143    40   625]\n",
    " [  721   414   230 45134 10983]\n",
    " [ 6153  2428  1923  8684 73095]]\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.92      0.98      0.95     80764\n",
    "           1       0.62      0.86      0.72      5708\n",
    "           2       0.34      0.57      0.42      2009\n",
    "           3       0.84      0.79      0.81     57482\n",
    "           4       0.84      0.79      0.82     92283\n",
    "\n",
    "    accuracy                           0.86    238246\n",
    "   macro avg       0.71      0.80      0.75    238246\n",
    "weighted avg       0.86      0.86      0.86    238246\n",
    "\n",
    "0.8555568613953645\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Vysledok ako predosle, ale bez pridelovania na zaklade zoznamu vytvorenemu kategori\n",
    "\n",
    "[[72236     0     8     0  8520]\n",
    " [    0  4206    19     6  1477]\n",
    " [    0    13   377    23  1596]\n",
    " [    2   231    79 39294 17876]\n",
    " [    0     0     0     0     0]]\n",
    "\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       1.00      0.89      0.94     80764\n",
    "           1       0.95      0.74      0.83      5708\n",
    "           2       0.78      0.19      0.30      2009\n",
    "           3       1.00      0.68      0.81     57482\n",
    "           4       0.00      0.00      0.00         0\n",
    "\n",
    "    accuracy                           0.80    145963\n",
    "   macro avg       0.74      0.50      0.58    145963\n",
    "weighted avg       0.99      0.80      0.88    145963\n",
    "\n",
    "0.795496118879442\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "V7sledok ako predosle, ale categorie pridelovanie na zaklade bigrams\n",
    "\n",
    "[[79537     5     6     9  1207]\n",
    " [   13  4924    69    20   682]\n",
    " [   37   164  1143    40   625]\n",
    " [  721   414   230 45134 10983]\n",
    " [    0     0     0     0     0]]\n",
    "              precision    recall  f1-score   support\n",
    "\n",
    "           0       0.99      0.98      0.99     80764\n",
    "           1       0.89      0.86      0.88      5708\n",
    "           2       0.79      0.57      0.66      2009\n",
    "           3       1.00      0.79      0.88     57482\n",
    "           4       0.00      0.00      0.00         0\n",
    "\n",
    "    accuracy                           0.90    145963\n",
    "   macro avg       0.73      0.64      0.68    145963\n",
    "weighted avg       0.99      0.90      0.94    145963\n",
    "\n",
    "0.8956927440515747\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test and data searching area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "import seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>David Stagg</td>\n",
       "      <td>{Infobox rugby league biography\\n|name        ...</td>\n",
       "      <td>{'type': 'rugby league biography', 'parameters...</td>\n",
       "      <td>A_Person</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Amaranthus mantegazzianus</td>\n",
       "      <td>redirect</td>\n",
       "      <td>Amaranthus caudatus</td>\n",
       "      <td>redirect::Amaranthus caudatus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Amaranthus quitensis</td>\n",
       "      <td>redirect</td>\n",
       "      <td>Amaranthus hybridus</td>\n",
       "      <td>redirect::Amaranthus hybridus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Maud Queen of Norway</td>\n",
       "      <td>redirect</td>\n",
       "      <td>Maud of Wales</td>\n",
       "      <td>redirect::Maud of Wales</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Milligram per litre</td>\n",
       "      <td>redirect</td>\n",
       "      <td>Gram per litre</td>\n",
       "      <td>redirect::Gram per litre</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Utica Psychiatric Center</td>\n",
       "      <td>{Infobox NRHP | name =Utica State Hospital, Ma...</td>\n",
       "      <td>{'type': 'nrhp', 'parameters': ['name', 'nrhp_...</td>\n",
       "      <td>C_Location</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Olean Wholesale Grocery</td>\n",
       "      <td>no infobox/redirect</td>\n",
       "      <td>{'categories': ['Companies based in Cattaraugu...</td>\n",
       "      <td>C_Company</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Queen Tiye</td>\n",
       "      <td>redirect</td>\n",
       "      <td>Tiye</td>\n",
       "      <td>redirect::Tiye</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Queen Hatshepsut</td>\n",
       "      <td>redirect</td>\n",
       "      <td>Hatshepsut</td>\n",
       "      <td>redirect::Hatshepsut</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Clibanarii</td>\n",
       "      <td>no infobox/redirect</td>\n",
       "      <td>{'categories': ['Cavalry', 'Asian armour', 'Ty...</td>\n",
       "      <td>Other/None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           0  \\\n",
       "0                David Stagg   \n",
       "1  Amaranthus mantegazzianus   \n",
       "2       Amaranthus quitensis   \n",
       "3       Maud Queen of Norway   \n",
       "4        Milligram per litre   \n",
       "5   Utica Psychiatric Center   \n",
       "6    Olean Wholesale Grocery   \n",
       "7                 Queen Tiye   \n",
       "8           Queen Hatshepsut   \n",
       "9                 Clibanarii   \n",
       "\n",
       "                                                   1  \\\n",
       "0  {Infobox rugby league biography\\n|name        ...   \n",
       "1                                           redirect   \n",
       "2                                           redirect   \n",
       "3                                           redirect   \n",
       "4                                           redirect   \n",
       "5  {Infobox NRHP | name =Utica State Hospital, Ma...   \n",
       "6                                no infobox/redirect   \n",
       "7                                           redirect   \n",
       "8                                           redirect   \n",
       "9                                no infobox/redirect   \n",
       "\n",
       "                                                   2  \\\n",
       "0  {'type': 'rugby league biography', 'parameters...   \n",
       "1                                Amaranthus caudatus   \n",
       "2                                Amaranthus hybridus   \n",
       "3                                      Maud of Wales   \n",
       "4                                     Gram per litre   \n",
       "5  {'type': 'nrhp', 'parameters': ['name', 'nrhp_...   \n",
       "6  {'categories': ['Companies based in Cattaraugu...   \n",
       "7                                               Tiye   \n",
       "8                                         Hatshepsut   \n",
       "9  {'categories': ['Cavalry', 'Asian armour', 'Ty...   \n",
       "\n",
       "                               3  \n",
       "0                       A_Person  \n",
       "1  redirect::Amaranthus caudatus  \n",
       "2  redirect::Amaranthus hybridus  \n",
       "3        redirect::Maud of Wales  \n",
       "4       redirect::Gram per litre  \n",
       "5                     C_Location  \n",
       "6                      C_Company  \n",
       "7                 redirect::Tiye  \n",
       "8           redirect::Hatshepsut  \n",
       "9                     Other/None  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(data)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Other/None        79378\n",
       "Other             21720\n",
       "Person            20884\n",
       "C_Person          12142\n",
       "Location          11074\n",
       "C_Location         6246\n",
       "A_Location         4270\n",
       "C_Organization     3251\n",
       "C_Company          2491\n",
       "Company            1830\n",
       "A_Person           1591\n",
       "Organization        681\n",
       "A_Company             9\n",
       "Name: 3, dtype: int64"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[3].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "redirect\n"
     ]
    }
   ],
   "source": [
    "data_path = '/home/xminarikd/.keras/datasets/enwiki-20201001-pages-articles9.xml-p2936261p4045402.bz2'\n",
    "sample_data_path = '/home/xminarikd/Documents/VINF/data/sample_wiki_articles2.xml.bz2'\n",
    "# Object for handling xml\n",
    "handler = ContentHandler()\n",
    "\n",
    "# Parsing object\n",
    "parser = xml.sax.make_parser()\n",
    "parser.setContentHandler(handler)\n",
    "\n",
    "for i, line in enumerate(subprocess.Popen(['bzcat'], \n",
    "                         stdin = open(data_path), \n",
    "                         stdout = subprocess.PIPE).stdout):\n",
    "    parser.feed(line)\n",
    "    \n",
    "    if len(handler.output) > 20000:\n",
    "        break\n",
    "\n",
    "print(handler.output[2][1])\n",
    "#print(regex.search(exp_inf_type, infobox).group().strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.DataFrame(data)\n",
    "rr = df2.loc[df2[3] == 'Other/None']\n",
    "rr = rr.loc[rr[2] == {'categories':[]}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Speciesbox\n",
    "Citation\n",
    "Image\n",
    "div\n",
    "Licensing\n",
    "summary\n",
    "May refers to\n",
    "Use dmy dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [0, 1, 2, 3, 4]\n",
       "Index: []"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Shipping companies of the United States', 'Companies based in Virginia']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp5 = ['History of Atlanta',\n",
    "  'North Carolina in the American Civil War',\n",
    "  'Shipping companies of the United States',\n",
    "  'Companies based in Virginia']\n",
    "list(filter(lambda x: regex.search('(compan[y|ies])(?i)', x), temp5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ano\n"
     ]
    }
   ],
   "source": [
    "temp2 = ['ano','nie jasd sad', 'asdasdasd asd']\n",
    "temp3 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Organisations based in Manama']"
      ]
     },
     "execution_count": 322,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(filter(lambda x: regex.search('(organisations*|associations*)(?i)', x),temp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'p2936261p4045402'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tt = '/home/xminarikd/.keras/datasets/enwiki-20201001-pages-articles9.xml-p2936261p4045402.bz2'\n",
    "tt.split('/')[-1].split('-')[-1].split('.')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/xminarikd/Documents/VINF/data/sample_wiki_articles2.xml.bz2'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "dirname = os.getcwd().rsplit('/', 1)[0]\n",
    "dirname = f'{dirname}/data/sample_wiki_articles2.xml.bz2'\n",
    "dirname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/xminarikd/.keras/datasets/enwiki-20201001-pages-articles9.xml-p2936261p4045402.bz2'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tt = '/home/xminarikd/.keras/datasets/enwiki-20201001-pages-articles9.xml-p2936261p4045402.bz2'\n",
    "tt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
